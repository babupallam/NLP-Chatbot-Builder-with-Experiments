{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Importing Libraries"],"metadata":{"id":"7SBNwrjDXPFz"}},{"cell_type":"code","source":["\n","import torch\n","from torch.jit import script, trace\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import csv\n","import random\n","import re\n","import os\n","import unicodedata\n","import codecs\n","from io import open\n","import itertools\n","import math\n","import json\n","\n","\n","USE_CUDA = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"],"metadata":{"id":"CrAzPVIKXNaP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### **Section 1: Data Loading and Preprocessing**\n","\n","This section is crucial as it prepares the data for the model. The Cornell Movie Dialogs Corpus is loaded, cleaned, and converted into a format that can be used to train the chatbot model.\n"],"metadata":{"id":"Mw4A--ltL-WH"}},{"cell_type":"markdown","source":["\n","#### **1.1 Loading the Dataset**\n","\n","- **Objective**: Load the Cornell Movie Dialogs Corpus, which contains movie dialogues between characters. These dialogues are used to create question-answer pairs, which will be the input-output pairs for the chatbot.\n","- **Explanation**: The dataset is stored in JSON format and contains multiple fields. We parse these fields to extract individual dialogues and group them into conversations.\n","\n"],"metadata":{"id":"sl1PlmpPL-aW"}},{"cell_type":"code","source":["import os  # Import the 'os' module to work with file and directory paths\n","from google.colab import drive  # Import 'drive' from Google Colab to access Google Drive\n","\n","# Mount Google Drive to access files\n","drive.mount('/content/drive')\n","# This line mounts the Google Drive filesystem at the specified directory (/content/drive),\n","# allowing us to access files stored on Google Drive from within this Colab environment.\n","\n","# Define the path to the corpus within Google Drive\n","corpus_name = \"movie-corpus\"  # Set the name of the corpus directory\n","corpus = os.path.join(\"/content/drive/My Drive/Colab Notebooks/nlp_pro_babu/data\", corpus_name)\n","# Using os.path.join to construct the full path to the corpus folder.\n","# This ensures compatibility across operating systems, as it correctly formats the path.\n","\n","def printLines(file, n=10):\n","    \"\"\"Print the first 'n' lines of the file for previewing its contents.\"\"\"\n","    # This function takes a file path and an optional number of lines (default is 10)\n","    # and prints the first 'n' lines from the file to give a quick preview.\n","\n","    with open(file, 'rb') as datafile:  # Open the file in binary read mode ('rb')\n","        lines = datafile.readlines()  # Read all lines from the file at once as a list of bytes\n","    # Note: 'lines' is a list where each item represents a line in the file.\n","\n","    for line in lines[:n]:  # Loop over the first 'n' lines (using list slicing)\n","        print(line)  # Print each line (note that it's still in byte format)\n","\n","# Preview the file's contents\n","# This will print the first few lines of the file, which is helpful for quickly inspecting its contents.\n","printLines(os.path.join(corpus, \"utterances.jsonl\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKrbTS1uMPOY","executionInfo":{"status":"ok","timestamp":1731464364346,"user_tz":0,"elapsed":30137,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"d183bcaf-54ef-4e8b-933a-70a1928b8b22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","b'{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\\n'\n","b'{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n","b'{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\\n'\n","b'{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n","b'{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let\\'s go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"\\'s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\\n'\n","b'{\"id\": \"L924\", \"conversation_id\": \"L924\", \"text\": \"Wow\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Wow\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n","b'{\"id\": \"L872\", \"conversation_id\": \"L870\", \"text\": \"Okay -- you\\'re gonna need to learn how to lie.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 4, \"toks\": [{\"tok\": \"Okay\", \"tag\": \"UH\", \"dep\": \"intj\", \"up\": 4, \"dn\": []}, {\"tok\": \"--\", \"tag\": \":\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"\\'re\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"gon\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 6, 12]}, {\"tok\": \"na\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 6, \"dn\": []}, {\"tok\": \"need\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 8]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 8, \"dn\": []}, {\"tok\": \"learn\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 6, \"dn\": [7, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 11, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 11, \"dn\": []}, {\"tok\": \"lie\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 8, \"dn\": [9, 10]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": \"L871\", \"timestamp\": null, \"vectors\": []}\\n'\n","b'{\"id\": \"L871\", \"conversation_id\": \"L870\", \"text\": \"No\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"No\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": \"L870\", \"timestamp\": null, \"vectors\": []}\\n'\n","b'{\"id\": \"L870\", \"conversation_id\": \"L870\", \"text\": \"I\\'m kidding.  You know how sometimes you just become this \\\\\"persona\\\\\"?  And you don\\'t know how to quit?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 2, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"\\'m\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 2, \"dn\": []}, {\"tok\": \"kidding\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 3]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 2, \"dn\": [4]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 3, \"dn\": []}]}, {\"rt\": 1, \"toks\": [{\"tok\": \"You\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 6, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 3, \"dn\": []}, {\"tok\": \"sometimes\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": [2]}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 6, \"dn\": []}, {\"tok\": \"just\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": []}, {\"tok\": \"become\", \"tag\": \"VBP\", \"dep\": \"ccomp\", \"up\": 1, \"dn\": [3, 4, 5, 9]}, {\"tok\": \"this\", \"tag\": \"DT\", \"dep\": \"det\", \"up\": 9, \"dn\": []}, {\"tok\": \"\\\\\"\", \"tag\": \"``\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"persona\", \"tag\": \"NN\", \"dep\": \"attr\", \"up\": 6, \"dn\": [7, 8, 10]}, {\"tok\": \"\\\\\"\", \"tag\": \"\\'\\'\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": [12]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 11, \"dn\": []}]}, {\"rt\": 4, \"toks\": [{\"tok\": \"And\", \"tag\": \"CC\", \"dep\": \"cc\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"n\\'t\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 4, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 7, 8]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 7, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 7, \"dn\": []}, {\"tok\": \"quit\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 6]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n","b'{\"id\": \"L869\", \"conversation_id\": \"L866\", \"text\": \"Like my fear of wearing pastels?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Like\", \"tag\": \"IN\", \"dep\": \"ROOT\", \"dn\": [2, 6]}, {\"tok\": \"my\", \"tag\": \"PRP$\", \"dep\": \"poss\", \"up\": 2, \"dn\": []}, {\"tok\": \"fear\", \"tag\": \"NN\", \"dep\": \"pobj\", \"up\": 0, \"dn\": [1, 3]}, {\"tok\": \"of\", \"tag\": \"IN\", \"dep\": \"prep\", \"up\": 2, \"dn\": [4]}, {\"tok\": \"wearing\", \"tag\": \"VBG\", \"dep\": \"pcomp\", \"up\": 3, \"dn\": [5]}, {\"tok\": \"pastels\", \"tag\": \"NNS\", \"dep\": \"dobj\", \"up\": 4, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L868\", \"timestamp\": null, \"vectors\": []}\\n'\n"]}]},{"cell_type":"markdown","source":["\n","#### **1.2 Parsing and Formatting Data**\n","\n","- **Objective**: Convert the raw JSON data into a clean format suitable for training, where each line contains a question-answer pair.\n","- **Explanation**:\n","  - We define functions to parse each line of the dataset, extracting essential information like the line ID, character ID, and text.\n","  - We then group these lines into conversations, forming dialogue pairs for the chatbot.\n","  \n"],"metadata":{"id":"cFGNuE1mL-ee"}},{"cell_type":"code","source":["import json  # Import the 'json' module to handle JSON data\n","\n","def loadLinesAndConversations(fileName):\n","    \"\"\"Parse lines of the file to create a dictionary of lines and conversations.\"\"\"\n","    lines = {}  # Dictionary to store individual line objects by line ID\n","    conversations = {}  # Dictionary to store conversations grouped by conversation ID\n","\n","    # Open the file in read mode with specified encoding (ISO-8859-1) to handle any special characters\n","    with open(fileName, 'r', encoding='iso-8859-1') as f:\n","        for line in f:  # Loop over each line in the file\n","            lineJson = json.loads(line)  # Parse the JSON string to a Python dictionary\n","\n","            # Extract relevant fields from the JSON object to create a line dictionary\n","            lineObj = {\n","                \"lineID\": lineJson[\"id\"],  # Unique identifier for this line\n","                \"characterID\": lineJson[\"speaker\"],  # Identifier for the character speaking\n","                \"text\": lineJson[\"text\"]  # The actual dialogue text\n","            }\n","            # Store the line object in the 'lines' dictionary, keyed by line ID\n","            lines[lineObj['lineID']] = lineObj\n","\n","            # Group lines into conversations based on the 'conversation_id' field\n","            if lineJson[\"conversation_id\"] not in conversations:\n","                # Create a new conversation object if it doesn't exist in 'conversations'\n","                convObj = {\n","                    \"conversationID\": lineJson[\"conversation_id\"],  # ID of the conversation\n","                    \"movieID\": lineJson[\"meta\"][\"movie_id\"],  # ID of the movie associated with this conversation\n","                    \"lines\": [lineObj]  # Initialize the conversation with this line\n","                }\n","            else:\n","                # If conversation already exists, retrieve it and add this line to its 'lines' list\n","                convObj = conversations[lineJson[\"conversation_id\"]]\n","                convObj[\"lines\"].insert(0, lineObj)  # Insert line at the beginning to keep order\n","\n","            # Store or update the conversation object in 'conversations', keyed by conversation ID\n","            conversations[convObj[\"conversationID\"]] = convObj\n","\n","    # Return the dictionaries of lines and conversations\n","    return lines, conversations\n"],"metadata":{"id":"wsxQqWZvMmBG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### **1.3 Extracting Sentence Pairs**\n","\n","- **Objective**: Create a list of question-answer sentence pairs from conversations.\n","- **Explanation**:\n","  - Each conversation contains multiple lines (utterances).\n","  - By taking each pair of consecutive lines in a conversation, we create a question-answer pair.\n","  \n"],"metadata":{"id":"OnIyziTyMGM4"}},{"cell_type":"code","source":["def extractSentencePairs(conversations):\n","    \"\"\"Extract sentence pairs from each conversation to create question-answer pairs.\"\"\"\n","    qa_pairs = []  # Initialize a list to store question-answer pairs\n","\n","    # Loop over each conversation in the provided 'conversations' dictionary\n","    for conversation in conversations.values():\n","\n","        # Iterate over all lines in a conversation to form question-answer pairs\n","        for i in range(len(conversation[\"lines\"]) - 1):  # Stop at the second-last line\n","            inputLine = conversation[\"lines\"][i][\"text\"].strip()  # Current line's text\n","            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()  # Next line's text\n","\n","            # Only include pairs where both inputLine and targetLine contain text\n","            if inputLine and targetLine:\n","                qa_pairs.append([inputLine, targetLine])  # Append as a question-answer pair\n","\n","    return qa_pairs  # Return the list of question-answer pairs\n","\n","# Load and format data\n","# Call loadLinesAndConversations to parse lines and conversations from the file\n","lines, conversations = loadLinesAndConversations(os.path.join(corpus, \"utterances.jsonl\"))\n","\n","# Extract question-answer pairs from conversations\n","qa_pairs = extractSentencePairs(conversations)\n","\n","# Print a sample of the extracted question-answer pairs to verify functionality\n","print(\"Sample question-answer pairs:\")\n","print(qa_pairs[:5])  # Print the first five pairs as a sample output\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtmlOub7Mq99","executionInfo":{"status":"ok","timestamp":1731464379063,"user_tz":0,"elapsed":14719,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"796e0b65-ec57-47a7-a888-6ebb3f83d75c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample question-answer pairs:\n","[['They do to!', 'They do not!'], ['She okay?', 'I hope so.'], ['Wow', \"Let's go.\"], ['I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?', 'No'], ['No', \"Okay -- you're gonna need to learn how to lie.\"]]\n"]}]},{"cell_type":"markdown","source":["\n","#### **1.4 Saving the Formatted Data**\n","\n","- **Objective**: Save the extracted question-answer pairs in a tab-separated format for ease of use.\n","- **Explanation**: Writing the processed question-answer pairs to a text file makes it easier to load them later for training or debugging.\n"],"metadata":{"id":"N3siKzwjMGKF"}},{"cell_type":"code","source":["import csv  # Import 'csv' module to handle CSV file operations\n","import codecs  # Import 'codecs' for encoding/decoding purposes\n","\n","# Define the output path for formatted data\n","datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")  # Path to save the formatted output file\n","delimiter = '\\t'  # Specify tab character as the delimiter for the file\n","delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))  # Decode the delimiter to handle escape sequences\n","\n","# Write pairs to the file\n","print(\"\\nWriting formatted file...\")\n","with open(datafile, 'w', encoding='utf-8') as outputfile:  # Open file in write mode with UTF-8 encoding\n","    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')  # Set up CSV writer with custom settings\n","\n","    # Write each question-answer pair from qa_pairs to the output file\n","    for pair in qa_pairs:\n","        writer.writerow(pair)  # Write each pair as a row in the output file\n","\n","# Verify the saved file\n","print(\"Sample lines from formatted file:\")\n","printLines(datafile)  # Use the printLines function to display a sample of the saved file\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Va7cNamMMwEA","executionInfo":{"status":"ok","timestamp":1731464381762,"user_tz":0,"elapsed":2702,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"9c47b8f2-045b-483c-d5e5-c90acb341c77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Writing formatted file...\n","Sample lines from formatted file:\n","b'They do to!\\tThey do not!\\n'\n","b'She okay?\\tI hope so.\\n'\n","b\"Wow\\tLet's go.\\n\"\n","b'\"I\\'m kidding.  You know how sometimes you just become this \"\"persona\"\"?  And you don\\'t know how to quit?\"\\tNo\\n'\n","b\"No\\tOkay -- you're gonna need to learn how to lie.\\n\"\n","b\"I figured you'd get to the good stuff eventually.\\tWhat good stuff?\\n\"\n","b'What good stuff?\\t\"The \"\"real you\"\".\"\\n'\n","b'\"The \"\"real you\"\".\"\\tLike my fear of wearing pastels?\\n'\n","b'do you listen to this crap?\\tWhat crap?\\n'\n","b\"What crap?\\tMe.  This endless ...blonde babble. I'm like, boring myself.\\n\"\n"]}]},{"cell_type":"markdown","source":["#### **1.5 Building Vocabulary**\n","\n","- **Objective**: Construct a vocabulary from unique words in the dataset, assigning each word a unique index.\n","\n","- **Explanation**:\n","  - **Numerical Representation**:\n","    - Neural networks work with numbers, not raw text, so each word in the dataset must be converted into a numeric format.\n","    - By creating a vocabulary that maps each unique word to an index, sentences can be represented as sequences of integers, enabling them to be processed by the model.\n","  - **Special Tokens**: The vocabulary includes essential special tokens:\n","    - **`PAD` (Padding)**: Used to fill shorter sequences up to a uniform length, ensuring consistent input size.\n","    - **`SOS` (Start of Sentence)**: Indicates the beginning of a sentence, useful in sequence generation models.\n","    - **`EOS` (End of Sentence)**: Marks the end of a sentence, helping the model identify where a sentence or response ends.\n","    -**`UNK` (Unknown Token)**: Represents words or tokens that are unknown to the model, meaning they are not part of the vocabulary that the model was trained on. This token is used to handle out-of-vocabulary (OOV) words, ensuring that the model can still process sequences containing unfamiliar tokens without breaking or producing errors.\n","\n","  \n","This step is foundational in NLP preprocessing, allowing efficient encoding of text data for neural network input and providing flexibility in handling sentences of varying lengths."],"metadata":{"id":"DwNJfjM-MGGZ"}},{"cell_type":"code","source":["# Define special tokens with fixed indices for padding, start of sentence, and end of sentence\n","PAD_token = 0  # Padding token\n","SOS_token = 1  # Start-of-sentence token\n","EOS_token = 2  # End-of-sentence token\n","UNK_token = 3  # Unknown word token # Added this line to define UNK_token\n","\n","class Voc:\n","    def __init__(self, name):\n","        # Initialize the vocabulary with a name and essential attributes\n","        self.name = name  # Name of the vocabulary (useful for identifying different vocabs)\n","        self.trimmed = False  # A flag indicating whether trimming has been applied to reduce vocab size\n","        self.word2index = {}  # Dictionary mapping words to their unique indices\n","        self.word2count = {}  # Dictionary to count occurrences of each word\n","        # Mapping of indices to words for easy conversion from index to word\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n","        self.num_words = 3  # Initial count includes only PAD, SOS, EOS tokens\n","\n","    def addSentence(self, sentence):\n","        # Add each word in the sentence to the vocabulary\n","        for word in sentence.split(' '):  # Split sentence by spaces to get individual words\n","            self.addWord(word)  # Add each word to vocabulary\n","\n","    def addWord(self, word):\n","        # Add a word to the vocabulary, updating dictionaries and word counts\n","        if word not in self.word2index:\n","            # Assign a new index to the word and update mappings\n","            self.word2index[word] = self.num_words  # Assign the next available index\n","            self.word2count[word] = 1  # Initialize word count to 1\n","            self.index2word[self.num_words] = word  # Map the index back to the word\n","            self.num_words += 1  # Increment total word count\n","        else:\n","            # If the word is already in the vocabulary, increment its count\n","            self.word2count[word] += 1\n","\n","    def trim(self, min_count):\n","        \"\"\"Trim rarely used words to reduce vocabulary size.\"\"\"\n","        if self.trimmed:\n","            return  # If already trimmed, do nothing\n","        self.trimmed = True  # Mark as trimmed to avoid redundant trimming\n","\n","        # Identify words that meet the minimum count threshold\n","        keep_words = [k for k, v in self.word2count.items() if v >= min_count]\n","\n","        # Reinitialize dictionaries to only include words that meet the threshold\n","        self.word2index = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3  # Reset count for PAD, SOS, EOS tokens\n","\n","        # Add words that meet the threshold back into the vocabulary\n","        for word in keep_words:\n","            self.addWord(word)\n"],"metadata":{"id":"krdn78jbM6T8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **1.6 Loading and Filtering Pairs**\n","\n","- **Objective**: Convert question-answer pairs into indexed representations using a vocabulary and filter out pairs with sentences that exceed a defined maximum length.\n","\n","- **Explanation**:\n","    - By converting words into indices, each sentence in a question-answer pair is transformed into a sequence of integers, making it ready for input to a neural network model.\n","    - Filtering pairs based on sentence length further optimizes the dataset by removing long sentences, which can increase computational requirements and potentially degrade model performance.\n","    - This length restriction ensures that all pairs are below a set threshold (`MAX_LENGTH`), allowing the model to focus on shorter, more manageable sentences typical in conversational data.\n","\n","This loading and filtering step is crucial in NLP preprocessing, especially for tasks like training a dialogue model where consistent sentence length and tokenized representation of words help standardize input and minimize processing time."],"metadata":{"id":"PcTmQOMfMGC8"}},{"cell_type":"code","source":["MAX_LENGTH = 10  # Maximum sentence length to consider\n","\n","# Turn a Unicode string to plain ASCII\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove non-letter characters\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    s = re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","# Read query/response pairs and return a voc object\n","def readVocs(datafile, corpus_name):\n","    print(\"Reading lines...\")\n","    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n","    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n","    voc = Voc(corpus_name)\n","    return voc, pairs\n","\n","# Returns True if both sentences in a pair 'p' are under the MAX_LENGTH threshold\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n","\n","# Filter pairs using the ``filterPair`` condition\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]\n","\n","# Load and prepare data by reading lines, normalizing, and filtering pairs\n","def loadPrepareData(corpus, corpus_name, datafile):\n","    print(\"Start preparing training data ...\")\n","    voc, pairs = readVocs(datafile, corpus_name)\n","    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","    print(\"Counted words:\", voc.num_words)\n","    return voc, pairs\n","\n","# Example usage: Load vocabulary and pairs\n","save_dir = os.path.join(\"data\", \"save\")\n","voc, pairs = loadPrepareData(corpus, corpus_name, datafile)\n","# Print some pairs for validation\n","print(\"\\nSample pairs:\")\n","for pair in pairs[:10]:\n","    print(pair)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_w_kCGnFNMYO","executionInfo":{"status":"ok","timestamp":1731464397915,"user_tz":0,"elapsed":16155,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"6035939c-1a7e-45d9-f8c0-0f888f3124c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Start preparing training data ...\n","Reading lines...\n","Read 221282 sentence pairs\n","Trimmed to 64313 sentence pairs\n","Counting words...\n","Counted words: 18082\n","\n","Sample pairs:\n","['they do to !', 'they do not !']\n","['she okay ?', 'i hope so .']\n","['wow', 'let s go .']\n","['what good stuff ?', 'the real you .']\n","['the real you .', 'like my fear of wearing pastels ?']\n","['do you listen to this crap ?', 'what crap ?']\n","['well no . . .', 'then that s all you had to say .']\n","['then that s all you had to say .', 'but']\n","['but', 'you always been this selfish ?']\n","['have fun tonight ?', 'tons']\n"]}]},{"cell_type":"markdown","source":["#### **1.7 Trimming Vocabulary**\n","\n","- **Objective**: Reduce the vocabulary size by removing words that appear fewer than `MIN_COUNT` times. This reduces the model's complexity and helps it focus on frequently used words, which are more likely to be useful for generating responses.\n","\n","- **Explanation**:\n","  - Words that appear rarely (less than `MIN_COUNT` times) are often not critical for generating meaningful responses and can increase the model’s memory usage and computation time.\n","  - The `trimRareWords` function first trims the vocabulary to remove rare words and then filters out pairs that contain any of these rare words, ensuring that only pairs with frequently used words are retained.\n","\n"],"metadata":{"id":"TsTi6HX9b2WB"}},{"cell_type":"code","source":["MIN_COUNT = 3  # Minimum word count threshold for trimming\n","\n","# Trim words used less than MIN_COUNT from the vocabulary\n","def trimRareWords(voc, pairs, MIN_COUNT):\n","    \"\"\"\n","    Trims words used less than MIN_COUNT times from the vocabulary and filters out pairs\n","    containing these rare words.\n","\n","    Args:\n","        voc (Voc): Vocabulary object with word frequency information.\n","        pairs (list of tuples): List of question-answer pairs (input-output sentence pairs).\n","        MIN_COUNT (int): Minimum occurrence threshold to keep a word in the vocabulary.\n","\n","    Returns:\n","        keep_pairs (list of tuples): Filtered list of pairs with only frequent words.\n","    \"\"\"\n","    # Step 1: Trim rare words from the vocabulary\n","    voc.trim(MIN_COUNT)\n","\n","    # Step 2: Filter pairs to keep only those with words remaining in the trimmed vocabulary\n","    keep_pairs = []\n","    for pair in pairs:\n","        input_sentence = pair[0]\n","        output_sentence = pair[1]\n","        keep_input = True\n","        keep_output = True\n","\n","        # Check if all words in the input sentence are in the trimmed vocabulary\n","        for word in input_sentence.split(' '):\n","            if word not in voc.word2index:\n","                keep_input = False  # Mark as False if any word is not in the trimmed vocabulary\n","                break\n","\n","        # Check if all words in the output sentence are in the trimmed vocabulary\n","        for word in output_sentence.split(' '):\n","            if word not in voc.word2index:\n","                keep_output = False  # Mark as False if any word is not in the trimmed vocabulary\n","                break\n","\n","        # Only keep pairs that have all words in the trimmed vocabulary\n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","\n","    # Print information about how many pairs were removed after trimming\n","    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n","\n","    return keep_pairs\n","\n","# Apply trimming to vocabulary and filter pairs accordingly\n","pairs = trimRareWords(voc, pairs, MIN_COUNT)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PV1cCk4kb2v2","executionInfo":{"status":"ok","timestamp":1731464398407,"user_tz":0,"elapsed":494,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"4a5aaa50-8fd8-4a58-e79f-383c2e437731"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Trimmed from 64313 pairs to 53131, 0.8261 of total\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"fbqfy3xwLodn"}},{"cell_type":"markdown","source":["#### **1.8 Preparing Batches for Training**\n","\n","- **Objective**: Prepare batches of data for model training. Each batch includes padded sequences, length tensors, and masks to help the model handle variable-length sequences efficiently.\n","\n","- **Explanation**:\n","  - **Padding**: Ensures all sequences in a batch are the same length by adding padding tokens to shorter sequences.\n","  - **Masking**: Creates binary masks to indicate which elements in each sequence are real tokens (1) versus padding tokens (0). This helps the model ignore padding during loss calculation.\n","  - **Batch Preparation**: Organizes data into batches so that each batch has padded input and target sequences, their lengths, and masks.\n","\n"],"metadata":{"id":"f-ELaQR5cb6U"}},{"cell_type":"code","source":["# Converts a sentence into a sequence of word indices, appending an EOS token at the end\n","def indexesFromSentence(voc, sentence):\n","    \"\"\"\n","    Converts a sentence to a list of word indices from the vocabulary.\n","    Adds an EOS token at the end to mark the end of the sequence.\n","\n","    Args:\n","        voc (Voc): The vocabulary object containing word-to-index mappings.\n","        sentence (str): The sentence to convert.\n","\n","    Returns:\n","        list: List of word indices representing the sentence.\n","    \"\"\"\n","    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n","\n","# Pads a list of sequences to the maximum sequence length in the batch\n","def zeroPadding(l, fillvalue=PAD_token):\n","    \"\"\"\n","    Pads a list of sequences to the maximum length in the batch.\n","\n","    Args:\n","        l (list of lists): List of sequences, where each sequence is a list of word indices.\n","        fillvalue (int): The value to use for padding (typically PAD_token).\n","\n","    Returns:\n","        list of lists: Padded sequences with each sequence having the same length.\n","    \"\"\"\n","    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n","\n","# Creates a binary mask indicating which elements in a sequence are non-padding\n","def binaryMatrix(l, value=PAD_token):\n","    \"\"\"\n","    Creates a binary mask to indicate non-padding elements in the sequences.\n","\n","    Args:\n","        l (list of lists): List of padded sequences.\n","        value (int): The padding token value (e.g., PAD_token).\n","\n","    Returns:\n","        list of lists: A binary mask where 1 indicates a real token and 0 indicates padding.\n","    \"\"\"\n","    m = []\n","    for i, seq in enumerate(l):\n","        m.append([])\n","        for token in seq:\n","            if token == PAD_token:\n","                m[i].append(0)  # Mark padding as 0\n","            else:\n","                m[i].append(1)  # Mark real tokens as 1\n","    return m\n","\n","# Converts input sentence batch into padded tensor and returns lengths\n","def inputVar(l, voc):\n","    \"\"\"\n","    Converts a batch of input sentences into a padded tensor with lengths.\n","\n","    Args:\n","        l (list of str): List of input sentences.\n","        voc (Voc): The vocabulary object for converting words to indices.\n","\n","    Returns:\n","        padVar (LongTensor): Padded tensor of input sequences.\n","        lengths (Tensor): Tensor containing the lengths of each input sequence.\n","    \"\"\"\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]  # Convert sentences to indices\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])  # Get length of each sequence\n","    padList = zeroPadding(indexes_batch)  # Pad sequences\n","    padVar = torch.LongTensor(padList)  # Convert to tensor\n","    return padVar, lengths\n","\n","# Converts target sentence batch into padded tensor, binary mask, and max target length\n","def outputVar(l, voc):\n","    \"\"\"\n","    Converts a batch of target sentences into a padded tensor, binary mask, and max target length.\n","\n","    Args:\n","        l (list of str): List of target sentences.\n","        voc (Voc): The vocabulary object for converting words to indices.\n","\n","    Returns:\n","        padVar (LongTensor): Padded tensor of target sequences.\n","        mask (BoolTensor): Binary mask indicating non-padding tokens.\n","        max_target_len (int): Maximum length of the target sequences.\n","    \"\"\"\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]  # Convert sentences to indices\n","    max_target_len = max([len(indexes) for indexes in indexes_batch])  # Find maximum sequence length\n","    padList = zeroPadding(indexes_batch)  # Pad sequences\n","    mask = binaryMatrix(padList)  # Create binary mask\n","    mask = torch.BoolTensor(mask)  # Convert to BoolTensor\n","    padVar = torch.LongTensor(padList)  # Convert to tensor\n","    return padVar, mask, max_target_len\n","\n","# Groups question-answer pairs into a batch of padded input and target tensors, lengths, and masks\n","def batch2TrainData(voc, pair_batch):\n","    \"\"\"\n","    Groups question-answer pairs into batches of padded input and target tensors, with lengths and masks.\n","\n","    Args:\n","        voc (Voc): The vocabulary object.\n","        pair_batch (list of tuples): List of question-answer sentence pairs.\n","\n","    Returns:\n","        inp (LongTensor): Padded tensor of input sequences.\n","        lengths (Tensor): Tensor containing lengths of each input sequence.\n","        output (LongTensor): Padded tensor of target sequences.\n","        mask (BoolTensor): Binary mask indicating non-padding tokens in target sequences.\n","        max_target_len (int): Maximum length of the target sequences.\n","    \"\"\"\n","    # Sort pairs by length of the input sequence in descending order (for efficient RNN processing)\n","    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n","    input_batch, output_batch = [], []\n","    for pair in pair_batch:\n","        input_batch.append(pair[0])  # Add input sentence\n","        output_batch.append(pair[1])  # Add output sentence\n","    inp, lengths = inputVar(input_batch, voc)  # Convert input batch to tensor and lengths\n","    output, mask, max_target_len = outputVar(output_batch, voc)  # Convert output batch to tensor, mask, and max length\n","    return inp, lengths, output, mask, max_target_len\n","\n","# Example for validation\n","small_batch_size = 5\n","batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n","input_variable, lengths, target_variable, mask, max_target_len = batches\n","\n","# Display the processed batch data for verification\n","print(\"input_variable:\", input_variable)\n","print(\"lengths:\", lengths)\n","print(\"target_variable:\", target_variable)\n","print(\"mask:\", mask)\n","print(\"max_target_len:\", max_target_len)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fw9WFmAOb9Y8","executionInfo":{"status":"ok","timestamp":1731464398408,"user_tz":0,"elapsed":5,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"7d5882cf-4a3a-4d36-c2d5-68ca1a10aa7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input_variable: tensor([[ 111,   20,  384,   54,  321],\n","        [  11,   98,   24, 1124,    6],\n","        [1026,   14,   14,   14,    2],\n","        [ 706,    2,    2,    2,    0],\n","        [  10,    0,    0,    0,    0],\n","        [   2,    0,    0,    0,    0]])\n","lengths: tensor([6, 4, 4, 4, 3])\n","target_variable: tensor([[ 317,   98,  184,   54,  321],\n","        [  24,   14,  140, 1001,   14],\n","        [ 136,    2,   17,   14,    2],\n","        [   5,    0,  132,    2,    0],\n","        [  14,    0,  185,    0,    0],\n","        [   2,    0, 1198,    0,    0],\n","        [   0,    0,   17,    0,    0],\n","        [   0,    0,   10,    0,    0],\n","        [   0,    0,    2,    0,    0]])\n","mask: tensor([[ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True,  True,  True,  True,  True],\n","        [ True, False,  True,  True, False],\n","        [ True, False,  True, False, False],\n","        [ True, False,  True, False, False],\n","        [False, False,  True, False, False],\n","        [False, False,  True, False, False],\n","        [False, False,  True, False, False]])\n","max_target_len: 9\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"SJ7jARmzcnKr"}},{"cell_type":"markdown","source":["### **Section 2: Model Definition (Encoder-Decoder with Attention)**\n","\n","The model consists of an encoder and a decoder, with an attention mechanism that allows the decoder to focus on specific parts of the input sequence. This architecture is designed to handle variable-length input and output sequences, which is essential for a chatbot.\n"],"metadata":{"id":"T08usYjJMF7h"}},{"cell_type":"markdown","source":["#### **2.1 Encoder**\n","\n","- **Objective**: Encode the input sentence into a fixed-size context vector that captures its semantic meaning.\n","\n","- **Explanation**:\n","  - The encoder employs a bidirectional Gated Recurrent Unit (GRU) to process input sentences. A bidirectional GRU is used to capture both past and future context within the sentence, providing a richer representation.\n","  - Each word in the input sequence is transformed into an embedding vector, which is a dense representation capturing semantic information.\n","  - The bidirectional GRU processes the embeddings, producing hidden states in both forward and backward directions. These states are summed to create a context vector that effectively encodes the meaning of the sentence.\n","  - This context vector is passed to the decoder for generating a response.\n","\n"],"metadata":{"id":"rGmr6PjOMF3J"}},{"cell_type":"code","source":["import torch  # Import PyTorch for neural network operations\n","import torch.nn as nn  # Import nn module for building neural network layers\n","import torch.nn.functional as F  # Import functional module for activation functions\n","\n","class EncoderRNN(nn.Module):\n","    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n","        super(EncoderRNN, self).__init__()  # Initialize the nn.Module class\n","        self.n_layers = n_layers  # Number of GRU layers\n","        self.hidden_size = hidden_size  # Size of the hidden state vector\n","        self.embedding = embedding  # Embedding layer to convert word indices to dense vectors\n","\n","        # Initialize a bidirectional GRU\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n","                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n","\n","    def forward(self, input_seq, input_lengths, hidden=None):\n","        # Convert word indices to embeddings\n","        embedded = self.embedding(input_seq)  # Shape: (max_length, batch_size, hidden_size)\n","\n","        # Pack padded batch of sequences for efficient processing by the GRU\n","        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n","\n","        # Forward pass through the bidirectional GRU\n","        outputs, hidden = self.gru(packed, hidden)  # GRU processes packed input\n","\n","        # Unpack the packed sequence to obtain outputs of all time steps\n","        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n","\n","        # Sum bidirectional GRU outputs to combine forward and backward contexts\n","        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n","\n","        return outputs, hidden  # Return context vector and final hidden state\n"],"metadata":{"id":"rJC23k9SOjFG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **2.2 Attention Mechanism**\n","\n","- **Objective**: Enable the decoder to selectively focus on relevant parts of the encoder’s output sequence when generating each word in the output sequence.\n","\n","- **Explanation**:\n","  - This code implements Luong’s \"Global attention\" mechanism, where the attention weights (or scores) for each encoder output are calculated at every decoding step.\n","  - The `Attn` class supports three methods of calculating attention weights:\n","    - **Dot**: A simple dot product between the decoder hidden state and the encoder output.\n","    - **General**: Applies a linear transformation to the encoder output before computing the dot product.\n","    - **Concat**: Concatenates the hidden state and encoder output, passes them through a linear layer, and then applies a learned parameter vector to compute the score.\n","  - These attention weights indicate how relevant each encoder output is to the current decoding step, allowing the model to focus on specific parts of the input sequence when producing each word.\n"],"metadata":{"id":"DQ9zj5OpMFzL"}},{"cell_type":"code","source":["\n","class Attn(nn.Module):\n","    def __init__(self, method, hidden_size):\n","        super(Attn, self).__init__()\n","        self.method = method  # Attention method: 'dot', 'general', or 'concat'\n","        self.hidden_size = hidden_size  # Size of the hidden state\n","\n","        # Define layers based on the attention method\n","        if self.method == 'general':\n","            self.attn = nn.Linear(hidden_size, hidden_size)  # Linear layer for 'general' method\n","        elif self.method == 'concat':\n","            # Linear layer for concatenated input (hidden state + encoder output)\n","            self.attn = nn.Linear(hidden_size * 2, hidden_size)\n","            # Parameter vector for 'concat' method\n","            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n","\n","    def dot_score(self, hidden, encoder_output):\n","        # Dot product of hidden state and encoder output for 'dot' method\n","        return torch.sum(hidden * encoder_output, dim=2)\n","\n","    def general_score(self, hidden, encoder_output):\n","        # Linear transform on encoder output for 'general' method\n","        energy = self.attn(encoder_output)\n","        return torch.sum(hidden * energy, dim=2)  # Dot product with transformed output\n","\n","    def concat_score(self, hidden, encoder_output):\n","        # Concatenate expanded hidden state and encoder output, apply tanh\n","        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n","        # Multiply by learned parameter vector 'v' and sum for the score\n","        return torch.sum(self.v * energy, dim=2)\n","\n","    def forward(self, hidden, encoder_outputs):\n","        # Calculate attention scores using the chosen method\n","        if self.method == 'general':\n","            attn_energies = self.general_score(hidden, encoder_outputs)\n","        elif self.method == 'concat':\n","            attn_energies = self.concat_score(hidden, encoder_outputs)\n","        elif self.method == 'dot':\n","            attn_energies = self.dot_score(hidden, encoder_outputs)\n","\n","        # Normalize energies to obtain attention weights using softmax\n","        return F.softmax(attn_energies.t(), dim=1).unsqueeze(1)\n"],"metadata":{"id":"SIq87v7HO8p3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **2.3 Decoder with Attention**\n","\n","- **Objective**: Generate a response sequence by leveraging both the context vector from the encoder and attention weights that focus on relevant parts of the input at each step.\n","\n","- **Explanation**:\n","  - The decoder takes the context vector generated by the encoder and applies the attention mechanism at each decoding step to focus on the most relevant parts of the encoder’s outputs.\n","  - Using Luong’s approach, the attention-weighted context vector is combined with the GRU output before predicting the next word, allowing the decoder to focus on specific words in the input sequence.\n","  - This step-by-step attention enables the model to generate more contextually appropriate responses, especially for tasks like machine translation or dialogue generation.\n"],"metadata":{"id":"RaYt0vRcMFwP"}},{"cell_type":"code","source":["\n","class LuongAttnDecoderRNN(nn.Module):\n","    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n","        super(LuongAttnDecoderRNN, self).__init__()\n","        self.attn_model = attn_model  # Type of attention model to use\n","        self.hidden_size = hidden_size  # Size of hidden state\n","        self.output_size = output_size  # Vocabulary size for output\n","        self.n_layers = n_layers  # Number of layers in GRU\n","        self.dropout = dropout  # Dropout probability for regularization\n","\n","        # Define layers\n","        self.embedding = embedding  # Embedding layer for input words\n","        self.embedding_dropout = nn.Dropout(dropout)  # Dropout for embeddings\n","        # GRU layer with specified hidden size, layers, and dropout\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n","        # Linear layer to concatenate context vector and GRU output\n","        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n","        # Linear layer to generate output scores for each word in the vocabulary\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","        # Initialize the attention mechanism\n","        self.attn = Attn(attn_model, hidden_size)\n","\n","    def forward(self, input_step, last_hidden, encoder_outputs):\n","        # Embed the current input word and apply dropout\n","        embedded = self.embedding(input_step)  # Shape: (1, batch_size, hidden_size)\n","        embedded = self.embedding_dropout(embedded)\n","\n","        # Pass through the GRU layer\n","        rnn_output, hidden = self.gru(embedded, last_hidden)  # rnn_output shape: (1, batch_size, hidden_size)\n","\n","        # Calculate attention weights based on the current GRU output and encoder outputs\n","        attn_weights = self.attn(rnn_output, encoder_outputs)  # Shape: (batch_size, 1, max_length)\n","\n","        # Multiply attention weights by encoder outputs to get the context vector\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # Shape: (batch_size, 1, hidden_size)\n","\n","        # Concatenate context vector and GRU output\n","        rnn_output = rnn_output.squeeze(0)  # Shape: (batch_size, hidden_size)\n","        context = context.squeeze(1)  # Shape: (batch_size, hidden_size)\n","        concat_input = torch.cat((rnn_output, context), 1)  # Shape: (batch_size, hidden_size * 2)\n","        concat_output = torch.tanh(self.concat(concat_input))  # Shape: (batch_size, hidden_size)\n","\n","        # Generate output scores for each word in the vocabulary\n","        output = self.out(concat_output)  # Shape: (batch_size, output_size)\n","        output = F.softmax(output, dim=1)  # Apply softmax to get probabilities for each word\n","        return output, hidden  # Return output probabilities and hidden state\n"],"metadata":{"id":"aCltMBwDPUsx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **2.4 Initializing the Model**\n","\n","- **Objective**: Set model parameters, initialize the encoder and decoder with the chosen configurations, and, if available, load pretrained embeddings. Move the models to GPU for faster computations if it’s available.\n","\n","- **Explanation**:\n","  - **Model Parameters**: Key settings such as `hidden_size`, number of layers, `dropout` rate, and `batch_size` are configured here. These parameters affect the complexity and capacity of the model.\n","  - **Embedding Layer**: An embedding layer is initialized, with its size determined by the vocabulary (`voc.num_words`) and `hidden_size`. This layer will convert word indices into dense vector representations.\n","  - **Encoder and Decoder Initialization**:\n","    - The encoder is initialized with the embedding layer, number of layers, and dropout rate.\n","    - The decoder is initialized with the same embedding layer, attention mechanism (`attn_model`), hidden size, and output size (equal to vocabulary size).\n","  - **GPU Transfer**: Both models are moved to the GPU (`device`) for accelerated training if a GPU is available.\n"],"metadata":{"id":"xMBwH1m8MFui"}},{"cell_type":"code","source":["import torch\n","\n","# Define device to use GPU if available, otherwise fall back to CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define model parameters\n","hidden_size = 500\n","encoder_n_layers = 2\n","decoder_n_layers = 2\n","dropout = 0.1\n","batch_size = 64\n","attn_model = 'dot'  # Attention model type: dot, general, concat\n","\n","# Initialize embedding layer\n","embedding = nn.Embedding(voc.num_words, hidden_size)\n","\n","# Initialize encoder and decoder models\n","encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout).to(device)\n","decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout).to(device)\n","\n","print(\"Encoder and decoder initialized and ready for training.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOBYvoFMPkpk","executionInfo":{"status":"ok","timestamp":1731464398763,"user_tz":0,"elapsed":358,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"f1e13931-c613-4a6d-c225-9bef9e88d969"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoder and decoder initialized and ready for training.\n"]}]},{"cell_type":"markdown","source":["### **Section 3: Training the Model**\n","\n","This section includes defining a custom loss function that handles padded sequences, creating a function for a single training iteration, and implementing the main training loop.\n","\n"],"metadata":{"id":"Oxd8krRCMFos"}},{"cell_type":"markdown","source":["#### **3.1 Masked Loss Function**\n","\n","- **Objective**: Define a custom loss function that calculates the loss only for meaningful (non-padded) tokens, effectively ignoring padding tokens in variable-length sequences.\n","\n","- **Explanation**:\n","  - In sequence processing, padding tokens are added to align sequences in a batch to the same length. However, these padding tokens don’t contain meaningful information, so they shouldn’t affect the model’s loss calculation.\n","  - The `maskNLLLoss` function calculates the Negative Log Likelihood (NLL) loss while ignoring padded tokens, using a binary mask that marks non-padded elements.\n","  - This approach ensures that only the actual tokens contribute to the loss, making training more efficient and focused.\n","\n"],"metadata":{"id":"87TGEWv-MFka"}},{"cell_type":"code","source":["\n","def maskNLLLoss(inp, target, mask):\n","    \"\"\"\n","    Calculate the Negative Log Likelihood loss for non-padded elements.\n","    Args:\n","        inp: Decoder output probabilities (tensor of shape [batch_size, vocab_size]).\n","        target: Target word indices (tensor of shape [batch_size]).\n","        mask: Binary mask indicating non-padded elements (1 for actual tokens, 0 for padding).\n","    Returns:\n","        loss: Average masked NLL loss for non-padded tokens.\n","        nTotal: Total count of non-padded elements.\n","    \"\"\"\n","    # Total number of non-padded tokens\n","    nTotal = mask.sum()\n","\n","    # Compute the Negative Log Likelihood for the target words\n","    # torch.gather extracts the log probabilities for each target word in the batch\n","    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n","\n","    # Apply the mask to filter out losses from padding elements\n","    loss = crossEntropy.masked_select(mask).mean()  # Only average over non-padded tokens\n","\n","    # Move the loss to the specified device (GPU if available)\n","    loss = loss.to(device)\n","\n","    return loss, nTotal.item()  # Return the loss and the count of non-padded tokens\n"],"metadata":{"id":"kDrruhiYSx88"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### **3.2 Single Training Iteration**\n","\n","- **Objective**: Define the process for a single training iteration on one batch of data, involving both the encoder and decoder, to minimize the loss.\n","\n","- **Explanation**:\n","  - **Encoder Pass**: Each batch of input sentences is passed through the encoder to produce a context vector (encoder hidden states).\n","  - **Decoder Pass with Teacher Forcing**:\n","    - The decoder generates each word in the output sequence, starting with the start-of-sequence (`SOS`) token.\n","    - **Teacher Forcing**: A technique where the actual target word is used as the next input to the decoder. This can improve convergence but is only applied based on a probability (`teacher_forcing_ratio`), to help the model learn to generate predictions without always relying on correct inputs.\n","  - **Loss Calculation**: For each predicted word, a masked NLL loss is computed to only include meaningful (non-padded) tokens.\n","  - **Backpropagation and Gradient Clipping**: Gradients are computed and clipped to prevent the \"exploding gradient\" problem, which can occur in RNNs. Finally, the model weights are updated based on the gradients.\n","\n"],"metadata":{"id":"GQHyvkNOMFI0"}},{"cell_type":"code","source":["def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n","          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n","    \"\"\"\n","    Perform a single training iteration (single batch).\n","    Args:\n","        input_variable: Padded input sentence batch.\n","        lengths: List of sentence lengths.\n","        target_variable: Padded target sentence batch.\n","        mask: Binary mask for target sentences.\n","        max_target_len: Maximum target sentence length.\n","        encoder: Encoder model.\n","        decoder: Decoder model.\n","        embedding: Embedding layer.\n","        encoder_optimizer: Optimizer for encoder.\n","        decoder_optimizer: Optimizer for decoder.\n","        batch_size: Batch size.\n","        clip: Gradient clipping threshold.\n","    Returns:\n","        print_losses: List of losses for each non-padded token.\n","        n_totals: Total number of non-padded tokens.\n","    \"\"\"\n","    # Zero gradients for both optimizers\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    # Move tensors to device\n","    input_variable = input_variable.to(device)\n","    target_variable = target_variable.to(device)\n","    mask = mask.to(device)\n","    lengths = lengths.to(\"cpu\")  # Lengths should always be on CPU for packing\n","\n","    # Initialize variables\n","    loss = 0\n","    print_losses = []\n","    n_totals = 0\n","\n","    # Forward pass through encoder\n","    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n","\n","    # Create initial decoder input (start with SOS tokens)\n","    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]]).to(device)\n","\n","    # Set initial decoder hidden state to the encoder's final hidden state\n","    decoder_hidden = encoder_hidden[:decoder.n_layers]\n","\n","    # Determine if using teacher forcing\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    # Forward batch of sequences through decoder one time step at a time\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for t in range(max_target_len):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n","            decoder_input = target_variable[t].view(1, -1)  # Teacher forcing\n","            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n","            loss += mask_loss\n","            print_losses.append(mask_loss.item() * nTotal)\n","            n_totals += nTotal\n","    else:\n","        # Without teacher forcing: Use decoder's own predictions as the next input\n","        for t in range(max_target_len):\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n","            _, topi = decoder_output.topk(1)\n","            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]]).to(device)\n","            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n","            loss += mask_loss\n","            print_losses.append(mask_loss.item() * nTotal)\n","            n_totals += nTotal\n","\n","    # Perform backpropagation\n","    loss.backward()\n","\n","    # Clip gradients to avoid exploding gradients\n","    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n","    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n","\n","    # Update model weights\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return sum(print_losses) / n_totals\n"],"metadata":{"id":"T1idZb9MTGIy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **3.3 Training Loop**\n","\n","- **Objective**: Define the main training loop to run multiple training iterations, print progress updates, and save model checkpoints periodically.\n","\n","- **Explanation**:\n","  - The `trainIters` function manages the entire training process, including loading batches, running training iterations, printing progress, and saving the model at specified intervals.\n","  - For each iteration, a batch of data is processed by the `train` function, and the loss is accumulated.\n","  - The function prints average loss every `print_every` iterations to monitor training progress and saves a checkpoint every `save_every` iterations to preserve model state, which is essential for long training sessions or when training might be interrupted.\n","\n"],"metadata":{"id":"Yjjbz1x5SkrX"}},{"cell_type":"code","source":["def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n","    \"\"\"\n","    Run training iterations.\n","    Args:\n","        model_name: Name of the model.\n","        voc: Vocabulary object.\n","        pairs: List of question-answer pairs.\n","        encoder, decoder: Encoder and decoder models.\n","        encoder_optimizer, decoder_optimizer: Optimizers.\n","        embedding: Embedding layer.\n","        encoder_n_layers, decoder_n_layers: Number of layers in encoder and decoder.\n","        save_dir: Directory to save checkpoints.\n","        n_iteration: Number of iterations to run.\n","        batch_size: Batch size.\n","        print_every: Frequency of print statements.\n","        save_every: Frequency of model checkpoint saving.\n","        clip: Gradient clipping threshold.\n","    \"\"\"\n","    # Load batches for each iteration\n","    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n","                        for _ in range(n_iteration)]\n","\n","    # Initialize variables\n","    print('Initializing ...')\n","    start_iteration = 1\n","    print_loss = 0\n","    if loadFilename:\n","        start_iteration = checkpoint['iteration'] + 1\n","\n","    # Training loop\n","    print(\"Training...\")\n","    for iteration in range(start_iteration, n_iteration + 1):\n","        training_batch = training_batches[iteration - 1]\n","        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n","\n","        # Run a training iteration\n","        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n","                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n","        print_loss += loss\n","\n","        # Print progress\n","        if iteration % print_every == 0:\n","            print_loss_avg = print_loss / print_every\n","            print(f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; Average loss: {print_loss_avg:.4f}\")\n","            print_loss = 0\n","\n","        # Save checkpoint\n","        if (iteration % save_every == 0):\n","            directory = os.path.join(save_dir, model_name, corpus_name, f'{encoder_n_layers}-{decoder_n_layers}_{hidden_size}')\n","            if not os.path.exists(directory):\n","                os.makedirs(directory)\n","            torch.save({\n","                'iteration': iteration,\n","                'en': encoder.state_dict(),\n","                'de': decoder.state_dict(),\n","                'en_opt': encoder_optimizer.state_dict(),\n","                'de_opt': decoder_optimizer.state_dict(),\n","                'loss': loss,\n","                'voc_dict': voc.__dict__,\n","                'embedding': embedding.state_dict()\n","            }, os.path.join(directory, f'{iteration}_checkpoint.tar'))\n"],"metadata":{"id":"YOozC0xkTbeg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **3.4 Configuring and Running the Training**\n","\n","- **Objective**: Set up training parameters and optimizers, and then start the main training loop.\n","\n","- **Explanation**:\n","  - Key training parameters such as learning rate, number of iterations, gradient clipping threshold, and teacher forcing ratio are configured here. These parameters affect the training dynamics, model convergence, and overall performance.\n","  - Optimizers for the encoder and decoder are initialized, with the decoder learning rate scaled by a factor (`decoder_learning_ratio`) to allow faster learning in the decoder.\n","  - If a checkpoint (`loadFilename`) exists, the model can resume training from the last saved iteration.\n"],"metadata":{"id":"fffPs2lnSnDl"}},{"cell_type":"code","source":["# Configure models\n","model_name = 'cb_model'\n","attn_model = 'dot'\n","#``attn_model = 'general'``\n","#``attn_model = 'concat'``\n","hidden_size = 500\n","encoder_n_layers = 2\n","decoder_n_layers = 2\n","dropout = 0.1\n","batch_size = 64\n","\n","# Set checkpoint to load from; set to None if starting from scratch\n","loadFilename = None\n","checkpoint_iter = 4000"],"metadata":{"id":"rXH5IYzPVT-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model if a ``loadFilename`` is provided\n","if loadFilename:\n","    # If loading on same machine the model was trained on\n","    checkpoint = torch.load(loadFilename)\n","    # If loading a model trained on GPU to CPU\n","    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n","    encoder_sd = checkpoint['en']\n","    decoder_sd = checkpoint['de']\n","    encoder_optimizer_sd = checkpoint['en_opt']\n","    decoder_optimizer_sd = checkpoint['de_opt']\n","    embedding_sd = checkpoint['embedding']\n","    voc.__dict__ = checkpoint['voc_dict']\n","\n","\n","print('Building encoder and decoder ...')\n","# Initialize word embeddings\n","embedding = nn.Embedding(voc.num_words, hidden_size)\n","if loadFilename:\n","    embedding.load_state_dict(embedding_sd)\n","# Initialize encoder & decoder models\n","encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n","decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n","if loadFilename:\n","    encoder.load_state_dict(encoder_sd)\n","    decoder.load_state_dict(decoder_sd)\n","# Use appropriate device\n","encoder = encoder.to(device)\n","decoder = decoder.to(device)\n","print('Models built and ready to go!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTvc2gOYVBM-","executionInfo":{"status":"ok","timestamp":1731464398763,"user_tz":0,"elapsed":5,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"920a729c-f724-4b3c-cff4-dea80fb21903"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Building encoder and decoder ...\n","Models built and ready to go!\n"]}]},{"cell_type":"code","source":["# Configure training and optimization parameters\n","clip = 50.0  # Gradient clipping threshold to prevent exploding gradients\n","teacher_forcing_ratio = 1.0  # Probability of using teacher forcing for each decoding step\n","learning_rate = 0.0001  # Learning rate for the optimizer\n","decoder_learning_ratio = 5.0  # Multiplier for decoder's learning rate to speed up learning in the decoder\n","n_iteration = 4000  # Total number of training iterations\n","print_every = 1  # Print training progress every iteration\n","save_every = 500  # Save a checkpoint every 500 iterations\n","\n","# Ensure dropout layers are in training mode (required for layers like dropout and batchnorm)\n","encoder.train()\n","decoder.train()\n","\n","# Initialize optimizers for encoder and decoder\n","print('Building optimizers ...')\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)  # Adam optimizer for encoder\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)  # Adam optimizer for decoder with increased learning rate\n","\n","# If resuming from a checkpoint, load optimizer states\n","if loadFilename:\n","    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n","    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n","\n","# If CUDA is available, move optimizer state tensors to the GPU\n","# This is necessary if training on GPU to ensure all optimizer states are on the same device\n","for state in encoder_optimizer.state.values():\n","    for k, v in state.items():\n","        if isinstance(v, torch.Tensor):\n","            state[k] = v.cuda()\n","\n","for state in decoder_optimizer.state.values():\n","    for k, v in state.items():\n","        if isinstance(v, torch.Tensor):\n","            state[k] = v.cuda()\n","\n","# Define and create directory for saving checkpoints on Google Drive\n","save_dir = \"/content/drive/My Drive/Colab Notebooks/nlp_pro_babu/checkpoints\"\n","if not os.path.exists(save_dir):\n","    os.makedirs(save_dir)  # Create the directory if it doesn't exist\n","\n","# Start the training loop\n","print(\"Starting Training!\")\n","trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n","           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n","           print_every, save_every, clip, corpus_name, loadFilename)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uepsilz2T30A","executionInfo":{"status":"ok","timestamp":1731343121050,"user_tz":0,"elapsed":5982130,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"499ad0e0-a2ed-4cb1-ff33-5ecf3d77bc7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Building optimizers ...\n","Starting Training!\n","Initializing ...\n","Training...\n","Iteration: 1; Percent complete: 0.0%; Average loss: 8.9827\n","Iteration: 2; Percent complete: 0.1%; Average loss: 8.8688\n","Iteration: 3; Percent complete: 0.1%; Average loss: 8.7029\n","Iteration: 4; Percent complete: 0.1%; Average loss: 8.4000\n","Iteration: 5; Percent complete: 0.1%; Average loss: 8.0336\n","Iteration: 6; Percent complete: 0.1%; Average loss: 7.5086\n","Iteration: 7; Percent complete: 0.2%; Average loss: 7.0995\n","Iteration: 8; Percent complete: 0.2%; Average loss: 6.8496\n","Iteration: 9; Percent complete: 0.2%; Average loss: 6.7947\n","Iteration: 10; Percent complete: 0.2%; Average loss: 6.8786\n","Iteration: 11; Percent complete: 0.3%; Average loss: 6.3095\n","Iteration: 12; Percent complete: 0.3%; Average loss: 6.1585\n","Iteration: 13; Percent complete: 0.3%; Average loss: 5.6968\n","Iteration: 14; Percent complete: 0.4%; Average loss: 5.5459\n","Iteration: 15; Percent complete: 0.4%; Average loss: 5.5340\n","Iteration: 16; Percent complete: 0.4%; Average loss: 5.5340\n","Iteration: 17; Percent complete: 0.4%; Average loss: 5.2308\n","Iteration: 18; Percent complete: 0.4%; Average loss: 4.9796\n","Iteration: 19; Percent complete: 0.5%; Average loss: 5.3307\n","Iteration: 20; Percent complete: 0.5%; Average loss: 4.9056\n","Iteration: 21; Percent complete: 0.5%; Average loss: 4.9679\n","Iteration: 22; Percent complete: 0.5%; Average loss: 4.9365\n","Iteration: 23; Percent complete: 0.6%; Average loss: 4.8589\n","Iteration: 24; Percent complete: 0.6%; Average loss: 4.7397\n","Iteration: 25; Percent complete: 0.6%; Average loss: 4.7838\n","Iteration: 26; Percent complete: 0.7%; Average loss: 4.7140\n","Iteration: 27; Percent complete: 0.7%; Average loss: 4.7452\n","Iteration: 28; Percent complete: 0.7%; Average loss: 4.7647\n","Iteration: 29; Percent complete: 0.7%; Average loss: 4.9325\n","Iteration: 30; Percent complete: 0.8%; Average loss: 4.6691\n","Iteration: 31; Percent complete: 0.8%; Average loss: 4.8597\n","Iteration: 32; Percent complete: 0.8%; Average loss: 4.7142\n","Iteration: 33; Percent complete: 0.8%; Average loss: 4.6600\n","Iteration: 34; Percent complete: 0.9%; Average loss: 4.7483\n","Iteration: 35; Percent complete: 0.9%; Average loss: 4.7484\n","Iteration: 36; Percent complete: 0.9%; Average loss: 4.7043\n","Iteration: 37; Percent complete: 0.9%; Average loss: 4.7240\n","Iteration: 38; Percent complete: 0.9%; Average loss: 4.8297\n","Iteration: 39; Percent complete: 1.0%; Average loss: 4.7787\n","Iteration: 40; Percent complete: 1.0%; Average loss: 4.6608\n","Iteration: 41; Percent complete: 1.0%; Average loss: 4.8525\n","Iteration: 42; Percent complete: 1.1%; Average loss: 4.6556\n","Iteration: 43; Percent complete: 1.1%; Average loss: 4.5233\n","Iteration: 44; Percent complete: 1.1%; Average loss: 4.6840\n","Iteration: 45; Percent complete: 1.1%; Average loss: 4.6281\n","Iteration: 46; Percent complete: 1.1%; Average loss: 4.8151\n","Iteration: 47; Percent complete: 1.2%; Average loss: 4.7672\n","Iteration: 48; Percent complete: 1.2%; Average loss: 4.3948\n","Iteration: 49; Percent complete: 1.2%; Average loss: 4.4690\n","Iteration: 50; Percent complete: 1.2%; Average loss: 4.5241\n","Iteration: 51; Percent complete: 1.3%; Average loss: 4.6553\n","Iteration: 52; Percent complete: 1.3%; Average loss: 4.7219\n","Iteration: 53; Percent complete: 1.3%; Average loss: 4.6875\n","Iteration: 54; Percent complete: 1.4%; Average loss: 4.4885\n","Iteration: 55; Percent complete: 1.4%; Average loss: 4.6175\n","Iteration: 56; Percent complete: 1.4%; Average loss: 4.4755\n","Iteration: 57; Percent complete: 1.4%; Average loss: 4.7187\n","Iteration: 58; Percent complete: 1.5%; Average loss: 4.6555\n","Iteration: 59; Percent complete: 1.5%; Average loss: 4.4964\n","Iteration: 60; Percent complete: 1.5%; Average loss: 4.6998\n","Iteration: 61; Percent complete: 1.5%; Average loss: 4.5943\n","Iteration: 62; Percent complete: 1.6%; Average loss: 4.3498\n","Iteration: 63; Percent complete: 1.6%; Average loss: 4.6807\n","Iteration: 64; Percent complete: 1.6%; Average loss: 4.6447\n","Iteration: 65; Percent complete: 1.6%; Average loss: 4.7148\n","Iteration: 66; Percent complete: 1.7%; Average loss: 4.6878\n","Iteration: 67; Percent complete: 1.7%; Average loss: 4.6999\n","Iteration: 68; Percent complete: 1.7%; Average loss: 4.5685\n","Iteration: 69; Percent complete: 1.7%; Average loss: 4.5203\n","Iteration: 70; Percent complete: 1.8%; Average loss: 4.7016\n","Iteration: 71; Percent complete: 1.8%; Average loss: 4.4798\n","Iteration: 72; Percent complete: 1.8%; Average loss: 4.3020\n","Iteration: 73; Percent complete: 1.8%; Average loss: 4.4883\n","Iteration: 74; Percent complete: 1.8%; Average loss: 4.2263\n","Iteration: 75; Percent complete: 1.9%; Average loss: 4.4996\n","Iteration: 76; Percent complete: 1.9%; Average loss: 4.7013\n","Iteration: 77; Percent complete: 1.9%; Average loss: 4.2864\n","Iteration: 78; Percent complete: 1.9%; Average loss: 4.4523\n","Iteration: 79; Percent complete: 2.0%; Average loss: 4.6700\n","Iteration: 80; Percent complete: 2.0%; Average loss: 4.5231\n","Iteration: 81; Percent complete: 2.0%; Average loss: 4.2581\n","Iteration: 82; Percent complete: 2.1%; Average loss: 4.4982\n","Iteration: 83; Percent complete: 2.1%; Average loss: 4.3978\n","Iteration: 84; Percent complete: 2.1%; Average loss: 4.4842\n","Iteration: 85; Percent complete: 2.1%; Average loss: 4.3506\n","Iteration: 86; Percent complete: 2.1%; Average loss: 4.5128\n","Iteration: 87; Percent complete: 2.2%; Average loss: 4.6169\n","Iteration: 88; Percent complete: 2.2%; Average loss: 4.4616\n","Iteration: 89; Percent complete: 2.2%; Average loss: 4.6826\n","Iteration: 90; Percent complete: 2.2%; Average loss: 4.4740\n","Iteration: 91; Percent complete: 2.3%; Average loss: 4.4821\n","Iteration: 92; Percent complete: 2.3%; Average loss: 4.3013\n","Iteration: 93; Percent complete: 2.3%; Average loss: 4.5187\n","Iteration: 94; Percent complete: 2.4%; Average loss: 4.4148\n","Iteration: 95; Percent complete: 2.4%; Average loss: 4.3458\n","Iteration: 96; Percent complete: 2.4%; Average loss: 4.0864\n","Iteration: 97; Percent complete: 2.4%; Average loss: 4.5380\n","Iteration: 98; Percent complete: 2.5%; Average loss: 4.2253\n","Iteration: 99; Percent complete: 2.5%; Average loss: 4.2502\n","Iteration: 100; Percent complete: 2.5%; Average loss: 4.2372\n","Iteration: 101; Percent complete: 2.5%; Average loss: 4.3270\n","Iteration: 102; Percent complete: 2.5%; Average loss: 4.3133\n","Iteration: 103; Percent complete: 2.6%; Average loss: 4.5093\n","Iteration: 104; Percent complete: 2.6%; Average loss: 4.1919\n","Iteration: 105; Percent complete: 2.6%; Average loss: 4.3793\n","Iteration: 106; Percent complete: 2.6%; Average loss: 4.4660\n","Iteration: 107; Percent complete: 2.7%; Average loss: 4.4400\n","Iteration: 108; Percent complete: 2.7%; Average loss: 4.3666\n","Iteration: 109; Percent complete: 2.7%; Average loss: 4.4047\n","Iteration: 110; Percent complete: 2.8%; Average loss: 4.3042\n","Iteration: 111; Percent complete: 2.8%; Average loss: 4.4014\n","Iteration: 112; Percent complete: 2.8%; Average loss: 4.3880\n","Iteration: 113; Percent complete: 2.8%; Average loss: 4.1329\n","Iteration: 114; Percent complete: 2.9%; Average loss: 4.3789\n","Iteration: 115; Percent complete: 2.9%; Average loss: 4.3366\n","Iteration: 116; Percent complete: 2.9%; Average loss: 4.3424\n","Iteration: 117; Percent complete: 2.9%; Average loss: 4.2567\n","Iteration: 118; Percent complete: 2.9%; Average loss: 4.2706\n","Iteration: 119; Percent complete: 3.0%; Average loss: 4.4202\n","Iteration: 120; Percent complete: 3.0%; Average loss: 4.0751\n","Iteration: 121; Percent complete: 3.0%; Average loss: 4.1872\n","Iteration: 122; Percent complete: 3.0%; Average loss: 4.3316\n","Iteration: 123; Percent complete: 3.1%; Average loss: 4.4278\n","Iteration: 124; Percent complete: 3.1%; Average loss: 4.1945\n","Iteration: 125; Percent complete: 3.1%; Average loss: 4.4505\n","Iteration: 126; Percent complete: 3.1%; Average loss: 4.0107\n","Iteration: 127; Percent complete: 3.2%; Average loss: 4.3893\n","Iteration: 128; Percent complete: 3.2%; Average loss: 4.4992\n","Iteration: 129; Percent complete: 3.2%; Average loss: 4.2410\n","Iteration: 130; Percent complete: 3.2%; Average loss: 4.2416\n","Iteration: 131; Percent complete: 3.3%; Average loss: 4.4450\n","Iteration: 132; Percent complete: 3.3%; Average loss: 4.4353\n","Iteration: 133; Percent complete: 3.3%; Average loss: 4.3988\n","Iteration: 134; Percent complete: 3.4%; Average loss: 4.2340\n","Iteration: 135; Percent complete: 3.4%; Average loss: 4.0927\n","Iteration: 136; Percent complete: 3.4%; Average loss: 4.6662\n","Iteration: 137; Percent complete: 3.4%; Average loss: 4.2396\n","Iteration: 138; Percent complete: 3.5%; Average loss: 4.3741\n","Iteration: 139; Percent complete: 3.5%; Average loss: 4.4114\n","Iteration: 140; Percent complete: 3.5%; Average loss: 4.5398\n","Iteration: 141; Percent complete: 3.5%; Average loss: 3.9901\n","Iteration: 142; Percent complete: 3.5%; Average loss: 3.9483\n","Iteration: 143; Percent complete: 3.6%; Average loss: 4.4540\n","Iteration: 144; Percent complete: 3.6%; Average loss: 4.4021\n","Iteration: 145; Percent complete: 3.6%; Average loss: 4.2934\n","Iteration: 146; Percent complete: 3.6%; Average loss: 4.3230\n","Iteration: 147; Percent complete: 3.7%; Average loss: 4.1319\n","Iteration: 148; Percent complete: 3.7%; Average loss: 4.1679\n","Iteration: 149; Percent complete: 3.7%; Average loss: 4.1716\n","Iteration: 150; Percent complete: 3.8%; Average loss: 4.3961\n","Iteration: 151; Percent complete: 3.8%; Average loss: 4.0483\n","Iteration: 152; Percent complete: 3.8%; Average loss: 4.2395\n","Iteration: 153; Percent complete: 3.8%; Average loss: 4.3021\n","Iteration: 154; Percent complete: 3.9%; Average loss: 4.3486\n","Iteration: 155; Percent complete: 3.9%; Average loss: 4.5857\n","Iteration: 156; Percent complete: 3.9%; Average loss: 4.2753\n","Iteration: 157; Percent complete: 3.9%; Average loss: 4.2508\n","Iteration: 158; Percent complete: 4.0%; Average loss: 4.3081\n","Iteration: 159; Percent complete: 4.0%; Average loss: 4.1715\n","Iteration: 160; Percent complete: 4.0%; Average loss: 4.3765\n","Iteration: 161; Percent complete: 4.0%; Average loss: 4.0975\n","Iteration: 162; Percent complete: 4.0%; Average loss: 3.9270\n","Iteration: 163; Percent complete: 4.1%; Average loss: 4.0574\n","Iteration: 164; Percent complete: 4.1%; Average loss: 4.0260\n","Iteration: 165; Percent complete: 4.1%; Average loss: 4.1139\n","Iteration: 166; Percent complete: 4.2%; Average loss: 4.2763\n","Iteration: 167; Percent complete: 4.2%; Average loss: 3.9073\n","Iteration: 168; Percent complete: 4.2%; Average loss: 4.3201\n","Iteration: 169; Percent complete: 4.2%; Average loss: 4.0267\n","Iteration: 170; Percent complete: 4.2%; Average loss: 3.9396\n","Iteration: 171; Percent complete: 4.3%; Average loss: 3.9764\n","Iteration: 172; Percent complete: 4.3%; Average loss: 4.2736\n","Iteration: 173; Percent complete: 4.3%; Average loss: 4.1262\n","Iteration: 174; Percent complete: 4.3%; Average loss: 4.0829\n","Iteration: 175; Percent complete: 4.4%; Average loss: 4.3333\n","Iteration: 176; Percent complete: 4.4%; Average loss: 4.1166\n","Iteration: 177; Percent complete: 4.4%; Average loss: 4.2112\n","Iteration: 178; Percent complete: 4.5%; Average loss: 4.0966\n","Iteration: 179; Percent complete: 4.5%; Average loss: 4.4625\n","Iteration: 180; Percent complete: 4.5%; Average loss: 4.1952\n","Iteration: 181; Percent complete: 4.5%; Average loss: 4.2888\n","Iteration: 182; Percent complete: 4.5%; Average loss: 4.2179\n","Iteration: 183; Percent complete: 4.6%; Average loss: 4.3390\n","Iteration: 184; Percent complete: 4.6%; Average loss: 4.2497\n","Iteration: 185; Percent complete: 4.6%; Average loss: 4.2970\n","Iteration: 186; Percent complete: 4.7%; Average loss: 4.3313\n","Iteration: 187; Percent complete: 4.7%; Average loss: 4.2328\n","Iteration: 188; Percent complete: 4.7%; Average loss: 3.7482\n","Iteration: 189; Percent complete: 4.7%; Average loss: 4.0378\n","Iteration: 190; Percent complete: 4.8%; Average loss: 4.1240\n","Iteration: 191; Percent complete: 4.8%; Average loss: 4.1675\n","Iteration: 192; Percent complete: 4.8%; Average loss: 4.1598\n","Iteration: 193; Percent complete: 4.8%; Average loss: 4.3742\n","Iteration: 194; Percent complete: 4.9%; Average loss: 3.9076\n","Iteration: 195; Percent complete: 4.9%; Average loss: 4.0538\n","Iteration: 196; Percent complete: 4.9%; Average loss: 4.0962\n","Iteration: 197; Percent complete: 4.9%; Average loss: 4.1549\n","Iteration: 198; Percent complete: 5.0%; Average loss: 4.1427\n","Iteration: 199; Percent complete: 5.0%; Average loss: 4.1779\n","Iteration: 200; Percent complete: 5.0%; Average loss: 3.8070\n","Iteration: 201; Percent complete: 5.0%; Average loss: 4.0990\n","Iteration: 202; Percent complete: 5.1%; Average loss: 4.3162\n","Iteration: 203; Percent complete: 5.1%; Average loss: 4.2769\n","Iteration: 204; Percent complete: 5.1%; Average loss: 4.1300\n","Iteration: 205; Percent complete: 5.1%; Average loss: 4.0626\n","Iteration: 206; Percent complete: 5.1%; Average loss: 4.1341\n","Iteration: 207; Percent complete: 5.2%; Average loss: 4.0584\n","Iteration: 208; Percent complete: 5.2%; Average loss: 4.1267\n","Iteration: 209; Percent complete: 5.2%; Average loss: 4.0498\n","Iteration: 210; Percent complete: 5.2%; Average loss: 3.9506\n","Iteration: 211; Percent complete: 5.3%; Average loss: 4.1121\n","Iteration: 212; Percent complete: 5.3%; Average loss: 3.8602\n","Iteration: 213; Percent complete: 5.3%; Average loss: 4.0061\n","Iteration: 214; Percent complete: 5.3%; Average loss: 3.7097\n","Iteration: 215; Percent complete: 5.4%; Average loss: 4.3425\n","Iteration: 216; Percent complete: 5.4%; Average loss: 3.9544\n","Iteration: 217; Percent complete: 5.4%; Average loss: 4.2553\n","Iteration: 218; Percent complete: 5.5%; Average loss: 3.8754\n","Iteration: 219; Percent complete: 5.5%; Average loss: 4.3075\n","Iteration: 220; Percent complete: 5.5%; Average loss: 3.9906\n","Iteration: 221; Percent complete: 5.5%; Average loss: 3.7933\n","Iteration: 222; Percent complete: 5.5%; Average loss: 4.0218\n","Iteration: 223; Percent complete: 5.6%; Average loss: 3.9599\n","Iteration: 224; Percent complete: 5.6%; Average loss: 4.0349\n","Iteration: 225; Percent complete: 5.6%; Average loss: 4.1754\n","Iteration: 226; Percent complete: 5.7%; Average loss: 4.1578\n","Iteration: 227; Percent complete: 5.7%; Average loss: 4.2515\n","Iteration: 228; Percent complete: 5.7%; Average loss: 3.8856\n","Iteration: 229; Percent complete: 5.7%; Average loss: 3.9058\n","Iteration: 230; Percent complete: 5.8%; Average loss: 4.0820\n","Iteration: 231; Percent complete: 5.8%; Average loss: 3.4879\n","Iteration: 232; Percent complete: 5.8%; Average loss: 3.8586\n","Iteration: 233; Percent complete: 5.8%; Average loss: 4.0423\n","Iteration: 234; Percent complete: 5.9%; Average loss: 3.9917\n","Iteration: 235; Percent complete: 5.9%; Average loss: 4.1921\n","Iteration: 236; Percent complete: 5.9%; Average loss: 4.1338\n","Iteration: 237; Percent complete: 5.9%; Average loss: 3.9402\n","Iteration: 238; Percent complete: 5.9%; Average loss: 3.9821\n","Iteration: 239; Percent complete: 6.0%; Average loss: 4.0120\n","Iteration: 240; Percent complete: 6.0%; Average loss: 4.1347\n","Iteration: 241; Percent complete: 6.0%; Average loss: 3.8130\n","Iteration: 242; Percent complete: 6.0%; Average loss: 3.9096\n","Iteration: 243; Percent complete: 6.1%; Average loss: 4.2222\n","Iteration: 244; Percent complete: 6.1%; Average loss: 3.7899\n","Iteration: 245; Percent complete: 6.1%; Average loss: 4.1826\n","Iteration: 246; Percent complete: 6.2%; Average loss: 4.1096\n","Iteration: 247; Percent complete: 6.2%; Average loss: 3.9279\n","Iteration: 248; Percent complete: 6.2%; Average loss: 4.0739\n","Iteration: 249; Percent complete: 6.2%; Average loss: 3.9642\n","Iteration: 250; Percent complete: 6.2%; Average loss: 4.0888\n","Iteration: 251; Percent complete: 6.3%; Average loss: 4.0554\n","Iteration: 252; Percent complete: 6.3%; Average loss: 4.0582\n","Iteration: 253; Percent complete: 6.3%; Average loss: 4.0169\n","Iteration: 254; Percent complete: 6.3%; Average loss: 4.1373\n","Iteration: 255; Percent complete: 6.4%; Average loss: 3.9732\n","Iteration: 256; Percent complete: 6.4%; Average loss: 3.7278\n","Iteration: 257; Percent complete: 6.4%; Average loss: 4.1141\n","Iteration: 258; Percent complete: 6.5%; Average loss: 3.9651\n","Iteration: 259; Percent complete: 6.5%; Average loss: 3.8236\n","Iteration: 260; Percent complete: 6.5%; Average loss: 3.7632\n","Iteration: 261; Percent complete: 6.5%; Average loss: 4.0568\n","Iteration: 262; Percent complete: 6.6%; Average loss: 4.1061\n","Iteration: 263; Percent complete: 6.6%; Average loss: 4.0642\n","Iteration: 264; Percent complete: 6.6%; Average loss: 3.7636\n","Iteration: 265; Percent complete: 6.6%; Average loss: 3.7642\n","Iteration: 266; Percent complete: 6.7%; Average loss: 3.9455\n","Iteration: 267; Percent complete: 6.7%; Average loss: 3.8409\n","Iteration: 268; Percent complete: 6.7%; Average loss: 3.7515\n","Iteration: 269; Percent complete: 6.7%; Average loss: 4.1316\n","Iteration: 270; Percent complete: 6.8%; Average loss: 4.0801\n","Iteration: 271; Percent complete: 6.8%; Average loss: 4.0156\n","Iteration: 272; Percent complete: 6.8%; Average loss: 4.0734\n","Iteration: 273; Percent complete: 6.8%; Average loss: 3.8237\n","Iteration: 274; Percent complete: 6.9%; Average loss: 4.0586\n","Iteration: 275; Percent complete: 6.9%; Average loss: 3.9673\n","Iteration: 276; Percent complete: 6.9%; Average loss: 3.9295\n","Iteration: 277; Percent complete: 6.9%; Average loss: 3.8093\n","Iteration: 278; Percent complete: 7.0%; Average loss: 3.9064\n","Iteration: 279; Percent complete: 7.0%; Average loss: 4.3575\n","Iteration: 280; Percent complete: 7.0%; Average loss: 3.9990\n","Iteration: 281; Percent complete: 7.0%; Average loss: 3.8552\n","Iteration: 282; Percent complete: 7.0%; Average loss: 3.9092\n","Iteration: 283; Percent complete: 7.1%; Average loss: 3.8734\n","Iteration: 284; Percent complete: 7.1%; Average loss: 4.1793\n","Iteration: 285; Percent complete: 7.1%; Average loss: 4.0531\n","Iteration: 286; Percent complete: 7.1%; Average loss: 3.7984\n","Iteration: 287; Percent complete: 7.2%; Average loss: 3.9110\n","Iteration: 288; Percent complete: 7.2%; Average loss: 3.8148\n","Iteration: 289; Percent complete: 7.2%; Average loss: 3.9234\n","Iteration: 290; Percent complete: 7.2%; Average loss: 3.8315\n","Iteration: 291; Percent complete: 7.3%; Average loss: 4.1073\n","Iteration: 292; Percent complete: 7.3%; Average loss: 4.2416\n","Iteration: 293; Percent complete: 7.3%; Average loss: 3.9494\n","Iteration: 294; Percent complete: 7.3%; Average loss: 3.8260\n","Iteration: 295; Percent complete: 7.4%; Average loss: 3.8019\n","Iteration: 296; Percent complete: 7.4%; Average loss: 4.0563\n","Iteration: 297; Percent complete: 7.4%; Average loss: 3.7333\n","Iteration: 298; Percent complete: 7.4%; Average loss: 3.8998\n","Iteration: 299; Percent complete: 7.5%; Average loss: 3.9099\n","Iteration: 300; Percent complete: 7.5%; Average loss: 4.0900\n","Iteration: 301; Percent complete: 7.5%; Average loss: 3.6523\n","Iteration: 302; Percent complete: 7.5%; Average loss: 3.9390\n","Iteration: 303; Percent complete: 7.6%; Average loss: 4.0466\n","Iteration: 304; Percent complete: 7.6%; Average loss: 3.9134\n","Iteration: 305; Percent complete: 7.6%; Average loss: 3.7720\n","Iteration: 306; Percent complete: 7.6%; Average loss: 4.2051\n","Iteration: 307; Percent complete: 7.7%; Average loss: 3.9496\n","Iteration: 308; Percent complete: 7.7%; Average loss: 3.9577\n","Iteration: 309; Percent complete: 7.7%; Average loss: 4.0279\n","Iteration: 310; Percent complete: 7.8%; Average loss: 3.7703\n","Iteration: 311; Percent complete: 7.8%; Average loss: 4.1970\n","Iteration: 312; Percent complete: 7.8%; Average loss: 3.9054\n","Iteration: 313; Percent complete: 7.8%; Average loss: 3.9747\n","Iteration: 314; Percent complete: 7.8%; Average loss: 3.8121\n","Iteration: 315; Percent complete: 7.9%; Average loss: 3.8258\n","Iteration: 316; Percent complete: 7.9%; Average loss: 4.0525\n","Iteration: 317; Percent complete: 7.9%; Average loss: 3.9743\n","Iteration: 318; Percent complete: 8.0%; Average loss: 3.9728\n","Iteration: 319; Percent complete: 8.0%; Average loss: 3.8926\n","Iteration: 320; Percent complete: 8.0%; Average loss: 4.0162\n","Iteration: 321; Percent complete: 8.0%; Average loss: 3.7425\n","Iteration: 322; Percent complete: 8.1%; Average loss: 3.6708\n","Iteration: 323; Percent complete: 8.1%; Average loss: 3.8932\n","Iteration: 324; Percent complete: 8.1%; Average loss: 4.1304\n","Iteration: 325; Percent complete: 8.1%; Average loss: 3.8142\n","Iteration: 326; Percent complete: 8.2%; Average loss: 4.0362\n","Iteration: 327; Percent complete: 8.2%; Average loss: 4.0809\n","Iteration: 328; Percent complete: 8.2%; Average loss: 3.7678\n","Iteration: 329; Percent complete: 8.2%; Average loss: 3.6901\n","Iteration: 330; Percent complete: 8.2%; Average loss: 3.9372\n","Iteration: 331; Percent complete: 8.3%; Average loss: 3.7044\n","Iteration: 332; Percent complete: 8.3%; Average loss: 3.9134\n","Iteration: 333; Percent complete: 8.3%; Average loss: 3.8038\n","Iteration: 334; Percent complete: 8.3%; Average loss: 3.5938\n","Iteration: 335; Percent complete: 8.4%; Average loss: 3.9061\n","Iteration: 336; Percent complete: 8.4%; Average loss: 3.9457\n","Iteration: 337; Percent complete: 8.4%; Average loss: 3.8367\n","Iteration: 338; Percent complete: 8.5%; Average loss: 3.7407\n","Iteration: 339; Percent complete: 8.5%; Average loss: 3.9887\n","Iteration: 340; Percent complete: 8.5%; Average loss: 3.8915\n","Iteration: 341; Percent complete: 8.5%; Average loss: 3.7987\n","Iteration: 342; Percent complete: 8.6%; Average loss: 3.9153\n","Iteration: 343; Percent complete: 8.6%; Average loss: 3.9009\n","Iteration: 344; Percent complete: 8.6%; Average loss: 3.9318\n","Iteration: 345; Percent complete: 8.6%; Average loss: 3.8570\n","Iteration: 346; Percent complete: 8.6%; Average loss: 3.8273\n","Iteration: 347; Percent complete: 8.7%; Average loss: 3.8536\n","Iteration: 348; Percent complete: 8.7%; Average loss: 4.1051\n","Iteration: 349; Percent complete: 8.7%; Average loss: 3.8052\n","Iteration: 350; Percent complete: 8.8%; Average loss: 3.7917\n","Iteration: 351; Percent complete: 8.8%; Average loss: 4.0763\n","Iteration: 352; Percent complete: 8.8%; Average loss: 3.9020\n","Iteration: 353; Percent complete: 8.8%; Average loss: 3.7989\n","Iteration: 354; Percent complete: 8.8%; Average loss: 3.9901\n","Iteration: 355; Percent complete: 8.9%; Average loss: 3.7993\n","Iteration: 356; Percent complete: 8.9%; Average loss: 3.8679\n","Iteration: 357; Percent complete: 8.9%; Average loss: 3.7115\n","Iteration: 358; Percent complete: 8.9%; Average loss: 3.7281\n","Iteration: 359; Percent complete: 9.0%; Average loss: 3.9437\n","Iteration: 360; Percent complete: 9.0%; Average loss: 4.0883\n","Iteration: 361; Percent complete: 9.0%; Average loss: 3.8626\n","Iteration: 362; Percent complete: 9.0%; Average loss: 4.0164\n","Iteration: 363; Percent complete: 9.1%; Average loss: 4.0437\n","Iteration: 364; Percent complete: 9.1%; Average loss: 3.8901\n","Iteration: 365; Percent complete: 9.1%; Average loss: 3.7972\n","Iteration: 366; Percent complete: 9.2%; Average loss: 3.8128\n","Iteration: 367; Percent complete: 9.2%; Average loss: 4.0405\n","Iteration: 368; Percent complete: 9.2%; Average loss: 3.7665\n","Iteration: 369; Percent complete: 9.2%; Average loss: 3.7441\n","Iteration: 370; Percent complete: 9.2%; Average loss: 3.5980\n","Iteration: 371; Percent complete: 9.3%; Average loss: 3.9086\n","Iteration: 372; Percent complete: 9.3%; Average loss: 3.9527\n","Iteration: 373; Percent complete: 9.3%; Average loss: 3.9662\n","Iteration: 374; Percent complete: 9.3%; Average loss: 3.7690\n","Iteration: 375; Percent complete: 9.4%; Average loss: 3.9407\n","Iteration: 376; Percent complete: 9.4%; Average loss: 3.9229\n","Iteration: 377; Percent complete: 9.4%; Average loss: 3.5232\n","Iteration: 378; Percent complete: 9.4%; Average loss: 3.8076\n","Iteration: 379; Percent complete: 9.5%; Average loss: 3.7192\n","Iteration: 380; Percent complete: 9.5%; Average loss: 3.9142\n","Iteration: 381; Percent complete: 9.5%; Average loss: 3.7442\n","Iteration: 382; Percent complete: 9.6%; Average loss: 3.5929\n","Iteration: 383; Percent complete: 9.6%; Average loss: 3.5545\n","Iteration: 384; Percent complete: 9.6%; Average loss: 4.0683\n","Iteration: 385; Percent complete: 9.6%; Average loss: 3.9090\n","Iteration: 386; Percent complete: 9.7%; Average loss: 3.8836\n","Iteration: 387; Percent complete: 9.7%; Average loss: 4.0495\n","Iteration: 388; Percent complete: 9.7%; Average loss: 3.9112\n","Iteration: 389; Percent complete: 9.7%; Average loss: 3.9253\n","Iteration: 390; Percent complete: 9.8%; Average loss: 3.8001\n","Iteration: 391; Percent complete: 9.8%; Average loss: 3.7451\n","Iteration: 392; Percent complete: 9.8%; Average loss: 3.6274\n","Iteration: 393; Percent complete: 9.8%; Average loss: 3.8085\n","Iteration: 394; Percent complete: 9.8%; Average loss: 4.0851\n","Iteration: 395; Percent complete: 9.9%; Average loss: 3.6734\n","Iteration: 396; Percent complete: 9.9%; Average loss: 3.7871\n","Iteration: 397; Percent complete: 9.9%; Average loss: 3.8896\n","Iteration: 398; Percent complete: 10.0%; Average loss: 3.8487\n","Iteration: 399; Percent complete: 10.0%; Average loss: 3.9104\n","Iteration: 400; Percent complete: 10.0%; Average loss: 3.8643\n","Iteration: 401; Percent complete: 10.0%; Average loss: 3.6811\n","Iteration: 402; Percent complete: 10.1%; Average loss: 3.7108\n","Iteration: 403; Percent complete: 10.1%; Average loss: 4.0011\n","Iteration: 404; Percent complete: 10.1%; Average loss: 3.6396\n","Iteration: 405; Percent complete: 10.1%; Average loss: 4.1547\n","Iteration: 406; Percent complete: 10.2%; Average loss: 3.8160\n","Iteration: 407; Percent complete: 10.2%; Average loss: 3.9959\n","Iteration: 408; Percent complete: 10.2%; Average loss: 3.8583\n","Iteration: 409; Percent complete: 10.2%; Average loss: 3.9885\n","Iteration: 410; Percent complete: 10.2%; Average loss: 3.5428\n","Iteration: 411; Percent complete: 10.3%; Average loss: 3.8554\n","Iteration: 412; Percent complete: 10.3%; Average loss: 3.8589\n","Iteration: 413; Percent complete: 10.3%; Average loss: 3.9104\n","Iteration: 414; Percent complete: 10.3%; Average loss: 4.0718\n","Iteration: 415; Percent complete: 10.4%; Average loss: 3.6916\n","Iteration: 416; Percent complete: 10.4%; Average loss: 3.8485\n","Iteration: 417; Percent complete: 10.4%; Average loss: 4.0309\n","Iteration: 418; Percent complete: 10.4%; Average loss: 3.8175\n","Iteration: 419; Percent complete: 10.5%; Average loss: 3.7126\n","Iteration: 420; Percent complete: 10.5%; Average loss: 3.6560\n","Iteration: 421; Percent complete: 10.5%; Average loss: 3.8991\n","Iteration: 422; Percent complete: 10.5%; Average loss: 3.9384\n","Iteration: 423; Percent complete: 10.6%; Average loss: 3.7591\n","Iteration: 424; Percent complete: 10.6%; Average loss: 3.6587\n","Iteration: 425; Percent complete: 10.6%; Average loss: 3.9213\n","Iteration: 426; Percent complete: 10.7%; Average loss: 3.7609\n","Iteration: 427; Percent complete: 10.7%; Average loss: 4.0766\n","Iteration: 428; Percent complete: 10.7%; Average loss: 3.7655\n","Iteration: 429; Percent complete: 10.7%; Average loss: 3.6143\n","Iteration: 430; Percent complete: 10.8%; Average loss: 3.6967\n","Iteration: 431; Percent complete: 10.8%; Average loss: 3.7667\n","Iteration: 432; Percent complete: 10.8%; Average loss: 3.7584\n","Iteration: 433; Percent complete: 10.8%; Average loss: 3.6631\n","Iteration: 434; Percent complete: 10.8%; Average loss: 3.9616\n","Iteration: 435; Percent complete: 10.9%; Average loss: 3.6922\n","Iteration: 436; Percent complete: 10.9%; Average loss: 3.8929\n","Iteration: 437; Percent complete: 10.9%; Average loss: 3.6387\n","Iteration: 438; Percent complete: 10.9%; Average loss: 3.6481\n","Iteration: 439; Percent complete: 11.0%; Average loss: 3.7336\n","Iteration: 440; Percent complete: 11.0%; Average loss: 3.6993\n","Iteration: 441; Percent complete: 11.0%; Average loss: 3.7238\n","Iteration: 442; Percent complete: 11.1%; Average loss: 3.8895\n","Iteration: 443; Percent complete: 11.1%; Average loss: 3.9573\n","Iteration: 444; Percent complete: 11.1%; Average loss: 3.4333\n","Iteration: 445; Percent complete: 11.1%; Average loss: 3.7472\n","Iteration: 446; Percent complete: 11.2%; Average loss: 3.6847\n","Iteration: 447; Percent complete: 11.2%; Average loss: 3.8874\n","Iteration: 448; Percent complete: 11.2%; Average loss: 3.8121\n","Iteration: 449; Percent complete: 11.2%; Average loss: 3.6864\n","Iteration: 450; Percent complete: 11.2%; Average loss: 3.7439\n","Iteration: 451; Percent complete: 11.3%; Average loss: 3.6798\n","Iteration: 452; Percent complete: 11.3%; Average loss: 3.5205\n","Iteration: 453; Percent complete: 11.3%; Average loss: 3.8301\n","Iteration: 454; Percent complete: 11.3%; Average loss: 3.8723\n","Iteration: 455; Percent complete: 11.4%; Average loss: 3.9936\n","Iteration: 456; Percent complete: 11.4%; Average loss: 3.9320\n","Iteration: 457; Percent complete: 11.4%; Average loss: 3.6953\n","Iteration: 458; Percent complete: 11.5%; Average loss: 3.7482\n","Iteration: 459; Percent complete: 11.5%; Average loss: 3.6513\n","Iteration: 460; Percent complete: 11.5%; Average loss: 3.7050\n","Iteration: 461; Percent complete: 11.5%; Average loss: 3.7112\n","Iteration: 462; Percent complete: 11.6%; Average loss: 3.9061\n","Iteration: 463; Percent complete: 11.6%; Average loss: 3.7415\n","Iteration: 464; Percent complete: 11.6%; Average loss: 3.9377\n","Iteration: 465; Percent complete: 11.6%; Average loss: 3.7222\n","Iteration: 466; Percent complete: 11.7%; Average loss: 3.8956\n","Iteration: 467; Percent complete: 11.7%; Average loss: 3.9223\n","Iteration: 468; Percent complete: 11.7%; Average loss: 3.4331\n","Iteration: 469; Percent complete: 11.7%; Average loss: 3.6779\n","Iteration: 470; Percent complete: 11.8%; Average loss: 3.8628\n","Iteration: 471; Percent complete: 11.8%; Average loss: 3.7342\n","Iteration: 472; Percent complete: 11.8%; Average loss: 4.0094\n","Iteration: 473; Percent complete: 11.8%; Average loss: 3.5867\n","Iteration: 474; Percent complete: 11.8%; Average loss: 3.8038\n","Iteration: 475; Percent complete: 11.9%; Average loss: 3.5936\n","Iteration: 476; Percent complete: 11.9%; Average loss: 3.7999\n","Iteration: 477; Percent complete: 11.9%; Average loss: 3.6866\n","Iteration: 478; Percent complete: 11.9%; Average loss: 4.0773\n","Iteration: 479; Percent complete: 12.0%; Average loss: 4.0023\n","Iteration: 480; Percent complete: 12.0%; Average loss: 3.7014\n","Iteration: 481; Percent complete: 12.0%; Average loss: 3.9610\n","Iteration: 482; Percent complete: 12.0%; Average loss: 3.8653\n","Iteration: 483; Percent complete: 12.1%; Average loss: 3.6304\n","Iteration: 484; Percent complete: 12.1%; Average loss: 3.4387\n","Iteration: 485; Percent complete: 12.1%; Average loss: 3.8582\n","Iteration: 486; Percent complete: 12.2%; Average loss: 3.7194\n","Iteration: 487; Percent complete: 12.2%; Average loss: 3.4391\n","Iteration: 488; Percent complete: 12.2%; Average loss: 3.4933\n","Iteration: 489; Percent complete: 12.2%; Average loss: 3.4673\n","Iteration: 490; Percent complete: 12.2%; Average loss: 3.6413\n","Iteration: 491; Percent complete: 12.3%; Average loss: 3.9206\n","Iteration: 492; Percent complete: 12.3%; Average loss: 3.9648\n","Iteration: 493; Percent complete: 12.3%; Average loss: 3.6438\n","Iteration: 494; Percent complete: 12.3%; Average loss: 3.7141\n","Iteration: 495; Percent complete: 12.4%; Average loss: 3.5824\n","Iteration: 496; Percent complete: 12.4%; Average loss: 3.8489\n","Iteration: 497; Percent complete: 12.4%; Average loss: 3.9254\n","Iteration: 498; Percent complete: 12.4%; Average loss: 3.8971\n","Iteration: 499; Percent complete: 12.5%; Average loss: 3.7417\n","Iteration: 500; Percent complete: 12.5%; Average loss: 3.8923\n","Iteration: 501; Percent complete: 12.5%; Average loss: 3.6445\n","Iteration: 502; Percent complete: 12.6%; Average loss: 3.6692\n","Iteration: 503; Percent complete: 12.6%; Average loss: 3.7928\n","Iteration: 504; Percent complete: 12.6%; Average loss: 3.7436\n","Iteration: 505; Percent complete: 12.6%; Average loss: 3.6232\n","Iteration: 506; Percent complete: 12.7%; Average loss: 3.6094\n","Iteration: 507; Percent complete: 12.7%; Average loss: 3.8881\n","Iteration: 508; Percent complete: 12.7%; Average loss: 3.7783\n","Iteration: 509; Percent complete: 12.7%; Average loss: 3.4731\n","Iteration: 510; Percent complete: 12.8%; Average loss: 3.8672\n","Iteration: 511; Percent complete: 12.8%; Average loss: 3.7159\n","Iteration: 512; Percent complete: 12.8%; Average loss: 3.6820\n","Iteration: 513; Percent complete: 12.8%; Average loss: 3.9741\n","Iteration: 514; Percent complete: 12.8%; Average loss: 3.6760\n","Iteration: 515; Percent complete: 12.9%; Average loss: 3.5086\n","Iteration: 516; Percent complete: 12.9%; Average loss: 3.8041\n","Iteration: 517; Percent complete: 12.9%; Average loss: 3.6606\n","Iteration: 518; Percent complete: 13.0%; Average loss: 3.7085\n","Iteration: 519; Percent complete: 13.0%; Average loss: 3.8631\n","Iteration: 520; Percent complete: 13.0%; Average loss: 3.8295\n","Iteration: 521; Percent complete: 13.0%; Average loss: 3.6442\n","Iteration: 522; Percent complete: 13.1%; Average loss: 3.7105\n","Iteration: 523; Percent complete: 13.1%; Average loss: 3.8992\n","Iteration: 524; Percent complete: 13.1%; Average loss: 3.6017\n","Iteration: 525; Percent complete: 13.1%; Average loss: 3.6059\n","Iteration: 526; Percent complete: 13.2%; Average loss: 3.9252\n","Iteration: 527; Percent complete: 13.2%; Average loss: 3.5331\n","Iteration: 528; Percent complete: 13.2%; Average loss: 3.6026\n","Iteration: 529; Percent complete: 13.2%; Average loss: 3.6260\n","Iteration: 530; Percent complete: 13.2%; Average loss: 3.6194\n","Iteration: 531; Percent complete: 13.3%; Average loss: 4.0154\n","Iteration: 532; Percent complete: 13.3%; Average loss: 3.7804\n","Iteration: 533; Percent complete: 13.3%; Average loss: 3.4210\n","Iteration: 534; Percent complete: 13.4%; Average loss: 3.9149\n","Iteration: 535; Percent complete: 13.4%; Average loss: 3.5701\n","Iteration: 536; Percent complete: 13.4%; Average loss: 3.7032\n","Iteration: 537; Percent complete: 13.4%; Average loss: 3.7766\n","Iteration: 538; Percent complete: 13.5%; Average loss: 3.6484\n","Iteration: 539; Percent complete: 13.5%; Average loss: 3.8321\n","Iteration: 540; Percent complete: 13.5%; Average loss: 3.7780\n","Iteration: 541; Percent complete: 13.5%; Average loss: 3.5736\n","Iteration: 542; Percent complete: 13.6%; Average loss: 3.5535\n","Iteration: 543; Percent complete: 13.6%; Average loss: 3.7978\n","Iteration: 544; Percent complete: 13.6%; Average loss: 3.7812\n","Iteration: 545; Percent complete: 13.6%; Average loss: 3.7090\n","Iteration: 546; Percent complete: 13.7%; Average loss: 3.5301\n","Iteration: 547; Percent complete: 13.7%; Average loss: 3.8084\n","Iteration: 548; Percent complete: 13.7%; Average loss: 3.8603\n","Iteration: 549; Percent complete: 13.7%; Average loss: 3.7161\n","Iteration: 550; Percent complete: 13.8%; Average loss: 3.5573\n","Iteration: 551; Percent complete: 13.8%; Average loss: 3.5390\n","Iteration: 552; Percent complete: 13.8%; Average loss: 3.6924\n","Iteration: 553; Percent complete: 13.8%; Average loss: 3.7021\n","Iteration: 554; Percent complete: 13.9%; Average loss: 3.5044\n","Iteration: 555; Percent complete: 13.9%; Average loss: 3.8229\n","Iteration: 556; Percent complete: 13.9%; Average loss: 3.5427\n","Iteration: 557; Percent complete: 13.9%; Average loss: 3.7800\n","Iteration: 558; Percent complete: 14.0%; Average loss: 3.5854\n","Iteration: 559; Percent complete: 14.0%; Average loss: 3.5696\n","Iteration: 560; Percent complete: 14.0%; Average loss: 3.5859\n","Iteration: 561; Percent complete: 14.0%; Average loss: 3.6632\n","Iteration: 562; Percent complete: 14.1%; Average loss: 3.5623\n","Iteration: 563; Percent complete: 14.1%; Average loss: 3.6642\n","Iteration: 564; Percent complete: 14.1%; Average loss: 3.8960\n","Iteration: 565; Percent complete: 14.1%; Average loss: 3.6802\n","Iteration: 566; Percent complete: 14.1%; Average loss: 3.5692\n","Iteration: 567; Percent complete: 14.2%; Average loss: 3.7242\n","Iteration: 568; Percent complete: 14.2%; Average loss: 3.4679\n","Iteration: 569; Percent complete: 14.2%; Average loss: 3.5941\n","Iteration: 570; Percent complete: 14.2%; Average loss: 3.6394\n","Iteration: 571; Percent complete: 14.3%; Average loss: 4.0238\n","Iteration: 572; Percent complete: 14.3%; Average loss: 3.6408\n","Iteration: 573; Percent complete: 14.3%; Average loss: 3.7804\n","Iteration: 574; Percent complete: 14.3%; Average loss: 3.9772\n","Iteration: 575; Percent complete: 14.4%; Average loss: 3.7120\n","Iteration: 576; Percent complete: 14.4%; Average loss: 3.7468\n","Iteration: 577; Percent complete: 14.4%; Average loss: 3.5243\n","Iteration: 578; Percent complete: 14.4%; Average loss: 3.7037\n","Iteration: 579; Percent complete: 14.5%; Average loss: 3.7310\n","Iteration: 580; Percent complete: 14.5%; Average loss: 3.6292\n","Iteration: 581; Percent complete: 14.5%; Average loss: 3.8234\n","Iteration: 582; Percent complete: 14.5%; Average loss: 3.8368\n","Iteration: 583; Percent complete: 14.6%; Average loss: 3.6662\n","Iteration: 584; Percent complete: 14.6%; Average loss: 3.9132\n","Iteration: 585; Percent complete: 14.6%; Average loss: 3.6490\n","Iteration: 586; Percent complete: 14.6%; Average loss: 3.6479\n","Iteration: 587; Percent complete: 14.7%; Average loss: 3.2502\n","Iteration: 588; Percent complete: 14.7%; Average loss: 3.8035\n","Iteration: 589; Percent complete: 14.7%; Average loss: 3.7227\n","Iteration: 590; Percent complete: 14.8%; Average loss: 3.4067\n","Iteration: 591; Percent complete: 14.8%; Average loss: 3.3163\n","Iteration: 592; Percent complete: 14.8%; Average loss: 3.4334\n","Iteration: 593; Percent complete: 14.8%; Average loss: 3.9154\n","Iteration: 594; Percent complete: 14.8%; Average loss: 3.4852\n","Iteration: 595; Percent complete: 14.9%; Average loss: 3.8196\n","Iteration: 596; Percent complete: 14.9%; Average loss: 3.7573\n","Iteration: 597; Percent complete: 14.9%; Average loss: 3.6681\n","Iteration: 598; Percent complete: 14.9%; Average loss: 3.6660\n","Iteration: 599; Percent complete: 15.0%; Average loss: 3.8020\n","Iteration: 600; Percent complete: 15.0%; Average loss: 3.8479\n","Iteration: 601; Percent complete: 15.0%; Average loss: 3.6722\n","Iteration: 602; Percent complete: 15.0%; Average loss: 3.5480\n","Iteration: 603; Percent complete: 15.1%; Average loss: 3.7109\n","Iteration: 604; Percent complete: 15.1%; Average loss: 3.4444\n","Iteration: 605; Percent complete: 15.1%; Average loss: 3.7148\n","Iteration: 606; Percent complete: 15.2%; Average loss: 3.5537\n","Iteration: 607; Percent complete: 15.2%; Average loss: 3.5399\n","Iteration: 608; Percent complete: 15.2%; Average loss: 3.5735\n","Iteration: 609; Percent complete: 15.2%; Average loss: 3.5079\n","Iteration: 610; Percent complete: 15.2%; Average loss: 3.7716\n","Iteration: 611; Percent complete: 15.3%; Average loss: 3.8505\n","Iteration: 612; Percent complete: 15.3%; Average loss: 3.6919\n","Iteration: 613; Percent complete: 15.3%; Average loss: 3.5850\n","Iteration: 614; Percent complete: 15.3%; Average loss: 3.6817\n","Iteration: 615; Percent complete: 15.4%; Average loss: 3.5631\n","Iteration: 616; Percent complete: 15.4%; Average loss: 3.6078\n","Iteration: 617; Percent complete: 15.4%; Average loss: 3.6022\n","Iteration: 618; Percent complete: 15.4%; Average loss: 3.2719\n","Iteration: 619; Percent complete: 15.5%; Average loss: 3.5104\n","Iteration: 620; Percent complete: 15.5%; Average loss: 3.5457\n","Iteration: 621; Percent complete: 15.5%; Average loss: 3.7650\n","Iteration: 622; Percent complete: 15.6%; Average loss: 3.8535\n","Iteration: 623; Percent complete: 15.6%; Average loss: 3.7478\n","Iteration: 624; Percent complete: 15.6%; Average loss: 3.7039\n","Iteration: 625; Percent complete: 15.6%; Average loss: 3.6420\n","Iteration: 626; Percent complete: 15.7%; Average loss: 3.8732\n","Iteration: 627; Percent complete: 15.7%; Average loss: 3.6000\n","Iteration: 628; Percent complete: 15.7%; Average loss: 3.5784\n","Iteration: 629; Percent complete: 15.7%; Average loss: 3.5742\n","Iteration: 630; Percent complete: 15.8%; Average loss: 3.5783\n","Iteration: 631; Percent complete: 15.8%; Average loss: 3.8133\n","Iteration: 632; Percent complete: 15.8%; Average loss: 3.6214\n","Iteration: 633; Percent complete: 15.8%; Average loss: 3.7087\n","Iteration: 634; Percent complete: 15.8%; Average loss: 3.7985\n","Iteration: 635; Percent complete: 15.9%; Average loss: 3.9382\n","Iteration: 636; Percent complete: 15.9%; Average loss: 3.5990\n","Iteration: 637; Percent complete: 15.9%; Average loss: 3.7028\n","Iteration: 638; Percent complete: 16.0%; Average loss: 3.9166\n","Iteration: 639; Percent complete: 16.0%; Average loss: 3.7177\n","Iteration: 640; Percent complete: 16.0%; Average loss: 3.6242\n","Iteration: 641; Percent complete: 16.0%; Average loss: 3.6823\n","Iteration: 642; Percent complete: 16.1%; Average loss: 3.8052\n","Iteration: 643; Percent complete: 16.1%; Average loss: 3.4562\n","Iteration: 644; Percent complete: 16.1%; Average loss: 3.9463\n","Iteration: 645; Percent complete: 16.1%; Average loss: 3.5452\n","Iteration: 646; Percent complete: 16.2%; Average loss: 3.5669\n","Iteration: 647; Percent complete: 16.2%; Average loss: 3.8214\n","Iteration: 648; Percent complete: 16.2%; Average loss: 3.7546\n","Iteration: 649; Percent complete: 16.2%; Average loss: 3.5624\n","Iteration: 650; Percent complete: 16.2%; Average loss: 3.4345\n","Iteration: 651; Percent complete: 16.3%; Average loss: 3.6370\n","Iteration: 652; Percent complete: 16.3%; Average loss: 3.6601\n","Iteration: 653; Percent complete: 16.3%; Average loss: 3.4860\n","Iteration: 654; Percent complete: 16.4%; Average loss: 3.2884\n","Iteration: 655; Percent complete: 16.4%; Average loss: 3.5101\n","Iteration: 656; Percent complete: 16.4%; Average loss: 3.5572\n","Iteration: 657; Percent complete: 16.4%; Average loss: 3.6604\n","Iteration: 658; Percent complete: 16.4%; Average loss: 3.9530\n","Iteration: 659; Percent complete: 16.5%; Average loss: 3.5943\n","Iteration: 660; Percent complete: 16.5%; Average loss: 3.7565\n","Iteration: 661; Percent complete: 16.5%; Average loss: 3.5804\n","Iteration: 662; Percent complete: 16.6%; Average loss: 3.6113\n","Iteration: 663; Percent complete: 16.6%; Average loss: 3.7606\n","Iteration: 664; Percent complete: 16.6%; Average loss: 3.6518\n","Iteration: 665; Percent complete: 16.6%; Average loss: 3.6885\n","Iteration: 666; Percent complete: 16.7%; Average loss: 3.5834\n","Iteration: 667; Percent complete: 16.7%; Average loss: 3.6697\n","Iteration: 668; Percent complete: 16.7%; Average loss: 3.7515\n","Iteration: 669; Percent complete: 16.7%; Average loss: 3.4886\n","Iteration: 670; Percent complete: 16.8%; Average loss: 3.4435\n","Iteration: 671; Percent complete: 16.8%; Average loss: 3.4087\n","Iteration: 672; Percent complete: 16.8%; Average loss: 3.6664\n","Iteration: 673; Percent complete: 16.8%; Average loss: 3.4507\n","Iteration: 674; Percent complete: 16.9%; Average loss: 3.4899\n","Iteration: 675; Percent complete: 16.9%; Average loss: 3.6845\n","Iteration: 676; Percent complete: 16.9%; Average loss: 3.7172\n","Iteration: 677; Percent complete: 16.9%; Average loss: 3.7514\n","Iteration: 678; Percent complete: 17.0%; Average loss: 3.7474\n","Iteration: 679; Percent complete: 17.0%; Average loss: 3.5838\n","Iteration: 680; Percent complete: 17.0%; Average loss: 3.6227\n","Iteration: 681; Percent complete: 17.0%; Average loss: 3.6910\n","Iteration: 682; Percent complete: 17.1%; Average loss: 3.4716\n","Iteration: 683; Percent complete: 17.1%; Average loss: 3.5704\n","Iteration: 684; Percent complete: 17.1%; Average loss: 3.8235\n","Iteration: 685; Percent complete: 17.1%; Average loss: 3.5019\n","Iteration: 686; Percent complete: 17.2%; Average loss: 3.3701\n","Iteration: 687; Percent complete: 17.2%; Average loss: 3.6348\n","Iteration: 688; Percent complete: 17.2%; Average loss: 3.4725\n","Iteration: 689; Percent complete: 17.2%; Average loss: 3.6197\n","Iteration: 690; Percent complete: 17.2%; Average loss: 3.6651\n","Iteration: 691; Percent complete: 17.3%; Average loss: 3.6690\n","Iteration: 692; Percent complete: 17.3%; Average loss: 3.4770\n","Iteration: 693; Percent complete: 17.3%; Average loss: 3.5059\n","Iteration: 694; Percent complete: 17.3%; Average loss: 3.5231\n","Iteration: 695; Percent complete: 17.4%; Average loss: 3.5372\n","Iteration: 696; Percent complete: 17.4%; Average loss: 3.6618\n","Iteration: 697; Percent complete: 17.4%; Average loss: 3.6475\n","Iteration: 698; Percent complete: 17.4%; Average loss: 3.9248\n","Iteration: 699; Percent complete: 17.5%; Average loss: 3.8225\n","Iteration: 700; Percent complete: 17.5%; Average loss: 3.5827\n","Iteration: 701; Percent complete: 17.5%; Average loss: 3.6737\n","Iteration: 702; Percent complete: 17.5%; Average loss: 3.7761\n","Iteration: 703; Percent complete: 17.6%; Average loss: 3.6871\n","Iteration: 704; Percent complete: 17.6%; Average loss: 3.4718\n","Iteration: 705; Percent complete: 17.6%; Average loss: 3.5492\n","Iteration: 706; Percent complete: 17.6%; Average loss: 3.7415\n","Iteration: 707; Percent complete: 17.7%; Average loss: 3.6640\n","Iteration: 708; Percent complete: 17.7%; Average loss: 3.5119\n","Iteration: 709; Percent complete: 17.7%; Average loss: 3.7545\n","Iteration: 710; Percent complete: 17.8%; Average loss: 3.7731\n","Iteration: 711; Percent complete: 17.8%; Average loss: 3.6825\n","Iteration: 712; Percent complete: 17.8%; Average loss: 3.5717\n","Iteration: 713; Percent complete: 17.8%; Average loss: 3.5154\n","Iteration: 714; Percent complete: 17.8%; Average loss: 3.7714\n","Iteration: 715; Percent complete: 17.9%; Average loss: 3.7428\n","Iteration: 716; Percent complete: 17.9%; Average loss: 3.5685\n","Iteration: 717; Percent complete: 17.9%; Average loss: 3.4045\n","Iteration: 718; Percent complete: 17.9%; Average loss: 3.4257\n","Iteration: 719; Percent complete: 18.0%; Average loss: 3.6726\n","Iteration: 720; Percent complete: 18.0%; Average loss: 3.9605\n","Iteration: 721; Percent complete: 18.0%; Average loss: 3.6687\n","Iteration: 722; Percent complete: 18.1%; Average loss: 3.8230\n","Iteration: 723; Percent complete: 18.1%; Average loss: 3.6440\n","Iteration: 724; Percent complete: 18.1%; Average loss: 3.4120\n","Iteration: 725; Percent complete: 18.1%; Average loss: 3.7343\n","Iteration: 726; Percent complete: 18.1%; Average loss: 3.4740\n","Iteration: 727; Percent complete: 18.2%; Average loss: 3.3830\n","Iteration: 728; Percent complete: 18.2%; Average loss: 3.7615\n","Iteration: 729; Percent complete: 18.2%; Average loss: 3.6104\n","Iteration: 730; Percent complete: 18.2%; Average loss: 3.3300\n","Iteration: 731; Percent complete: 18.3%; Average loss: 3.8750\n","Iteration: 732; Percent complete: 18.3%; Average loss: 3.5340\n","Iteration: 733; Percent complete: 18.3%; Average loss: 3.6643\n","Iteration: 734; Percent complete: 18.4%; Average loss: 3.5382\n","Iteration: 735; Percent complete: 18.4%; Average loss: 3.6292\n","Iteration: 736; Percent complete: 18.4%; Average loss: 3.8768\n","Iteration: 737; Percent complete: 18.4%; Average loss: 3.5098\n","Iteration: 738; Percent complete: 18.4%; Average loss: 3.5960\n","Iteration: 739; Percent complete: 18.5%; Average loss: 3.7441\n","Iteration: 740; Percent complete: 18.5%; Average loss: 3.5790\n","Iteration: 741; Percent complete: 18.5%; Average loss: 3.7587\n","Iteration: 742; Percent complete: 18.6%; Average loss: 3.7714\n","Iteration: 743; Percent complete: 18.6%; Average loss: 3.5075\n","Iteration: 744; Percent complete: 18.6%; Average loss: 3.6559\n","Iteration: 745; Percent complete: 18.6%; Average loss: 3.5073\n","Iteration: 746; Percent complete: 18.6%; Average loss: 3.6056\n","Iteration: 747; Percent complete: 18.7%; Average loss: 3.6830\n","Iteration: 748; Percent complete: 18.7%; Average loss: 3.5371\n","Iteration: 749; Percent complete: 18.7%; Average loss: 3.3403\n","Iteration: 750; Percent complete: 18.8%; Average loss: 3.6964\n","Iteration: 751; Percent complete: 18.8%; Average loss: 3.5521\n","Iteration: 752; Percent complete: 18.8%; Average loss: 3.6743\n","Iteration: 753; Percent complete: 18.8%; Average loss: 3.2824\n","Iteration: 754; Percent complete: 18.9%; Average loss: 3.4780\n","Iteration: 755; Percent complete: 18.9%; Average loss: 3.5605\n","Iteration: 756; Percent complete: 18.9%; Average loss: 3.4472\n","Iteration: 757; Percent complete: 18.9%; Average loss: 3.6947\n","Iteration: 758; Percent complete: 18.9%; Average loss: 3.4018\n","Iteration: 759; Percent complete: 19.0%; Average loss: 3.7412\n","Iteration: 760; Percent complete: 19.0%; Average loss: 3.6295\n","Iteration: 761; Percent complete: 19.0%; Average loss: 3.5314\n","Iteration: 762; Percent complete: 19.1%; Average loss: 3.5834\n","Iteration: 763; Percent complete: 19.1%; Average loss: 3.5254\n","Iteration: 764; Percent complete: 19.1%; Average loss: 3.5286\n","Iteration: 765; Percent complete: 19.1%; Average loss: 3.3785\n","Iteration: 766; Percent complete: 19.1%; Average loss: 3.7418\n","Iteration: 767; Percent complete: 19.2%; Average loss: 3.5595\n","Iteration: 768; Percent complete: 19.2%; Average loss: 3.4744\n","Iteration: 769; Percent complete: 19.2%; Average loss: 3.3946\n","Iteration: 770; Percent complete: 19.2%; Average loss: 3.5886\n","Iteration: 771; Percent complete: 19.3%; Average loss: 3.4318\n","Iteration: 772; Percent complete: 19.3%; Average loss: 3.6411\n","Iteration: 773; Percent complete: 19.3%; Average loss: 3.7773\n","Iteration: 774; Percent complete: 19.4%; Average loss: 3.7976\n","Iteration: 775; Percent complete: 19.4%; Average loss: 3.7073\n","Iteration: 776; Percent complete: 19.4%; Average loss: 3.6950\n","Iteration: 777; Percent complete: 19.4%; Average loss: 3.7196\n","Iteration: 778; Percent complete: 19.4%; Average loss: 3.5930\n","Iteration: 779; Percent complete: 19.5%; Average loss: 3.4152\n","Iteration: 780; Percent complete: 19.5%; Average loss: 3.6485\n","Iteration: 781; Percent complete: 19.5%; Average loss: 3.3956\n","Iteration: 782; Percent complete: 19.6%; Average loss: 3.5554\n","Iteration: 783; Percent complete: 19.6%; Average loss: 3.5979\n","Iteration: 784; Percent complete: 19.6%; Average loss: 3.6897\n","Iteration: 785; Percent complete: 19.6%; Average loss: 3.3097\n","Iteration: 786; Percent complete: 19.7%; Average loss: 3.5962\n","Iteration: 787; Percent complete: 19.7%; Average loss: 3.3486\n","Iteration: 788; Percent complete: 19.7%; Average loss: 3.6741\n","Iteration: 789; Percent complete: 19.7%; Average loss: 3.5162\n","Iteration: 790; Percent complete: 19.8%; Average loss: 3.5182\n","Iteration: 791; Percent complete: 19.8%; Average loss: 3.7129\n","Iteration: 792; Percent complete: 19.8%; Average loss: 3.6040\n","Iteration: 793; Percent complete: 19.8%; Average loss: 3.7390\n","Iteration: 794; Percent complete: 19.9%; Average loss: 3.8146\n","Iteration: 795; Percent complete: 19.9%; Average loss: 3.4426\n","Iteration: 796; Percent complete: 19.9%; Average loss: 3.4647\n","Iteration: 797; Percent complete: 19.9%; Average loss: 3.6405\n","Iteration: 798; Percent complete: 20.0%; Average loss: 3.4721\n","Iteration: 799; Percent complete: 20.0%; Average loss: 3.9323\n","Iteration: 800; Percent complete: 20.0%; Average loss: 3.4785\n","Iteration: 801; Percent complete: 20.0%; Average loss: 3.3367\n","Iteration: 802; Percent complete: 20.1%; Average loss: 3.6163\n","Iteration: 803; Percent complete: 20.1%; Average loss: 3.3844\n","Iteration: 804; Percent complete: 20.1%; Average loss: 3.4529\n","Iteration: 805; Percent complete: 20.1%; Average loss: 3.5393\n","Iteration: 806; Percent complete: 20.2%; Average loss: 3.6488\n","Iteration: 807; Percent complete: 20.2%; Average loss: 3.2550\n","Iteration: 808; Percent complete: 20.2%; Average loss: 3.6223\n","Iteration: 809; Percent complete: 20.2%; Average loss: 3.2769\n","Iteration: 810; Percent complete: 20.2%; Average loss: 3.3676\n","Iteration: 811; Percent complete: 20.3%; Average loss: 3.4306\n","Iteration: 812; Percent complete: 20.3%; Average loss: 3.7904\n","Iteration: 813; Percent complete: 20.3%; Average loss: 3.5221\n","Iteration: 814; Percent complete: 20.3%; Average loss: 3.4067\n","Iteration: 815; Percent complete: 20.4%; Average loss: 3.6168\n","Iteration: 816; Percent complete: 20.4%; Average loss: 3.3370\n","Iteration: 817; Percent complete: 20.4%; Average loss: 3.4140\n","Iteration: 818; Percent complete: 20.4%; Average loss: 3.4958\n","Iteration: 819; Percent complete: 20.5%; Average loss: 3.6737\n","Iteration: 820; Percent complete: 20.5%; Average loss: 3.5722\n","Iteration: 821; Percent complete: 20.5%; Average loss: 3.7197\n","Iteration: 822; Percent complete: 20.5%; Average loss: 3.6166\n","Iteration: 823; Percent complete: 20.6%; Average loss: 3.5357\n","Iteration: 824; Percent complete: 20.6%; Average loss: 3.4253\n","Iteration: 825; Percent complete: 20.6%; Average loss: 3.4890\n","Iteration: 826; Percent complete: 20.6%; Average loss: 3.3522\n","Iteration: 827; Percent complete: 20.7%; Average loss: 3.6129\n","Iteration: 828; Percent complete: 20.7%; Average loss: 3.5163\n","Iteration: 829; Percent complete: 20.7%; Average loss: 3.6181\n","Iteration: 830; Percent complete: 20.8%; Average loss: 3.4693\n","Iteration: 831; Percent complete: 20.8%; Average loss: 3.6797\n","Iteration: 832; Percent complete: 20.8%; Average loss: 3.7975\n","Iteration: 833; Percent complete: 20.8%; Average loss: 3.5981\n","Iteration: 834; Percent complete: 20.8%; Average loss: 3.5172\n","Iteration: 835; Percent complete: 20.9%; Average loss: 3.6029\n","Iteration: 836; Percent complete: 20.9%; Average loss: 3.3820\n","Iteration: 837; Percent complete: 20.9%; Average loss: 3.6288\n","Iteration: 838; Percent complete: 20.9%; Average loss: 3.7195\n","Iteration: 839; Percent complete: 21.0%; Average loss: 3.2768\n","Iteration: 840; Percent complete: 21.0%; Average loss: 3.3768\n","Iteration: 841; Percent complete: 21.0%; Average loss: 3.3796\n","Iteration: 842; Percent complete: 21.1%; Average loss: 3.7265\n","Iteration: 843; Percent complete: 21.1%; Average loss: 3.4352\n","Iteration: 844; Percent complete: 21.1%; Average loss: 3.5949\n","Iteration: 845; Percent complete: 21.1%; Average loss: 3.4616\n","Iteration: 846; Percent complete: 21.1%; Average loss: 3.6099\n","Iteration: 847; Percent complete: 21.2%; Average loss: 3.6905\n","Iteration: 848; Percent complete: 21.2%; Average loss: 3.5957\n","Iteration: 849; Percent complete: 21.2%; Average loss: 3.5793\n","Iteration: 850; Percent complete: 21.2%; Average loss: 3.3417\n","Iteration: 851; Percent complete: 21.3%; Average loss: 3.5988\n","Iteration: 852; Percent complete: 21.3%; Average loss: 3.7662\n","Iteration: 853; Percent complete: 21.3%; Average loss: 3.3423\n","Iteration: 854; Percent complete: 21.3%; Average loss: 3.2245\n","Iteration: 855; Percent complete: 21.4%; Average loss: 3.6501\n","Iteration: 856; Percent complete: 21.4%; Average loss: 3.8432\n","Iteration: 857; Percent complete: 21.4%; Average loss: 3.6049\n","Iteration: 858; Percent complete: 21.4%; Average loss: 3.7071\n","Iteration: 859; Percent complete: 21.5%; Average loss: 3.6199\n","Iteration: 860; Percent complete: 21.5%; Average loss: 3.3814\n","Iteration: 861; Percent complete: 21.5%; Average loss: 3.9107\n","Iteration: 862; Percent complete: 21.6%; Average loss: 3.7007\n","Iteration: 863; Percent complete: 21.6%; Average loss: 3.4810\n","Iteration: 864; Percent complete: 21.6%; Average loss: 3.8231\n","Iteration: 865; Percent complete: 21.6%; Average loss: 3.6659\n","Iteration: 866; Percent complete: 21.6%; Average loss: 3.6788\n","Iteration: 867; Percent complete: 21.7%; Average loss: 3.5443\n","Iteration: 868; Percent complete: 21.7%; Average loss: 3.6310\n","Iteration: 869; Percent complete: 21.7%; Average loss: 3.2492\n","Iteration: 870; Percent complete: 21.8%; Average loss: 3.3496\n","Iteration: 871; Percent complete: 21.8%; Average loss: 3.4949\n","Iteration: 872; Percent complete: 21.8%; Average loss: 3.4629\n","Iteration: 873; Percent complete: 21.8%; Average loss: 3.6677\n","Iteration: 874; Percent complete: 21.9%; Average loss: 3.5488\n","Iteration: 875; Percent complete: 21.9%; Average loss: 3.3294\n","Iteration: 876; Percent complete: 21.9%; Average loss: 3.5681\n","Iteration: 877; Percent complete: 21.9%; Average loss: 3.4359\n","Iteration: 878; Percent complete: 21.9%; Average loss: 3.4446\n","Iteration: 879; Percent complete: 22.0%; Average loss: 3.5351\n","Iteration: 880; Percent complete: 22.0%; Average loss: 3.4970\n","Iteration: 881; Percent complete: 22.0%; Average loss: 3.6858\n","Iteration: 882; Percent complete: 22.1%; Average loss: 3.6814\n","Iteration: 883; Percent complete: 22.1%; Average loss: 3.3601\n","Iteration: 884; Percent complete: 22.1%; Average loss: 3.6200\n","Iteration: 885; Percent complete: 22.1%; Average loss: 3.4475\n","Iteration: 886; Percent complete: 22.1%; Average loss: 3.6221\n","Iteration: 887; Percent complete: 22.2%; Average loss: 3.9340\n","Iteration: 888; Percent complete: 22.2%; Average loss: 3.4788\n","Iteration: 889; Percent complete: 22.2%; Average loss: 3.3585\n","Iteration: 890; Percent complete: 22.2%; Average loss: 3.4569\n","Iteration: 891; Percent complete: 22.3%; Average loss: 3.5101\n","Iteration: 892; Percent complete: 22.3%; Average loss: 3.6659\n","Iteration: 893; Percent complete: 22.3%; Average loss: 3.8003\n","Iteration: 894; Percent complete: 22.4%; Average loss: 3.8186\n","Iteration: 895; Percent complete: 22.4%; Average loss: 3.2227\n","Iteration: 896; Percent complete: 22.4%; Average loss: 3.7302\n","Iteration: 897; Percent complete: 22.4%; Average loss: 3.6614\n","Iteration: 898; Percent complete: 22.4%; Average loss: 3.5043\n","Iteration: 899; Percent complete: 22.5%; Average loss: 3.5004\n","Iteration: 900; Percent complete: 22.5%; Average loss: 3.7553\n","Iteration: 901; Percent complete: 22.5%; Average loss: 3.4243\n","Iteration: 902; Percent complete: 22.6%; Average loss: 3.3444\n","Iteration: 903; Percent complete: 22.6%; Average loss: 3.5258\n","Iteration: 904; Percent complete: 22.6%; Average loss: 3.5787\n","Iteration: 905; Percent complete: 22.6%; Average loss: 3.4932\n","Iteration: 906; Percent complete: 22.7%; Average loss: 3.4574\n","Iteration: 907; Percent complete: 22.7%; Average loss: 3.1841\n","Iteration: 908; Percent complete: 22.7%; Average loss: 3.2547\n","Iteration: 909; Percent complete: 22.7%; Average loss: 3.5268\n","Iteration: 910; Percent complete: 22.8%; Average loss: 3.5720\n","Iteration: 911; Percent complete: 22.8%; Average loss: 3.4807\n","Iteration: 912; Percent complete: 22.8%; Average loss: 3.5471\n","Iteration: 913; Percent complete: 22.8%; Average loss: 3.6190\n","Iteration: 914; Percent complete: 22.9%; Average loss: 3.6158\n","Iteration: 915; Percent complete: 22.9%; Average loss: 3.5393\n","Iteration: 916; Percent complete: 22.9%; Average loss: 3.3410\n","Iteration: 917; Percent complete: 22.9%; Average loss: 3.5569\n","Iteration: 918; Percent complete: 22.9%; Average loss: 3.8487\n","Iteration: 919; Percent complete: 23.0%; Average loss: 3.5082\n","Iteration: 920; Percent complete: 23.0%; Average loss: 3.6804\n","Iteration: 921; Percent complete: 23.0%; Average loss: 3.5458\n","Iteration: 922; Percent complete: 23.1%; Average loss: 3.3197\n","Iteration: 923; Percent complete: 23.1%; Average loss: 3.5007\n","Iteration: 924; Percent complete: 23.1%; Average loss: 3.5774\n","Iteration: 925; Percent complete: 23.1%; Average loss: 3.7371\n","Iteration: 926; Percent complete: 23.2%; Average loss: 3.4519\n","Iteration: 927; Percent complete: 23.2%; Average loss: 3.6553\n","Iteration: 928; Percent complete: 23.2%; Average loss: 3.2724\n","Iteration: 929; Percent complete: 23.2%; Average loss: 3.5949\n","Iteration: 930; Percent complete: 23.2%; Average loss: 3.8285\n","Iteration: 931; Percent complete: 23.3%; Average loss: 3.4935\n","Iteration: 932; Percent complete: 23.3%; Average loss: 3.3463\n","Iteration: 933; Percent complete: 23.3%; Average loss: 3.4922\n","Iteration: 934; Percent complete: 23.4%; Average loss: 3.3205\n","Iteration: 935; Percent complete: 23.4%; Average loss: 3.3726\n","Iteration: 936; Percent complete: 23.4%; Average loss: 3.4024\n","Iteration: 937; Percent complete: 23.4%; Average loss: 3.3517\n","Iteration: 938; Percent complete: 23.4%; Average loss: 3.3923\n","Iteration: 939; Percent complete: 23.5%; Average loss: 3.2320\n","Iteration: 940; Percent complete: 23.5%; Average loss: 3.3426\n","Iteration: 941; Percent complete: 23.5%; Average loss: 3.6324\n","Iteration: 942; Percent complete: 23.5%; Average loss: 3.2814\n","Iteration: 943; Percent complete: 23.6%; Average loss: 3.5964\n","Iteration: 944; Percent complete: 23.6%; Average loss: 3.4169\n","Iteration: 945; Percent complete: 23.6%; Average loss: 3.5531\n","Iteration: 946; Percent complete: 23.6%; Average loss: 3.4854\n","Iteration: 947; Percent complete: 23.7%; Average loss: 3.1994\n","Iteration: 948; Percent complete: 23.7%; Average loss: 3.5590\n","Iteration: 949; Percent complete: 23.7%; Average loss: 3.3331\n","Iteration: 950; Percent complete: 23.8%; Average loss: 3.5847\n","Iteration: 951; Percent complete: 23.8%; Average loss: 3.4372\n","Iteration: 952; Percent complete: 23.8%; Average loss: 3.5986\n","Iteration: 953; Percent complete: 23.8%; Average loss: 3.8333\n","Iteration: 954; Percent complete: 23.8%; Average loss: 3.4797\n","Iteration: 955; Percent complete: 23.9%; Average loss: 3.8127\n","Iteration: 956; Percent complete: 23.9%; Average loss: 3.5471\n","Iteration: 957; Percent complete: 23.9%; Average loss: 3.3508\n","Iteration: 958; Percent complete: 23.9%; Average loss: 3.6165\n","Iteration: 959; Percent complete: 24.0%; Average loss: 3.5136\n","Iteration: 960; Percent complete: 24.0%; Average loss: 3.5506\n","Iteration: 961; Percent complete: 24.0%; Average loss: 3.3412\n","Iteration: 962; Percent complete: 24.1%; Average loss: 3.3994\n","Iteration: 963; Percent complete: 24.1%; Average loss: 3.3131\n","Iteration: 964; Percent complete: 24.1%; Average loss: 3.7260\n","Iteration: 965; Percent complete: 24.1%; Average loss: 3.6619\n","Iteration: 966; Percent complete: 24.1%; Average loss: 3.6441\n","Iteration: 967; Percent complete: 24.2%; Average loss: 3.6393\n","Iteration: 968; Percent complete: 24.2%; Average loss: 3.3766\n","Iteration: 969; Percent complete: 24.2%; Average loss: 3.3611\n","Iteration: 970; Percent complete: 24.2%; Average loss: 3.3620\n","Iteration: 971; Percent complete: 24.3%; Average loss: 3.4866\n","Iteration: 972; Percent complete: 24.3%; Average loss: 3.5090\n","Iteration: 973; Percent complete: 24.3%; Average loss: 3.5680\n","Iteration: 974; Percent complete: 24.3%; Average loss: 3.4839\n","Iteration: 975; Percent complete: 24.4%; Average loss: 3.5774\n","Iteration: 976; Percent complete: 24.4%; Average loss: 3.7974\n","Iteration: 977; Percent complete: 24.4%; Average loss: 3.3905\n","Iteration: 978; Percent complete: 24.4%; Average loss: 3.3497\n","Iteration: 979; Percent complete: 24.5%; Average loss: 3.4896\n","Iteration: 980; Percent complete: 24.5%; Average loss: 3.4650\n","Iteration: 981; Percent complete: 24.5%; Average loss: 3.7709\n","Iteration: 982; Percent complete: 24.6%; Average loss: 3.3669\n","Iteration: 983; Percent complete: 24.6%; Average loss: 3.3244\n","Iteration: 984; Percent complete: 24.6%; Average loss: 3.3247\n","Iteration: 985; Percent complete: 24.6%; Average loss: 3.5007\n","Iteration: 986; Percent complete: 24.6%; Average loss: 3.7264\n","Iteration: 987; Percent complete: 24.7%; Average loss: 3.2041\n","Iteration: 988; Percent complete: 24.7%; Average loss: 3.7244\n","Iteration: 989; Percent complete: 24.7%; Average loss: 3.2693\n","Iteration: 990; Percent complete: 24.8%; Average loss: 3.5132\n","Iteration: 991; Percent complete: 24.8%; Average loss: 3.5652\n","Iteration: 992; Percent complete: 24.8%; Average loss: 3.6511\n","Iteration: 993; Percent complete: 24.8%; Average loss: 3.4113\n","Iteration: 994; Percent complete: 24.9%; Average loss: 3.5518\n","Iteration: 995; Percent complete: 24.9%; Average loss: 3.4983\n","Iteration: 996; Percent complete: 24.9%; Average loss: 3.4873\n","Iteration: 997; Percent complete: 24.9%; Average loss: 3.1517\n","Iteration: 998; Percent complete: 24.9%; Average loss: 3.4658\n","Iteration: 999; Percent complete: 25.0%; Average loss: 3.7716\n","Iteration: 1000; Percent complete: 25.0%; Average loss: 3.4565\n","Iteration: 1001; Percent complete: 25.0%; Average loss: 3.5513\n","Iteration: 1002; Percent complete: 25.1%; Average loss: 3.2909\n","Iteration: 1003; Percent complete: 25.1%; Average loss: 3.5461\n","Iteration: 1004; Percent complete: 25.1%; Average loss: 3.7247\n","Iteration: 1005; Percent complete: 25.1%; Average loss: 3.5253\n","Iteration: 1006; Percent complete: 25.1%; Average loss: 3.5381\n","Iteration: 1007; Percent complete: 25.2%; Average loss: 3.2866\n","Iteration: 1008; Percent complete: 25.2%; Average loss: 3.4387\n","Iteration: 1009; Percent complete: 25.2%; Average loss: 3.3581\n","Iteration: 1010; Percent complete: 25.2%; Average loss: 3.3697\n","Iteration: 1011; Percent complete: 25.3%; Average loss: 3.5715\n","Iteration: 1012; Percent complete: 25.3%; Average loss: 3.3380\n","Iteration: 1013; Percent complete: 25.3%; Average loss: 3.3727\n","Iteration: 1014; Percent complete: 25.4%; Average loss: 3.6450\n","Iteration: 1015; Percent complete: 25.4%; Average loss: 3.3802\n","Iteration: 1016; Percent complete: 25.4%; Average loss: 3.2927\n","Iteration: 1017; Percent complete: 25.4%; Average loss: 3.6200\n","Iteration: 1018; Percent complete: 25.4%; Average loss: 3.4722\n","Iteration: 1019; Percent complete: 25.5%; Average loss: 3.5667\n","Iteration: 1020; Percent complete: 25.5%; Average loss: 3.5673\n","Iteration: 1021; Percent complete: 25.5%; Average loss: 3.3198\n","Iteration: 1022; Percent complete: 25.6%; Average loss: 3.2239\n","Iteration: 1023; Percent complete: 25.6%; Average loss: 3.4038\n","Iteration: 1024; Percent complete: 25.6%; Average loss: 3.4463\n","Iteration: 1025; Percent complete: 25.6%; Average loss: 3.5982\n","Iteration: 1026; Percent complete: 25.7%; Average loss: 3.5580\n","Iteration: 1027; Percent complete: 25.7%; Average loss: 3.7070\n","Iteration: 1028; Percent complete: 25.7%; Average loss: 3.5084\n","Iteration: 1029; Percent complete: 25.7%; Average loss: 3.3604\n","Iteration: 1030; Percent complete: 25.8%; Average loss: 3.3934\n","Iteration: 1031; Percent complete: 25.8%; Average loss: 3.4305\n","Iteration: 1032; Percent complete: 25.8%; Average loss: 3.8261\n","Iteration: 1033; Percent complete: 25.8%; Average loss: 3.3517\n","Iteration: 1034; Percent complete: 25.9%; Average loss: 3.4806\n","Iteration: 1035; Percent complete: 25.9%; Average loss: 3.5923\n","Iteration: 1036; Percent complete: 25.9%; Average loss: 3.4894\n","Iteration: 1037; Percent complete: 25.9%; Average loss: 3.4317\n","Iteration: 1038; Percent complete: 25.9%; Average loss: 3.4377\n","Iteration: 1039; Percent complete: 26.0%; Average loss: 3.4393\n","Iteration: 1040; Percent complete: 26.0%; Average loss: 3.2677\n","Iteration: 1041; Percent complete: 26.0%; Average loss: 3.4778\n","Iteration: 1042; Percent complete: 26.1%; Average loss: 3.5995\n","Iteration: 1043; Percent complete: 26.1%; Average loss: 3.4027\n","Iteration: 1044; Percent complete: 26.1%; Average loss: 3.5819\n","Iteration: 1045; Percent complete: 26.1%; Average loss: 3.4672\n","Iteration: 1046; Percent complete: 26.2%; Average loss: 3.4778\n","Iteration: 1047; Percent complete: 26.2%; Average loss: 3.4657\n","Iteration: 1048; Percent complete: 26.2%; Average loss: 3.3601\n","Iteration: 1049; Percent complete: 26.2%; Average loss: 3.5869\n","Iteration: 1050; Percent complete: 26.2%; Average loss: 3.3904\n","Iteration: 1051; Percent complete: 26.3%; Average loss: 3.7169\n","Iteration: 1052; Percent complete: 26.3%; Average loss: 3.7147\n","Iteration: 1053; Percent complete: 26.3%; Average loss: 3.3324\n","Iteration: 1054; Percent complete: 26.4%; Average loss: 3.6103\n","Iteration: 1055; Percent complete: 26.4%; Average loss: 3.4183\n","Iteration: 1056; Percent complete: 26.4%; Average loss: 3.3799\n","Iteration: 1057; Percent complete: 26.4%; Average loss: 3.4862\n","Iteration: 1058; Percent complete: 26.5%; Average loss: 3.3213\n","Iteration: 1059; Percent complete: 26.5%; Average loss: 3.3304\n","Iteration: 1060; Percent complete: 26.5%; Average loss: 3.4161\n","Iteration: 1061; Percent complete: 26.5%; Average loss: 3.5450\n","Iteration: 1062; Percent complete: 26.6%; Average loss: 3.0125\n","Iteration: 1063; Percent complete: 26.6%; Average loss: 3.4073\n","Iteration: 1064; Percent complete: 26.6%; Average loss: 3.6486\n","Iteration: 1065; Percent complete: 26.6%; Average loss: 3.1009\n","Iteration: 1066; Percent complete: 26.7%; Average loss: 3.5692\n","Iteration: 1067; Percent complete: 26.7%; Average loss: 3.5294\n","Iteration: 1068; Percent complete: 26.7%; Average loss: 3.4640\n","Iteration: 1069; Percent complete: 26.7%; Average loss: 3.3319\n","Iteration: 1070; Percent complete: 26.8%; Average loss: 3.6102\n","Iteration: 1071; Percent complete: 26.8%; Average loss: 3.2229\n","Iteration: 1072; Percent complete: 26.8%; Average loss: 3.3406\n","Iteration: 1073; Percent complete: 26.8%; Average loss: 3.4203\n","Iteration: 1074; Percent complete: 26.9%; Average loss: 3.4667\n","Iteration: 1075; Percent complete: 26.9%; Average loss: 3.3303\n","Iteration: 1076; Percent complete: 26.9%; Average loss: 3.6562\n","Iteration: 1077; Percent complete: 26.9%; Average loss: 3.4128\n","Iteration: 1078; Percent complete: 27.0%; Average loss: 3.5351\n","Iteration: 1079; Percent complete: 27.0%; Average loss: 3.2266\n","Iteration: 1080; Percent complete: 27.0%; Average loss: 3.6694\n","Iteration: 1081; Percent complete: 27.0%; Average loss: 3.4192\n","Iteration: 1082; Percent complete: 27.1%; Average loss: 3.4473\n","Iteration: 1083; Percent complete: 27.1%; Average loss: 3.3981\n","Iteration: 1084; Percent complete: 27.1%; Average loss: 3.6249\n","Iteration: 1085; Percent complete: 27.1%; Average loss: 3.2700\n","Iteration: 1086; Percent complete: 27.2%; Average loss: 3.4197\n","Iteration: 1087; Percent complete: 27.2%; Average loss: 3.3468\n","Iteration: 1088; Percent complete: 27.2%; Average loss: 3.5008\n","Iteration: 1089; Percent complete: 27.2%; Average loss: 3.4656\n","Iteration: 1090; Percent complete: 27.3%; Average loss: 3.4692\n","Iteration: 1091; Percent complete: 27.3%; Average loss: 3.6160\n","Iteration: 1092; Percent complete: 27.3%; Average loss: 3.4283\n","Iteration: 1093; Percent complete: 27.3%; Average loss: 3.2378\n","Iteration: 1094; Percent complete: 27.4%; Average loss: 3.4109\n","Iteration: 1095; Percent complete: 27.4%; Average loss: 3.3142\n","Iteration: 1096; Percent complete: 27.4%; Average loss: 3.4179\n","Iteration: 1097; Percent complete: 27.4%; Average loss: 3.5625\n","Iteration: 1098; Percent complete: 27.5%; Average loss: 3.3714\n","Iteration: 1099; Percent complete: 27.5%; Average loss: 3.6651\n","Iteration: 1100; Percent complete: 27.5%; Average loss: 3.5688\n","Iteration: 1101; Percent complete: 27.5%; Average loss: 3.2629\n","Iteration: 1102; Percent complete: 27.6%; Average loss: 3.5085\n","Iteration: 1103; Percent complete: 27.6%; Average loss: 3.5794\n","Iteration: 1104; Percent complete: 27.6%; Average loss: 3.7645\n","Iteration: 1105; Percent complete: 27.6%; Average loss: 3.4959\n","Iteration: 1106; Percent complete: 27.7%; Average loss: 3.5846\n","Iteration: 1107; Percent complete: 27.7%; Average loss: 3.4514\n","Iteration: 1108; Percent complete: 27.7%; Average loss: 3.3566\n","Iteration: 1109; Percent complete: 27.7%; Average loss: 3.3884\n","Iteration: 1110; Percent complete: 27.8%; Average loss: 3.3364\n","Iteration: 1111; Percent complete: 27.8%; Average loss: 3.5986\n","Iteration: 1112; Percent complete: 27.8%; Average loss: 3.4690\n","Iteration: 1113; Percent complete: 27.8%; Average loss: 3.4376\n","Iteration: 1114; Percent complete: 27.9%; Average loss: 3.6217\n","Iteration: 1115; Percent complete: 27.9%; Average loss: 3.1439\n","Iteration: 1116; Percent complete: 27.9%; Average loss: 3.3045\n","Iteration: 1117; Percent complete: 27.9%; Average loss: 3.5621\n","Iteration: 1118; Percent complete: 28.0%; Average loss: 3.5926\n","Iteration: 1119; Percent complete: 28.0%; Average loss: 3.2964\n","Iteration: 1120; Percent complete: 28.0%; Average loss: 3.3947\n","Iteration: 1121; Percent complete: 28.0%; Average loss: 3.4041\n","Iteration: 1122; Percent complete: 28.1%; Average loss: 3.3008\n","Iteration: 1123; Percent complete: 28.1%; Average loss: 3.1450\n","Iteration: 1124; Percent complete: 28.1%; Average loss: 3.1649\n","Iteration: 1125; Percent complete: 28.1%; Average loss: 3.4777\n","Iteration: 1126; Percent complete: 28.1%; Average loss: 3.4091\n","Iteration: 1127; Percent complete: 28.2%; Average loss: 3.0406\n","Iteration: 1128; Percent complete: 28.2%; Average loss: 3.6651\n","Iteration: 1129; Percent complete: 28.2%; Average loss: 3.4904\n","Iteration: 1130; Percent complete: 28.2%; Average loss: 3.3998\n","Iteration: 1131; Percent complete: 28.3%; Average loss: 3.2704\n","Iteration: 1132; Percent complete: 28.3%; Average loss: 3.5624\n","Iteration: 1133; Percent complete: 28.3%; Average loss: 3.5649\n","Iteration: 1134; Percent complete: 28.3%; Average loss: 3.4180\n","Iteration: 1135; Percent complete: 28.4%; Average loss: 3.3057\n","Iteration: 1136; Percent complete: 28.4%; Average loss: 3.2559\n","Iteration: 1137; Percent complete: 28.4%; Average loss: 3.5187\n","Iteration: 1138; Percent complete: 28.4%; Average loss: 3.5550\n","Iteration: 1139; Percent complete: 28.5%; Average loss: 3.0867\n","Iteration: 1140; Percent complete: 28.5%; Average loss: 3.4352\n","Iteration: 1141; Percent complete: 28.5%; Average loss: 3.3814\n","Iteration: 1142; Percent complete: 28.5%; Average loss: 3.6913\n","Iteration: 1143; Percent complete: 28.6%; Average loss: 3.4644\n","Iteration: 1144; Percent complete: 28.6%; Average loss: 3.5599\n","Iteration: 1145; Percent complete: 28.6%; Average loss: 3.2538\n","Iteration: 1146; Percent complete: 28.6%; Average loss: 3.2405\n","Iteration: 1147; Percent complete: 28.7%; Average loss: 3.7670\n","Iteration: 1148; Percent complete: 28.7%; Average loss: 3.3726\n","Iteration: 1149; Percent complete: 28.7%; Average loss: 3.3586\n","Iteration: 1150; Percent complete: 28.7%; Average loss: 3.4166\n","Iteration: 1151; Percent complete: 28.8%; Average loss: 3.5490\n","Iteration: 1152; Percent complete: 28.8%; Average loss: 3.3517\n","Iteration: 1153; Percent complete: 28.8%; Average loss: 3.5634\n","Iteration: 1154; Percent complete: 28.8%; Average loss: 3.4462\n","Iteration: 1155; Percent complete: 28.9%; Average loss: 3.4616\n","Iteration: 1156; Percent complete: 28.9%; Average loss: 3.3560\n","Iteration: 1157; Percent complete: 28.9%; Average loss: 3.1948\n","Iteration: 1158; Percent complete: 28.9%; Average loss: 3.5389\n","Iteration: 1159; Percent complete: 29.0%; Average loss: 3.4017\n","Iteration: 1160; Percent complete: 29.0%; Average loss: 3.4388\n","Iteration: 1161; Percent complete: 29.0%; Average loss: 3.2867\n","Iteration: 1162; Percent complete: 29.0%; Average loss: 3.4614\n","Iteration: 1163; Percent complete: 29.1%; Average loss: 3.4799\n","Iteration: 1164; Percent complete: 29.1%; Average loss: 3.4722\n","Iteration: 1165; Percent complete: 29.1%; Average loss: 3.6793\n","Iteration: 1166; Percent complete: 29.1%; Average loss: 3.3127\n","Iteration: 1167; Percent complete: 29.2%; Average loss: 3.6236\n","Iteration: 1168; Percent complete: 29.2%; Average loss: 3.3552\n","Iteration: 1169; Percent complete: 29.2%; Average loss: 3.4384\n","Iteration: 1170; Percent complete: 29.2%; Average loss: 3.5002\n","Iteration: 1171; Percent complete: 29.3%; Average loss: 3.1872\n","Iteration: 1172; Percent complete: 29.3%; Average loss: 3.6539\n","Iteration: 1173; Percent complete: 29.3%; Average loss: 3.2638\n","Iteration: 1174; Percent complete: 29.3%; Average loss: 3.4859\n","Iteration: 1175; Percent complete: 29.4%; Average loss: 3.4711\n","Iteration: 1176; Percent complete: 29.4%; Average loss: 3.5044\n","Iteration: 1177; Percent complete: 29.4%; Average loss: 3.0635\n","Iteration: 1178; Percent complete: 29.4%; Average loss: 3.4259\n","Iteration: 1179; Percent complete: 29.5%; Average loss: 3.1622\n","Iteration: 1180; Percent complete: 29.5%; Average loss: 3.5929\n","Iteration: 1181; Percent complete: 29.5%; Average loss: 3.2972\n","Iteration: 1182; Percent complete: 29.5%; Average loss: 3.4156\n","Iteration: 1183; Percent complete: 29.6%; Average loss: 3.4746\n","Iteration: 1184; Percent complete: 29.6%; Average loss: 3.3321\n","Iteration: 1185; Percent complete: 29.6%; Average loss: 3.1710\n","Iteration: 1186; Percent complete: 29.6%; Average loss: 3.4009\n","Iteration: 1187; Percent complete: 29.7%; Average loss: 3.4622\n","Iteration: 1188; Percent complete: 29.7%; Average loss: 3.4697\n","Iteration: 1189; Percent complete: 29.7%; Average loss: 3.2027\n","Iteration: 1190; Percent complete: 29.8%; Average loss: 3.5442\n","Iteration: 1191; Percent complete: 29.8%; Average loss: 3.5120\n","Iteration: 1192; Percent complete: 29.8%; Average loss: 3.1891\n","Iteration: 1193; Percent complete: 29.8%; Average loss: 3.4302\n","Iteration: 1194; Percent complete: 29.8%; Average loss: 3.2736\n","Iteration: 1195; Percent complete: 29.9%; Average loss: 3.5560\n","Iteration: 1196; Percent complete: 29.9%; Average loss: 3.4897\n","Iteration: 1197; Percent complete: 29.9%; Average loss: 3.4634\n","Iteration: 1198; Percent complete: 29.9%; Average loss: 3.4070\n","Iteration: 1199; Percent complete: 30.0%; Average loss: 3.4422\n","Iteration: 1200; Percent complete: 30.0%; Average loss: 3.4717\n","Iteration: 1201; Percent complete: 30.0%; Average loss: 3.3338\n","Iteration: 1202; Percent complete: 30.0%; Average loss: 3.4254\n","Iteration: 1203; Percent complete: 30.1%; Average loss: 3.4856\n","Iteration: 1204; Percent complete: 30.1%; Average loss: 3.5186\n","Iteration: 1205; Percent complete: 30.1%; Average loss: 3.2861\n","Iteration: 1206; Percent complete: 30.1%; Average loss: 3.3784\n","Iteration: 1207; Percent complete: 30.2%; Average loss: 3.2223\n","Iteration: 1208; Percent complete: 30.2%; Average loss: 3.3668\n","Iteration: 1209; Percent complete: 30.2%; Average loss: 3.3447\n","Iteration: 1210; Percent complete: 30.2%; Average loss: 3.5926\n","Iteration: 1211; Percent complete: 30.3%; Average loss: 3.3153\n","Iteration: 1212; Percent complete: 30.3%; Average loss: 3.7615\n","Iteration: 1213; Percent complete: 30.3%; Average loss: 3.4387\n","Iteration: 1214; Percent complete: 30.3%; Average loss: 3.5209\n","Iteration: 1215; Percent complete: 30.4%; Average loss: 3.1249\n","Iteration: 1216; Percent complete: 30.4%; Average loss: 3.3900\n","Iteration: 1217; Percent complete: 30.4%; Average loss: 3.2651\n","Iteration: 1218; Percent complete: 30.4%; Average loss: 3.2815\n","Iteration: 1219; Percent complete: 30.5%; Average loss: 3.4490\n","Iteration: 1220; Percent complete: 30.5%; Average loss: 3.5357\n","Iteration: 1221; Percent complete: 30.5%; Average loss: 3.3173\n","Iteration: 1222; Percent complete: 30.6%; Average loss: 3.2974\n","Iteration: 1223; Percent complete: 30.6%; Average loss: 3.2654\n","Iteration: 1224; Percent complete: 30.6%; Average loss: 3.4673\n","Iteration: 1225; Percent complete: 30.6%; Average loss: 3.2964\n","Iteration: 1226; Percent complete: 30.6%; Average loss: 3.1667\n","Iteration: 1227; Percent complete: 30.7%; Average loss: 3.4895\n","Iteration: 1228; Percent complete: 30.7%; Average loss: 3.4267\n","Iteration: 1229; Percent complete: 30.7%; Average loss: 3.1686\n","Iteration: 1230; Percent complete: 30.8%; Average loss: 3.3102\n","Iteration: 1231; Percent complete: 30.8%; Average loss: 3.2466\n","Iteration: 1232; Percent complete: 30.8%; Average loss: 3.3281\n","Iteration: 1233; Percent complete: 30.8%; Average loss: 3.4577\n","Iteration: 1234; Percent complete: 30.9%; Average loss: 3.4666\n","Iteration: 1235; Percent complete: 30.9%; Average loss: 3.2725\n","Iteration: 1236; Percent complete: 30.9%; Average loss: 3.3270\n","Iteration: 1237; Percent complete: 30.9%; Average loss: 3.7152\n","Iteration: 1238; Percent complete: 30.9%; Average loss: 3.2674\n","Iteration: 1239; Percent complete: 31.0%; Average loss: 3.2974\n","Iteration: 1240; Percent complete: 31.0%; Average loss: 3.5269\n","Iteration: 1241; Percent complete: 31.0%; Average loss: 3.6176\n","Iteration: 1242; Percent complete: 31.1%; Average loss: 3.2097\n","Iteration: 1243; Percent complete: 31.1%; Average loss: 3.4944\n","Iteration: 1244; Percent complete: 31.1%; Average loss: 3.2892\n","Iteration: 1245; Percent complete: 31.1%; Average loss: 3.5447\n","Iteration: 1246; Percent complete: 31.1%; Average loss: 3.3018\n","Iteration: 1247; Percent complete: 31.2%; Average loss: 3.4208\n","Iteration: 1248; Percent complete: 31.2%; Average loss: 3.1927\n","Iteration: 1249; Percent complete: 31.2%; Average loss: 3.4636\n","Iteration: 1250; Percent complete: 31.2%; Average loss: 3.4981\n","Iteration: 1251; Percent complete: 31.3%; Average loss: 3.4419\n","Iteration: 1252; Percent complete: 31.3%; Average loss: 3.2255\n","Iteration: 1253; Percent complete: 31.3%; Average loss: 3.4404\n","Iteration: 1254; Percent complete: 31.4%; Average loss: 3.5158\n","Iteration: 1255; Percent complete: 31.4%; Average loss: 3.3941\n","Iteration: 1256; Percent complete: 31.4%; Average loss: 3.4351\n","Iteration: 1257; Percent complete: 31.4%; Average loss: 3.4604\n","Iteration: 1258; Percent complete: 31.4%; Average loss: 3.2131\n","Iteration: 1259; Percent complete: 31.5%; Average loss: 3.4206\n","Iteration: 1260; Percent complete: 31.5%; Average loss: 3.2618\n","Iteration: 1261; Percent complete: 31.5%; Average loss: 3.3733\n","Iteration: 1262; Percent complete: 31.6%; Average loss: 3.3237\n","Iteration: 1263; Percent complete: 31.6%; Average loss: 3.0395\n","Iteration: 1264; Percent complete: 31.6%; Average loss: 3.5426\n","Iteration: 1265; Percent complete: 31.6%; Average loss: 3.5458\n","Iteration: 1266; Percent complete: 31.6%; Average loss: 3.3012\n","Iteration: 1267; Percent complete: 31.7%; Average loss: 3.4341\n","Iteration: 1268; Percent complete: 31.7%; Average loss: 3.4316\n","Iteration: 1269; Percent complete: 31.7%; Average loss: 3.1820\n","Iteration: 1270; Percent complete: 31.8%; Average loss: 3.3220\n","Iteration: 1271; Percent complete: 31.8%; Average loss: 3.6884\n","Iteration: 1272; Percent complete: 31.8%; Average loss: 3.2640\n","Iteration: 1273; Percent complete: 31.8%; Average loss: 3.2434\n","Iteration: 1274; Percent complete: 31.9%; Average loss: 3.1706\n","Iteration: 1275; Percent complete: 31.9%; Average loss: 3.4568\n","Iteration: 1276; Percent complete: 31.9%; Average loss: 3.3660\n","Iteration: 1277; Percent complete: 31.9%; Average loss: 3.4750\n","Iteration: 1278; Percent complete: 31.9%; Average loss: 3.3094\n","Iteration: 1279; Percent complete: 32.0%; Average loss: 3.4951\n","Iteration: 1280; Percent complete: 32.0%; Average loss: 3.7656\n","Iteration: 1281; Percent complete: 32.0%; Average loss: 3.2710\n","Iteration: 1282; Percent complete: 32.0%; Average loss: 3.2246\n","Iteration: 1283; Percent complete: 32.1%; Average loss: 3.2587\n","Iteration: 1284; Percent complete: 32.1%; Average loss: 3.4332\n","Iteration: 1285; Percent complete: 32.1%; Average loss: 3.3906\n","Iteration: 1286; Percent complete: 32.1%; Average loss: 3.2075\n","Iteration: 1287; Percent complete: 32.2%; Average loss: 3.3094\n","Iteration: 1288; Percent complete: 32.2%; Average loss: 3.3274\n","Iteration: 1289; Percent complete: 32.2%; Average loss: 3.6164\n","Iteration: 1290; Percent complete: 32.2%; Average loss: 3.2019\n","Iteration: 1291; Percent complete: 32.3%; Average loss: 3.4564\n","Iteration: 1292; Percent complete: 32.3%; Average loss: 3.2723\n","Iteration: 1293; Percent complete: 32.3%; Average loss: 3.5524\n","Iteration: 1294; Percent complete: 32.4%; Average loss: 3.3454\n","Iteration: 1295; Percent complete: 32.4%; Average loss: 3.1911\n","Iteration: 1296; Percent complete: 32.4%; Average loss: 3.3381\n","Iteration: 1297; Percent complete: 32.4%; Average loss: 3.2628\n","Iteration: 1298; Percent complete: 32.5%; Average loss: 3.7068\n","Iteration: 1299; Percent complete: 32.5%; Average loss: 3.3113\n","Iteration: 1300; Percent complete: 32.5%; Average loss: 3.4636\n","Iteration: 1301; Percent complete: 32.5%; Average loss: 3.0880\n","Iteration: 1302; Percent complete: 32.6%; Average loss: 3.2167\n","Iteration: 1303; Percent complete: 32.6%; Average loss: 3.2461\n","Iteration: 1304; Percent complete: 32.6%; Average loss: 3.5257\n","Iteration: 1305; Percent complete: 32.6%; Average loss: 3.3870\n","Iteration: 1306; Percent complete: 32.6%; Average loss: 3.4313\n","Iteration: 1307; Percent complete: 32.7%; Average loss: 3.2802\n","Iteration: 1308; Percent complete: 32.7%; Average loss: 3.0357\n","Iteration: 1309; Percent complete: 32.7%; Average loss: 3.3349\n","Iteration: 1310; Percent complete: 32.8%; Average loss: 3.2358\n","Iteration: 1311; Percent complete: 32.8%; Average loss: 3.1875\n","Iteration: 1312; Percent complete: 32.8%; Average loss: 3.4294\n","Iteration: 1313; Percent complete: 32.8%; Average loss: 3.2273\n","Iteration: 1314; Percent complete: 32.9%; Average loss: 3.3383\n","Iteration: 1315; Percent complete: 32.9%; Average loss: 3.1654\n","Iteration: 1316; Percent complete: 32.9%; Average loss: 3.4671\n","Iteration: 1317; Percent complete: 32.9%; Average loss: 3.1654\n","Iteration: 1318; Percent complete: 33.0%; Average loss: 3.6369\n","Iteration: 1319; Percent complete: 33.0%; Average loss: 3.3019\n","Iteration: 1320; Percent complete: 33.0%; Average loss: 3.1121\n","Iteration: 1321; Percent complete: 33.0%; Average loss: 3.3213\n","Iteration: 1322; Percent complete: 33.1%; Average loss: 3.2254\n","Iteration: 1323; Percent complete: 33.1%; Average loss: 3.2646\n","Iteration: 1324; Percent complete: 33.1%; Average loss: 3.3884\n","Iteration: 1325; Percent complete: 33.1%; Average loss: 3.1391\n","Iteration: 1326; Percent complete: 33.1%; Average loss: 3.5396\n","Iteration: 1327; Percent complete: 33.2%; Average loss: 3.1950\n","Iteration: 1328; Percent complete: 33.2%; Average loss: 3.1566\n","Iteration: 1329; Percent complete: 33.2%; Average loss: 3.4359\n","Iteration: 1330; Percent complete: 33.2%; Average loss: 3.3383\n","Iteration: 1331; Percent complete: 33.3%; Average loss: 3.7192\n","Iteration: 1332; Percent complete: 33.3%; Average loss: 3.1584\n","Iteration: 1333; Percent complete: 33.3%; Average loss: 3.3063\n","Iteration: 1334; Percent complete: 33.4%; Average loss: 3.5580\n","Iteration: 1335; Percent complete: 33.4%; Average loss: 3.4924\n","Iteration: 1336; Percent complete: 33.4%; Average loss: 3.4207\n","Iteration: 1337; Percent complete: 33.4%; Average loss: 3.2253\n","Iteration: 1338; Percent complete: 33.5%; Average loss: 3.5743\n","Iteration: 1339; Percent complete: 33.5%; Average loss: 3.2381\n","Iteration: 1340; Percent complete: 33.5%; Average loss: 3.5628\n","Iteration: 1341; Percent complete: 33.5%; Average loss: 3.8834\n","Iteration: 1342; Percent complete: 33.6%; Average loss: 3.5346\n","Iteration: 1343; Percent complete: 33.6%; Average loss: 3.3478\n","Iteration: 1344; Percent complete: 33.6%; Average loss: 3.1233\n","Iteration: 1345; Percent complete: 33.6%; Average loss: 3.4457\n","Iteration: 1346; Percent complete: 33.7%; Average loss: 3.0895\n","Iteration: 1347; Percent complete: 33.7%; Average loss: 3.4356\n","Iteration: 1348; Percent complete: 33.7%; Average loss: 3.3228\n","Iteration: 1349; Percent complete: 33.7%; Average loss: 3.4557\n","Iteration: 1350; Percent complete: 33.8%; Average loss: 3.1528\n","Iteration: 1351; Percent complete: 33.8%; Average loss: 3.3740\n","Iteration: 1352; Percent complete: 33.8%; Average loss: 3.3057\n","Iteration: 1353; Percent complete: 33.8%; Average loss: 3.2069\n","Iteration: 1354; Percent complete: 33.9%; Average loss: 3.3903\n","Iteration: 1355; Percent complete: 33.9%; Average loss: 3.3711\n","Iteration: 1356; Percent complete: 33.9%; Average loss: 3.3640\n","Iteration: 1357; Percent complete: 33.9%; Average loss: 3.0358\n","Iteration: 1358; Percent complete: 34.0%; Average loss: 3.3123\n","Iteration: 1359; Percent complete: 34.0%; Average loss: 3.3529\n","Iteration: 1360; Percent complete: 34.0%; Average loss: 3.4782\n","Iteration: 1361; Percent complete: 34.0%; Average loss: 3.3887\n","Iteration: 1362; Percent complete: 34.1%; Average loss: 3.3121\n","Iteration: 1363; Percent complete: 34.1%; Average loss: 3.2619\n","Iteration: 1364; Percent complete: 34.1%; Average loss: 3.2721\n","Iteration: 1365; Percent complete: 34.1%; Average loss: 3.1772\n","Iteration: 1366; Percent complete: 34.2%; Average loss: 3.3493\n","Iteration: 1367; Percent complete: 34.2%; Average loss: 3.4762\n","Iteration: 1368; Percent complete: 34.2%; Average loss: 3.6000\n","Iteration: 1369; Percent complete: 34.2%; Average loss: 3.2873\n","Iteration: 1370; Percent complete: 34.2%; Average loss: 3.1924\n","Iteration: 1371; Percent complete: 34.3%; Average loss: 3.2421\n","Iteration: 1372; Percent complete: 34.3%; Average loss: 3.4584\n","Iteration: 1373; Percent complete: 34.3%; Average loss: 3.4560\n","Iteration: 1374; Percent complete: 34.4%; Average loss: 3.0393\n","Iteration: 1375; Percent complete: 34.4%; Average loss: 3.4821\n","Iteration: 1376; Percent complete: 34.4%; Average loss: 3.4469\n","Iteration: 1377; Percent complete: 34.4%; Average loss: 3.1620\n","Iteration: 1378; Percent complete: 34.4%; Average loss: 3.3608\n","Iteration: 1379; Percent complete: 34.5%; Average loss: 3.2315\n","Iteration: 1380; Percent complete: 34.5%; Average loss: 3.3265\n","Iteration: 1381; Percent complete: 34.5%; Average loss: 3.6502\n","Iteration: 1382; Percent complete: 34.5%; Average loss: 3.2348\n","Iteration: 1383; Percent complete: 34.6%; Average loss: 3.4669\n","Iteration: 1384; Percent complete: 34.6%; Average loss: 3.3645\n","Iteration: 1385; Percent complete: 34.6%; Average loss: 3.2195\n","Iteration: 1386; Percent complete: 34.6%; Average loss: 3.4736\n","Iteration: 1387; Percent complete: 34.7%; Average loss: 3.5134\n","Iteration: 1388; Percent complete: 34.7%; Average loss: 3.3907\n","Iteration: 1389; Percent complete: 34.7%; Average loss: 3.1769\n","Iteration: 1390; Percent complete: 34.8%; Average loss: 3.2412\n","Iteration: 1391; Percent complete: 34.8%; Average loss: 3.2788\n","Iteration: 1392; Percent complete: 34.8%; Average loss: 3.3484\n","Iteration: 1393; Percent complete: 34.8%; Average loss: 3.0618\n","Iteration: 1394; Percent complete: 34.8%; Average loss: 3.0796\n","Iteration: 1395; Percent complete: 34.9%; Average loss: 3.1385\n","Iteration: 1396; Percent complete: 34.9%; Average loss: 3.3700\n","Iteration: 1397; Percent complete: 34.9%; Average loss: 3.4225\n","Iteration: 1398; Percent complete: 34.9%; Average loss: 3.4297\n","Iteration: 1399; Percent complete: 35.0%; Average loss: 3.6323\n","Iteration: 1400; Percent complete: 35.0%; Average loss: 3.4356\n","Iteration: 1401; Percent complete: 35.0%; Average loss: 3.1506\n","Iteration: 1402; Percent complete: 35.0%; Average loss: 3.4973\n","Iteration: 1403; Percent complete: 35.1%; Average loss: 3.2823\n","Iteration: 1404; Percent complete: 35.1%; Average loss: 3.2064\n","Iteration: 1405; Percent complete: 35.1%; Average loss: 3.2251\n","Iteration: 1406; Percent complete: 35.1%; Average loss: 3.2843\n","Iteration: 1407; Percent complete: 35.2%; Average loss: 3.2887\n","Iteration: 1408; Percent complete: 35.2%; Average loss: 3.5212\n","Iteration: 1409; Percent complete: 35.2%; Average loss: 3.0926\n","Iteration: 1410; Percent complete: 35.2%; Average loss: 3.1799\n","Iteration: 1411; Percent complete: 35.3%; Average loss: 3.2768\n","Iteration: 1412; Percent complete: 35.3%; Average loss: 3.3363\n","Iteration: 1413; Percent complete: 35.3%; Average loss: 3.1662\n","Iteration: 1414; Percent complete: 35.4%; Average loss: 3.2278\n","Iteration: 1415; Percent complete: 35.4%; Average loss: 3.1140\n","Iteration: 1416; Percent complete: 35.4%; Average loss: 3.4443\n","Iteration: 1417; Percent complete: 35.4%; Average loss: 3.4630\n","Iteration: 1418; Percent complete: 35.4%; Average loss: 3.3462\n","Iteration: 1419; Percent complete: 35.5%; Average loss: 3.3624\n","Iteration: 1420; Percent complete: 35.5%; Average loss: 3.5642\n","Iteration: 1421; Percent complete: 35.5%; Average loss: 3.3048\n","Iteration: 1422; Percent complete: 35.5%; Average loss: 3.2066\n","Iteration: 1423; Percent complete: 35.6%; Average loss: 3.3944\n","Iteration: 1424; Percent complete: 35.6%; Average loss: 3.3680\n","Iteration: 1425; Percent complete: 35.6%; Average loss: 3.3750\n","Iteration: 1426; Percent complete: 35.6%; Average loss: 3.2832\n","Iteration: 1427; Percent complete: 35.7%; Average loss: 3.4890\n","Iteration: 1428; Percent complete: 35.7%; Average loss: 3.1180\n","Iteration: 1429; Percent complete: 35.7%; Average loss: 3.2888\n","Iteration: 1430; Percent complete: 35.8%; Average loss: 3.1062\n","Iteration: 1431; Percent complete: 35.8%; Average loss: 3.1037\n","Iteration: 1432; Percent complete: 35.8%; Average loss: 3.0384\n","Iteration: 1433; Percent complete: 35.8%; Average loss: 3.0181\n","Iteration: 1434; Percent complete: 35.9%; Average loss: 3.1710\n","Iteration: 1435; Percent complete: 35.9%; Average loss: 3.2709\n","Iteration: 1436; Percent complete: 35.9%; Average loss: 3.2498\n","Iteration: 1437; Percent complete: 35.9%; Average loss: 3.2457\n","Iteration: 1438; Percent complete: 35.9%; Average loss: 3.3378\n","Iteration: 1439; Percent complete: 36.0%; Average loss: 3.5989\n","Iteration: 1440; Percent complete: 36.0%; Average loss: 3.2685\n","Iteration: 1441; Percent complete: 36.0%; Average loss: 3.2432\n","Iteration: 1442; Percent complete: 36.0%; Average loss: 3.3749\n","Iteration: 1443; Percent complete: 36.1%; Average loss: 3.1373\n","Iteration: 1444; Percent complete: 36.1%; Average loss: 3.4560\n","Iteration: 1445; Percent complete: 36.1%; Average loss: 3.2501\n","Iteration: 1446; Percent complete: 36.1%; Average loss: 3.3621\n","Iteration: 1447; Percent complete: 36.2%; Average loss: 3.3801\n","Iteration: 1448; Percent complete: 36.2%; Average loss: 3.1968\n","Iteration: 1449; Percent complete: 36.2%; Average loss: 3.3032\n","Iteration: 1450; Percent complete: 36.2%; Average loss: 3.4393\n","Iteration: 1451; Percent complete: 36.3%; Average loss: 3.0908\n","Iteration: 1452; Percent complete: 36.3%; Average loss: 3.2176\n","Iteration: 1453; Percent complete: 36.3%; Average loss: 3.2395\n","Iteration: 1454; Percent complete: 36.4%; Average loss: 3.3506\n","Iteration: 1455; Percent complete: 36.4%; Average loss: 3.4908\n","Iteration: 1456; Percent complete: 36.4%; Average loss: 3.2696\n","Iteration: 1457; Percent complete: 36.4%; Average loss: 3.2345\n","Iteration: 1458; Percent complete: 36.4%; Average loss: 3.1976\n","Iteration: 1459; Percent complete: 36.5%; Average loss: 3.2748\n","Iteration: 1460; Percent complete: 36.5%; Average loss: 3.1113\n","Iteration: 1461; Percent complete: 36.5%; Average loss: 3.1544\n","Iteration: 1462; Percent complete: 36.5%; Average loss: 3.1571\n","Iteration: 1463; Percent complete: 36.6%; Average loss: 3.5543\n","Iteration: 1464; Percent complete: 36.6%; Average loss: 3.0299\n","Iteration: 1465; Percent complete: 36.6%; Average loss: 3.4481\n","Iteration: 1466; Percent complete: 36.6%; Average loss: 3.2877\n","Iteration: 1467; Percent complete: 36.7%; Average loss: 3.0384\n","Iteration: 1468; Percent complete: 36.7%; Average loss: 3.2047\n","Iteration: 1469; Percent complete: 36.7%; Average loss: 3.2677\n","Iteration: 1470; Percent complete: 36.8%; Average loss: 3.1019\n","Iteration: 1471; Percent complete: 36.8%; Average loss: 3.2695\n","Iteration: 1472; Percent complete: 36.8%; Average loss: 3.3807\n","Iteration: 1473; Percent complete: 36.8%; Average loss: 3.2065\n","Iteration: 1474; Percent complete: 36.9%; Average loss: 3.0171\n","Iteration: 1475; Percent complete: 36.9%; Average loss: 3.0458\n","Iteration: 1476; Percent complete: 36.9%; Average loss: 3.3619\n","Iteration: 1477; Percent complete: 36.9%; Average loss: 3.2220\n","Iteration: 1478; Percent complete: 37.0%; Average loss: 3.5918\n","Iteration: 1479; Percent complete: 37.0%; Average loss: 3.4630\n","Iteration: 1480; Percent complete: 37.0%; Average loss: 3.3979\n","Iteration: 1481; Percent complete: 37.0%; Average loss: 3.2959\n","Iteration: 1482; Percent complete: 37.0%; Average loss: 3.3508\n","Iteration: 1483; Percent complete: 37.1%; Average loss: 3.1520\n","Iteration: 1484; Percent complete: 37.1%; Average loss: 3.5367\n","Iteration: 1485; Percent complete: 37.1%; Average loss: 3.4199\n","Iteration: 1486; Percent complete: 37.1%; Average loss: 3.0759\n","Iteration: 1487; Percent complete: 37.2%; Average loss: 3.1254\n","Iteration: 1488; Percent complete: 37.2%; Average loss: 3.6789\n","Iteration: 1489; Percent complete: 37.2%; Average loss: 3.5169\n","Iteration: 1490; Percent complete: 37.2%; Average loss: 3.2951\n","Iteration: 1491; Percent complete: 37.3%; Average loss: 3.4810\n","Iteration: 1492; Percent complete: 37.3%; Average loss: 3.3935\n","Iteration: 1493; Percent complete: 37.3%; Average loss: 3.3933\n","Iteration: 1494; Percent complete: 37.4%; Average loss: 3.0155\n","Iteration: 1495; Percent complete: 37.4%; Average loss: 3.0995\n","Iteration: 1496; Percent complete: 37.4%; Average loss: 3.2783\n","Iteration: 1497; Percent complete: 37.4%; Average loss: 3.3363\n","Iteration: 1498; Percent complete: 37.5%; Average loss: 3.0722\n","Iteration: 1499; Percent complete: 37.5%; Average loss: 3.2135\n","Iteration: 1500; Percent complete: 37.5%; Average loss: 2.8914\n","Iteration: 1501; Percent complete: 37.5%; Average loss: 3.1729\n","Iteration: 1502; Percent complete: 37.5%; Average loss: 3.0751\n","Iteration: 1503; Percent complete: 37.6%; Average loss: 3.2816\n","Iteration: 1504; Percent complete: 37.6%; Average loss: 3.2621\n","Iteration: 1505; Percent complete: 37.6%; Average loss: 3.1968\n","Iteration: 1506; Percent complete: 37.6%; Average loss: 3.2701\n","Iteration: 1507; Percent complete: 37.7%; Average loss: 3.1118\n","Iteration: 1508; Percent complete: 37.7%; Average loss: 3.4117\n","Iteration: 1509; Percent complete: 37.7%; Average loss: 3.4522\n","Iteration: 1510; Percent complete: 37.8%; Average loss: 3.3787\n","Iteration: 1511; Percent complete: 37.8%; Average loss: 3.1573\n","Iteration: 1512; Percent complete: 37.8%; Average loss: 3.1811\n","Iteration: 1513; Percent complete: 37.8%; Average loss: 3.1759\n","Iteration: 1514; Percent complete: 37.9%; Average loss: 3.2910\n","Iteration: 1515; Percent complete: 37.9%; Average loss: 3.4680\n","Iteration: 1516; Percent complete: 37.9%; Average loss: 3.2138\n","Iteration: 1517; Percent complete: 37.9%; Average loss: 3.3967\n","Iteration: 1518; Percent complete: 38.0%; Average loss: 3.4393\n","Iteration: 1519; Percent complete: 38.0%; Average loss: 3.3820\n","Iteration: 1520; Percent complete: 38.0%; Average loss: 3.4202\n","Iteration: 1521; Percent complete: 38.0%; Average loss: 3.1168\n","Iteration: 1522; Percent complete: 38.0%; Average loss: 3.2631\n","Iteration: 1523; Percent complete: 38.1%; Average loss: 3.2065\n","Iteration: 1524; Percent complete: 38.1%; Average loss: 3.1469\n","Iteration: 1525; Percent complete: 38.1%; Average loss: 3.2356\n","Iteration: 1526; Percent complete: 38.1%; Average loss: 3.5917\n","Iteration: 1527; Percent complete: 38.2%; Average loss: 3.5720\n","Iteration: 1528; Percent complete: 38.2%; Average loss: 3.2145\n","Iteration: 1529; Percent complete: 38.2%; Average loss: 3.4297\n","Iteration: 1530; Percent complete: 38.2%; Average loss: 3.3965\n","Iteration: 1531; Percent complete: 38.3%; Average loss: 3.3042\n","Iteration: 1532; Percent complete: 38.3%; Average loss: 3.0860\n","Iteration: 1533; Percent complete: 38.3%; Average loss: 3.1121\n","Iteration: 1534; Percent complete: 38.4%; Average loss: 2.9984\n","Iteration: 1535; Percent complete: 38.4%; Average loss: 3.4747\n","Iteration: 1536; Percent complete: 38.4%; Average loss: 3.3784\n","Iteration: 1537; Percent complete: 38.4%; Average loss: 3.2929\n","Iteration: 1538; Percent complete: 38.5%; Average loss: 3.3810\n","Iteration: 1539; Percent complete: 38.5%; Average loss: 3.2882\n","Iteration: 1540; Percent complete: 38.5%; Average loss: 3.0337\n","Iteration: 1541; Percent complete: 38.5%; Average loss: 3.2003\n","Iteration: 1542; Percent complete: 38.6%; Average loss: 3.1466\n","Iteration: 1543; Percent complete: 38.6%; Average loss: 3.4865\n","Iteration: 1544; Percent complete: 38.6%; Average loss: 3.1766\n","Iteration: 1545; Percent complete: 38.6%; Average loss: 3.3333\n","Iteration: 1546; Percent complete: 38.6%; Average loss: 3.2677\n","Iteration: 1547; Percent complete: 38.7%; Average loss: 3.4296\n","Iteration: 1548; Percent complete: 38.7%; Average loss: 3.3001\n","Iteration: 1549; Percent complete: 38.7%; Average loss: 3.3788\n","Iteration: 1550; Percent complete: 38.8%; Average loss: 3.3370\n","Iteration: 1551; Percent complete: 38.8%; Average loss: 3.2243\n","Iteration: 1552; Percent complete: 38.8%; Average loss: 3.1091\n","Iteration: 1553; Percent complete: 38.8%; Average loss: 3.1855\n","Iteration: 1554; Percent complete: 38.9%; Average loss: 3.5292\n","Iteration: 1555; Percent complete: 38.9%; Average loss: 3.2668\n","Iteration: 1556; Percent complete: 38.9%; Average loss: 3.3162\n","Iteration: 1557; Percent complete: 38.9%; Average loss: 3.1947\n","Iteration: 1558; Percent complete: 39.0%; Average loss: 3.5135\n","Iteration: 1559; Percent complete: 39.0%; Average loss: 3.1136\n","Iteration: 1560; Percent complete: 39.0%; Average loss: 3.3772\n","Iteration: 1561; Percent complete: 39.0%; Average loss: 3.3617\n","Iteration: 1562; Percent complete: 39.1%; Average loss: 3.1699\n","Iteration: 1563; Percent complete: 39.1%; Average loss: 3.1920\n","Iteration: 1564; Percent complete: 39.1%; Average loss: 3.2480\n","Iteration: 1565; Percent complete: 39.1%; Average loss: 3.5666\n","Iteration: 1566; Percent complete: 39.1%; Average loss: 3.3509\n","Iteration: 1567; Percent complete: 39.2%; Average loss: 3.3086\n","Iteration: 1568; Percent complete: 39.2%; Average loss: 3.3396\n","Iteration: 1569; Percent complete: 39.2%; Average loss: 3.4234\n","Iteration: 1570; Percent complete: 39.2%; Average loss: 3.0583\n","Iteration: 1571; Percent complete: 39.3%; Average loss: 3.1964\n","Iteration: 1572; Percent complete: 39.3%; Average loss: 3.3326\n","Iteration: 1573; Percent complete: 39.3%; Average loss: 3.4664\n","Iteration: 1574; Percent complete: 39.4%; Average loss: 3.1057\n","Iteration: 1575; Percent complete: 39.4%; Average loss: 3.2397\n","Iteration: 1576; Percent complete: 39.4%; Average loss: 3.2943\n","Iteration: 1577; Percent complete: 39.4%; Average loss: 3.3427\n","Iteration: 1578; Percent complete: 39.5%; Average loss: 3.2518\n","Iteration: 1579; Percent complete: 39.5%; Average loss: 3.1931\n","Iteration: 1580; Percent complete: 39.5%; Average loss: 3.1787\n","Iteration: 1581; Percent complete: 39.5%; Average loss: 3.0771\n","Iteration: 1582; Percent complete: 39.6%; Average loss: 3.2376\n","Iteration: 1583; Percent complete: 39.6%; Average loss: 3.2454\n","Iteration: 1584; Percent complete: 39.6%; Average loss: 3.2808\n","Iteration: 1585; Percent complete: 39.6%; Average loss: 3.3684\n","Iteration: 1586; Percent complete: 39.6%; Average loss: 3.2870\n","Iteration: 1587; Percent complete: 39.7%; Average loss: 2.9220\n","Iteration: 1588; Percent complete: 39.7%; Average loss: 3.4963\n","Iteration: 1589; Percent complete: 39.7%; Average loss: 3.2835\n","Iteration: 1590; Percent complete: 39.8%; Average loss: 3.0804\n","Iteration: 1591; Percent complete: 39.8%; Average loss: 3.4958\n","Iteration: 1592; Percent complete: 39.8%; Average loss: 3.1975\n","Iteration: 1593; Percent complete: 39.8%; Average loss: 3.1024\n","Iteration: 1594; Percent complete: 39.9%; Average loss: 3.2580\n","Iteration: 1595; Percent complete: 39.9%; Average loss: 3.2436\n","Iteration: 1596; Percent complete: 39.9%; Average loss: 3.3588\n","Iteration: 1597; Percent complete: 39.9%; Average loss: 3.1460\n","Iteration: 1598; Percent complete: 40.0%; Average loss: 3.2359\n","Iteration: 1599; Percent complete: 40.0%; Average loss: 3.4089\n","Iteration: 1600; Percent complete: 40.0%; Average loss: 3.4277\n","Iteration: 1601; Percent complete: 40.0%; Average loss: 3.4210\n","Iteration: 1602; Percent complete: 40.1%; Average loss: 3.3207\n","Iteration: 1603; Percent complete: 40.1%; Average loss: 3.3617\n","Iteration: 1604; Percent complete: 40.1%; Average loss: 3.2506\n","Iteration: 1605; Percent complete: 40.1%; Average loss: 3.2446\n","Iteration: 1606; Percent complete: 40.2%; Average loss: 3.1391\n","Iteration: 1607; Percent complete: 40.2%; Average loss: 3.4398\n","Iteration: 1608; Percent complete: 40.2%; Average loss: 3.1142\n","Iteration: 1609; Percent complete: 40.2%; Average loss: 3.2520\n","Iteration: 1610; Percent complete: 40.2%; Average loss: 3.2950\n","Iteration: 1611; Percent complete: 40.3%; Average loss: 3.1726\n","Iteration: 1612; Percent complete: 40.3%; Average loss: 3.3322\n","Iteration: 1613; Percent complete: 40.3%; Average loss: 3.2101\n","Iteration: 1614; Percent complete: 40.4%; Average loss: 3.4413\n","Iteration: 1615; Percent complete: 40.4%; Average loss: 3.2590\n","Iteration: 1616; Percent complete: 40.4%; Average loss: 3.5862\n","Iteration: 1617; Percent complete: 40.4%; Average loss: 3.3052\n","Iteration: 1618; Percent complete: 40.5%; Average loss: 3.1023\n","Iteration: 1619; Percent complete: 40.5%; Average loss: 3.1814\n","Iteration: 1620; Percent complete: 40.5%; Average loss: 3.3362\n","Iteration: 1621; Percent complete: 40.5%; Average loss: 3.1748\n","Iteration: 1622; Percent complete: 40.6%; Average loss: 3.2014\n","Iteration: 1623; Percent complete: 40.6%; Average loss: 3.4001\n","Iteration: 1624; Percent complete: 40.6%; Average loss: 3.4294\n","Iteration: 1625; Percent complete: 40.6%; Average loss: 3.5048\n","Iteration: 1626; Percent complete: 40.6%; Average loss: 3.1775\n","Iteration: 1627; Percent complete: 40.7%; Average loss: 3.1230\n","Iteration: 1628; Percent complete: 40.7%; Average loss: 2.9438\n","Iteration: 1629; Percent complete: 40.7%; Average loss: 3.1823\n","Iteration: 1630; Percent complete: 40.8%; Average loss: 3.0902\n","Iteration: 1631; Percent complete: 40.8%; Average loss: 3.2704\n","Iteration: 1632; Percent complete: 40.8%; Average loss: 3.2532\n","Iteration: 1633; Percent complete: 40.8%; Average loss: 3.3014\n","Iteration: 1634; Percent complete: 40.8%; Average loss: 3.1419\n","Iteration: 1635; Percent complete: 40.9%; Average loss: 3.0513\n","Iteration: 1636; Percent complete: 40.9%; Average loss: 3.4661\n","Iteration: 1637; Percent complete: 40.9%; Average loss: 3.2930\n","Iteration: 1638; Percent complete: 40.9%; Average loss: 3.1267\n","Iteration: 1639; Percent complete: 41.0%; Average loss: 3.5235\n","Iteration: 1640; Percent complete: 41.0%; Average loss: 3.1795\n","Iteration: 1641; Percent complete: 41.0%; Average loss: 3.2629\n","Iteration: 1642; Percent complete: 41.0%; Average loss: 3.0893\n","Iteration: 1643; Percent complete: 41.1%; Average loss: 3.1806\n","Iteration: 1644; Percent complete: 41.1%; Average loss: 3.2808\n","Iteration: 1645; Percent complete: 41.1%; Average loss: 3.2034\n","Iteration: 1646; Percent complete: 41.1%; Average loss: 3.2854\n","Iteration: 1647; Percent complete: 41.2%; Average loss: 3.4034\n","Iteration: 1648; Percent complete: 41.2%; Average loss: 3.1467\n","Iteration: 1649; Percent complete: 41.2%; Average loss: 3.4458\n","Iteration: 1650; Percent complete: 41.2%; Average loss: 3.2914\n","Iteration: 1651; Percent complete: 41.3%; Average loss: 2.8954\n","Iteration: 1652; Percent complete: 41.3%; Average loss: 3.2977\n","Iteration: 1653; Percent complete: 41.3%; Average loss: 3.2067\n","Iteration: 1654; Percent complete: 41.3%; Average loss: 3.6668\n","Iteration: 1655; Percent complete: 41.4%; Average loss: 3.3247\n","Iteration: 1656; Percent complete: 41.4%; Average loss: 3.0351\n","Iteration: 1657; Percent complete: 41.4%; Average loss: 3.0909\n","Iteration: 1658; Percent complete: 41.4%; Average loss: 3.2850\n","Iteration: 1659; Percent complete: 41.5%; Average loss: 3.3190\n","Iteration: 1660; Percent complete: 41.5%; Average loss: 2.9346\n","Iteration: 1661; Percent complete: 41.5%; Average loss: 3.3558\n","Iteration: 1662; Percent complete: 41.5%; Average loss: 3.3952\n","Iteration: 1663; Percent complete: 41.6%; Average loss: 3.3656\n","Iteration: 1664; Percent complete: 41.6%; Average loss: 3.2181\n","Iteration: 1665; Percent complete: 41.6%; Average loss: 3.1079\n","Iteration: 1666; Percent complete: 41.6%; Average loss: 3.1614\n","Iteration: 1667; Percent complete: 41.7%; Average loss: 3.1553\n","Iteration: 1668; Percent complete: 41.7%; Average loss: 3.1297\n","Iteration: 1669; Percent complete: 41.7%; Average loss: 3.4679\n","Iteration: 1670; Percent complete: 41.8%; Average loss: 3.1521\n","Iteration: 1671; Percent complete: 41.8%; Average loss: 3.1894\n","Iteration: 1672; Percent complete: 41.8%; Average loss: 3.2064\n","Iteration: 1673; Percent complete: 41.8%; Average loss: 3.3745\n","Iteration: 1674; Percent complete: 41.9%; Average loss: 3.3741\n","Iteration: 1675; Percent complete: 41.9%; Average loss: 3.3374\n","Iteration: 1676; Percent complete: 41.9%; Average loss: 3.1803\n","Iteration: 1677; Percent complete: 41.9%; Average loss: 3.0489\n","Iteration: 1678; Percent complete: 41.9%; Average loss: 3.3579\n","Iteration: 1679; Percent complete: 42.0%; Average loss: 3.0542\n","Iteration: 1680; Percent complete: 42.0%; Average loss: 3.3301\n","Iteration: 1681; Percent complete: 42.0%; Average loss: 3.2055\n","Iteration: 1682; Percent complete: 42.0%; Average loss: 3.1876\n","Iteration: 1683; Percent complete: 42.1%; Average loss: 3.2094\n","Iteration: 1684; Percent complete: 42.1%; Average loss: 3.0526\n","Iteration: 1685; Percent complete: 42.1%; Average loss: 3.2526\n","Iteration: 1686; Percent complete: 42.1%; Average loss: 3.1287\n","Iteration: 1687; Percent complete: 42.2%; Average loss: 3.0686\n","Iteration: 1688; Percent complete: 42.2%; Average loss: 3.2704\n","Iteration: 1689; Percent complete: 42.2%; Average loss: 3.5289\n","Iteration: 1690; Percent complete: 42.2%; Average loss: 3.2767\n","Iteration: 1691; Percent complete: 42.3%; Average loss: 3.1521\n","Iteration: 1692; Percent complete: 42.3%; Average loss: 2.9738\n","Iteration: 1693; Percent complete: 42.3%; Average loss: 3.0991\n","Iteration: 1694; Percent complete: 42.4%; Average loss: 3.3595\n","Iteration: 1695; Percent complete: 42.4%; Average loss: 3.1827\n","Iteration: 1696; Percent complete: 42.4%; Average loss: 3.6438\n","Iteration: 1697; Percent complete: 42.4%; Average loss: 3.5741\n","Iteration: 1698; Percent complete: 42.4%; Average loss: 3.1969\n","Iteration: 1699; Percent complete: 42.5%; Average loss: 3.4375\n","Iteration: 1700; Percent complete: 42.5%; Average loss: 3.2808\n","Iteration: 1701; Percent complete: 42.5%; Average loss: 3.3145\n","Iteration: 1702; Percent complete: 42.5%; Average loss: 3.3603\n","Iteration: 1703; Percent complete: 42.6%; Average loss: 3.1673\n","Iteration: 1704; Percent complete: 42.6%; Average loss: 3.0273\n","Iteration: 1705; Percent complete: 42.6%; Average loss: 3.1391\n","Iteration: 1706; Percent complete: 42.6%; Average loss: 3.1986\n","Iteration: 1707; Percent complete: 42.7%; Average loss: 3.2972\n","Iteration: 1708; Percent complete: 42.7%; Average loss: 3.3371\n","Iteration: 1709; Percent complete: 42.7%; Average loss: 2.9830\n","Iteration: 1710; Percent complete: 42.8%; Average loss: 3.1197\n","Iteration: 1711; Percent complete: 42.8%; Average loss: 3.4586\n","Iteration: 1712; Percent complete: 42.8%; Average loss: 3.4655\n","Iteration: 1713; Percent complete: 42.8%; Average loss: 3.1341\n","Iteration: 1714; Percent complete: 42.9%; Average loss: 3.3641\n","Iteration: 1715; Percent complete: 42.9%; Average loss: 3.1169\n","Iteration: 1716; Percent complete: 42.9%; Average loss: 3.0900\n","Iteration: 1717; Percent complete: 42.9%; Average loss: 3.1434\n","Iteration: 1718; Percent complete: 43.0%; Average loss: 3.3322\n","Iteration: 1719; Percent complete: 43.0%; Average loss: 3.2435\n","Iteration: 1720; Percent complete: 43.0%; Average loss: 3.3016\n","Iteration: 1721; Percent complete: 43.0%; Average loss: 3.3500\n","Iteration: 1722; Percent complete: 43.0%; Average loss: 3.2853\n","Iteration: 1723; Percent complete: 43.1%; Average loss: 3.2915\n","Iteration: 1724; Percent complete: 43.1%; Average loss: 3.1619\n","Iteration: 1725; Percent complete: 43.1%; Average loss: 3.0902\n","Iteration: 1726; Percent complete: 43.1%; Average loss: 3.3677\n","Iteration: 1727; Percent complete: 43.2%; Average loss: 3.1253\n","Iteration: 1728; Percent complete: 43.2%; Average loss: 3.4635\n","Iteration: 1729; Percent complete: 43.2%; Average loss: 3.0131\n","Iteration: 1730; Percent complete: 43.2%; Average loss: 3.2247\n","Iteration: 1731; Percent complete: 43.3%; Average loss: 3.2525\n","Iteration: 1732; Percent complete: 43.3%; Average loss: 3.2452\n","Iteration: 1733; Percent complete: 43.3%; Average loss: 3.3771\n","Iteration: 1734; Percent complete: 43.4%; Average loss: 3.3054\n","Iteration: 1735; Percent complete: 43.4%; Average loss: 3.2897\n","Iteration: 1736; Percent complete: 43.4%; Average loss: 3.4050\n","Iteration: 1737; Percent complete: 43.4%; Average loss: 3.3208\n","Iteration: 1738; Percent complete: 43.5%; Average loss: 3.3474\n","Iteration: 1739; Percent complete: 43.5%; Average loss: 3.1367\n","Iteration: 1740; Percent complete: 43.5%; Average loss: 2.9791\n","Iteration: 1741; Percent complete: 43.5%; Average loss: 3.1651\n","Iteration: 1742; Percent complete: 43.5%; Average loss: 3.1379\n","Iteration: 1743; Percent complete: 43.6%; Average loss: 3.1918\n","Iteration: 1744; Percent complete: 43.6%; Average loss: 3.2591\n","Iteration: 1745; Percent complete: 43.6%; Average loss: 3.3060\n","Iteration: 1746; Percent complete: 43.6%; Average loss: 3.1178\n","Iteration: 1747; Percent complete: 43.7%; Average loss: 3.1751\n","Iteration: 1748; Percent complete: 43.7%; Average loss: 3.1495\n","Iteration: 1749; Percent complete: 43.7%; Average loss: 3.2381\n","Iteration: 1750; Percent complete: 43.8%; Average loss: 3.4724\n","Iteration: 1751; Percent complete: 43.8%; Average loss: 3.1579\n","Iteration: 1752; Percent complete: 43.8%; Average loss: 3.1498\n","Iteration: 1753; Percent complete: 43.8%; Average loss: 3.1299\n","Iteration: 1754; Percent complete: 43.9%; Average loss: 3.1083\n","Iteration: 1755; Percent complete: 43.9%; Average loss: 3.2899\n","Iteration: 1756; Percent complete: 43.9%; Average loss: 2.9380\n","Iteration: 1757; Percent complete: 43.9%; Average loss: 3.5576\n","Iteration: 1758; Percent complete: 44.0%; Average loss: 3.3675\n","Iteration: 1759; Percent complete: 44.0%; Average loss: 3.2174\n","Iteration: 1760; Percent complete: 44.0%; Average loss: 3.3716\n","Iteration: 1761; Percent complete: 44.0%; Average loss: 3.0227\n","Iteration: 1762; Percent complete: 44.0%; Average loss: 3.2888\n","Iteration: 1763; Percent complete: 44.1%; Average loss: 3.2484\n","Iteration: 1764; Percent complete: 44.1%; Average loss: 3.2600\n","Iteration: 1765; Percent complete: 44.1%; Average loss: 3.3305\n","Iteration: 1766; Percent complete: 44.1%; Average loss: 3.4851\n","Iteration: 1767; Percent complete: 44.2%; Average loss: 3.3977\n","Iteration: 1768; Percent complete: 44.2%; Average loss: 3.2414\n","Iteration: 1769; Percent complete: 44.2%; Average loss: 3.1116\n","Iteration: 1770; Percent complete: 44.2%; Average loss: 3.2822\n","Iteration: 1771; Percent complete: 44.3%; Average loss: 3.4206\n","Iteration: 1772; Percent complete: 44.3%; Average loss: 3.0521\n","Iteration: 1773; Percent complete: 44.3%; Average loss: 3.2681\n","Iteration: 1774; Percent complete: 44.4%; Average loss: 3.1379\n","Iteration: 1775; Percent complete: 44.4%; Average loss: 3.4476\n","Iteration: 1776; Percent complete: 44.4%; Average loss: 3.2001\n","Iteration: 1777; Percent complete: 44.4%; Average loss: 2.9043\n","Iteration: 1778; Percent complete: 44.5%; Average loss: 3.2877\n","Iteration: 1779; Percent complete: 44.5%; Average loss: 3.2620\n","Iteration: 1780; Percent complete: 44.5%; Average loss: 3.4735\n","Iteration: 1781; Percent complete: 44.5%; Average loss: 3.3627\n","Iteration: 1782; Percent complete: 44.5%; Average loss: 3.2965\n","Iteration: 1783; Percent complete: 44.6%; Average loss: 3.2040\n","Iteration: 1784; Percent complete: 44.6%; Average loss: 3.1887\n","Iteration: 1785; Percent complete: 44.6%; Average loss: 2.9355\n","Iteration: 1786; Percent complete: 44.6%; Average loss: 3.1840\n","Iteration: 1787; Percent complete: 44.7%; Average loss: 3.4495\n","Iteration: 1788; Percent complete: 44.7%; Average loss: 3.0250\n","Iteration: 1789; Percent complete: 44.7%; Average loss: 2.9796\n","Iteration: 1790; Percent complete: 44.8%; Average loss: 2.9601\n","Iteration: 1791; Percent complete: 44.8%; Average loss: 3.1416\n","Iteration: 1792; Percent complete: 44.8%; Average loss: 3.2972\n","Iteration: 1793; Percent complete: 44.8%; Average loss: 3.3437\n","Iteration: 1794; Percent complete: 44.9%; Average loss: 3.1390\n","Iteration: 1795; Percent complete: 44.9%; Average loss: 3.2488\n","Iteration: 1796; Percent complete: 44.9%; Average loss: 2.9277\n","Iteration: 1797; Percent complete: 44.9%; Average loss: 3.0779\n","Iteration: 1798; Percent complete: 45.0%; Average loss: 3.1132\n","Iteration: 1799; Percent complete: 45.0%; Average loss: 3.2006\n","Iteration: 1800; Percent complete: 45.0%; Average loss: 3.4437\n","Iteration: 1801; Percent complete: 45.0%; Average loss: 3.1113\n","Iteration: 1802; Percent complete: 45.1%; Average loss: 3.1209\n","Iteration: 1803; Percent complete: 45.1%; Average loss: 3.3418\n","Iteration: 1804; Percent complete: 45.1%; Average loss: 3.0477\n","Iteration: 1805; Percent complete: 45.1%; Average loss: 3.3649\n","Iteration: 1806; Percent complete: 45.1%; Average loss: 3.3317\n","Iteration: 1807; Percent complete: 45.2%; Average loss: 3.0957\n","Iteration: 1808; Percent complete: 45.2%; Average loss: 3.2991\n","Iteration: 1809; Percent complete: 45.2%; Average loss: 3.2202\n","Iteration: 1810; Percent complete: 45.2%; Average loss: 3.1534\n","Iteration: 1811; Percent complete: 45.3%; Average loss: 3.3005\n","Iteration: 1812; Percent complete: 45.3%; Average loss: 3.4806\n","Iteration: 1813; Percent complete: 45.3%; Average loss: 3.1538\n","Iteration: 1814; Percent complete: 45.4%; Average loss: 3.0588\n","Iteration: 1815; Percent complete: 45.4%; Average loss: 3.1306\n","Iteration: 1816; Percent complete: 45.4%; Average loss: 3.1429\n","Iteration: 1817; Percent complete: 45.4%; Average loss: 3.2278\n","Iteration: 1818; Percent complete: 45.5%; Average loss: 3.1794\n","Iteration: 1819; Percent complete: 45.5%; Average loss: 3.2044\n","Iteration: 1820; Percent complete: 45.5%; Average loss: 3.5493\n","Iteration: 1821; Percent complete: 45.5%; Average loss: 3.1654\n","Iteration: 1822; Percent complete: 45.6%; Average loss: 3.2470\n","Iteration: 1823; Percent complete: 45.6%; Average loss: 3.2570\n","Iteration: 1824; Percent complete: 45.6%; Average loss: 3.3567\n","Iteration: 1825; Percent complete: 45.6%; Average loss: 3.2735\n","Iteration: 1826; Percent complete: 45.6%; Average loss: 3.3304\n","Iteration: 1827; Percent complete: 45.7%; Average loss: 3.2857\n","Iteration: 1828; Percent complete: 45.7%; Average loss: 3.1306\n","Iteration: 1829; Percent complete: 45.7%; Average loss: 3.4187\n","Iteration: 1830; Percent complete: 45.8%; Average loss: 3.0264\n","Iteration: 1831; Percent complete: 45.8%; Average loss: 3.2408\n","Iteration: 1832; Percent complete: 45.8%; Average loss: 3.4222\n","Iteration: 1833; Percent complete: 45.8%; Average loss: 3.0134\n","Iteration: 1834; Percent complete: 45.9%; Average loss: 3.3866\n","Iteration: 1835; Percent complete: 45.9%; Average loss: 3.3416\n","Iteration: 1836; Percent complete: 45.9%; Average loss: 3.3650\n","Iteration: 1837; Percent complete: 45.9%; Average loss: 3.0235\n","Iteration: 1838; Percent complete: 46.0%; Average loss: 3.2570\n","Iteration: 1839; Percent complete: 46.0%; Average loss: 3.0919\n","Iteration: 1840; Percent complete: 46.0%; Average loss: 3.1449\n","Iteration: 1841; Percent complete: 46.0%; Average loss: 3.2453\n","Iteration: 1842; Percent complete: 46.1%; Average loss: 3.4032\n","Iteration: 1843; Percent complete: 46.1%; Average loss: 3.0467\n","Iteration: 1844; Percent complete: 46.1%; Average loss: 3.3683\n","Iteration: 1845; Percent complete: 46.1%; Average loss: 3.0784\n","Iteration: 1846; Percent complete: 46.2%; Average loss: 3.1289\n","Iteration: 1847; Percent complete: 46.2%; Average loss: 3.2774\n","Iteration: 1848; Percent complete: 46.2%; Average loss: 3.2572\n","Iteration: 1849; Percent complete: 46.2%; Average loss: 3.1524\n","Iteration: 1850; Percent complete: 46.2%; Average loss: 3.2217\n","Iteration: 1851; Percent complete: 46.3%; Average loss: 3.2984\n","Iteration: 1852; Percent complete: 46.3%; Average loss: 2.9586\n","Iteration: 1853; Percent complete: 46.3%; Average loss: 2.9253\n","Iteration: 1854; Percent complete: 46.4%; Average loss: 3.0189\n","Iteration: 1855; Percent complete: 46.4%; Average loss: 3.4311\n","Iteration: 1856; Percent complete: 46.4%; Average loss: 2.9742\n","Iteration: 1857; Percent complete: 46.4%; Average loss: 3.2738\n","Iteration: 1858; Percent complete: 46.5%; Average loss: 3.3702\n","Iteration: 1859; Percent complete: 46.5%; Average loss: 3.2293\n","Iteration: 1860; Percent complete: 46.5%; Average loss: 3.2013\n","Iteration: 1861; Percent complete: 46.5%; Average loss: 3.2935\n","Iteration: 1862; Percent complete: 46.6%; Average loss: 3.2129\n","Iteration: 1863; Percent complete: 46.6%; Average loss: 2.9890\n","Iteration: 1864; Percent complete: 46.6%; Average loss: 3.2618\n","Iteration: 1865; Percent complete: 46.6%; Average loss: 3.1986\n","Iteration: 1866; Percent complete: 46.7%; Average loss: 3.0758\n","Iteration: 1867; Percent complete: 46.7%; Average loss: 3.1830\n","Iteration: 1868; Percent complete: 46.7%; Average loss: 3.0736\n","Iteration: 1869; Percent complete: 46.7%; Average loss: 3.3218\n","Iteration: 1870; Percent complete: 46.8%; Average loss: 3.2495\n","Iteration: 1871; Percent complete: 46.8%; Average loss: 3.3527\n","Iteration: 1872; Percent complete: 46.8%; Average loss: 3.1000\n","Iteration: 1873; Percent complete: 46.8%; Average loss: 3.3261\n","Iteration: 1874; Percent complete: 46.9%; Average loss: 3.3664\n","Iteration: 1875; Percent complete: 46.9%; Average loss: 3.2645\n","Iteration: 1876; Percent complete: 46.9%; Average loss: 3.3955\n","Iteration: 1877; Percent complete: 46.9%; Average loss: 3.3216\n","Iteration: 1878; Percent complete: 46.9%; Average loss: 3.2491\n","Iteration: 1879; Percent complete: 47.0%; Average loss: 3.1724\n","Iteration: 1880; Percent complete: 47.0%; Average loss: 3.0085\n","Iteration: 1881; Percent complete: 47.0%; Average loss: 3.2700\n","Iteration: 1882; Percent complete: 47.0%; Average loss: 3.5817\n","Iteration: 1883; Percent complete: 47.1%; Average loss: 3.2147\n","Iteration: 1884; Percent complete: 47.1%; Average loss: 3.2175\n","Iteration: 1885; Percent complete: 47.1%; Average loss: 2.9988\n","Iteration: 1886; Percent complete: 47.1%; Average loss: 3.1157\n","Iteration: 1887; Percent complete: 47.2%; Average loss: 3.2647\n","Iteration: 1888; Percent complete: 47.2%; Average loss: 3.3428\n","Iteration: 1889; Percent complete: 47.2%; Average loss: 3.1292\n","Iteration: 1890; Percent complete: 47.2%; Average loss: 3.0963\n","Iteration: 1891; Percent complete: 47.3%; Average loss: 3.1075\n","Iteration: 1892; Percent complete: 47.3%; Average loss: 3.2224\n","Iteration: 1893; Percent complete: 47.3%; Average loss: 3.2491\n","Iteration: 1894; Percent complete: 47.3%; Average loss: 3.1278\n","Iteration: 1895; Percent complete: 47.4%; Average loss: 3.0542\n","Iteration: 1896; Percent complete: 47.4%; Average loss: 3.0788\n","Iteration: 1897; Percent complete: 47.4%; Average loss: 3.3557\n","Iteration: 1898; Percent complete: 47.4%; Average loss: 3.1022\n","Iteration: 1899; Percent complete: 47.5%; Average loss: 3.0037\n","Iteration: 1900; Percent complete: 47.5%; Average loss: 3.4059\n","Iteration: 1901; Percent complete: 47.5%; Average loss: 3.2064\n","Iteration: 1902; Percent complete: 47.5%; Average loss: 3.1326\n","Iteration: 1903; Percent complete: 47.6%; Average loss: 2.9795\n","Iteration: 1904; Percent complete: 47.6%; Average loss: 3.1264\n","Iteration: 1905; Percent complete: 47.6%; Average loss: 3.1060\n","Iteration: 1906; Percent complete: 47.6%; Average loss: 3.2668\n","Iteration: 1907; Percent complete: 47.7%; Average loss: 3.2194\n","Iteration: 1908; Percent complete: 47.7%; Average loss: 3.1238\n","Iteration: 1909; Percent complete: 47.7%; Average loss: 3.0669\n","Iteration: 1910; Percent complete: 47.8%; Average loss: 2.9584\n","Iteration: 1911; Percent complete: 47.8%; Average loss: 3.3380\n","Iteration: 1912; Percent complete: 47.8%; Average loss: 3.0782\n","Iteration: 1913; Percent complete: 47.8%; Average loss: 3.2431\n","Iteration: 1914; Percent complete: 47.9%; Average loss: 3.0200\n","Iteration: 1915; Percent complete: 47.9%; Average loss: 3.1000\n","Iteration: 1916; Percent complete: 47.9%; Average loss: 3.2346\n","Iteration: 1917; Percent complete: 47.9%; Average loss: 3.2433\n","Iteration: 1918; Percent complete: 47.9%; Average loss: 2.9728\n","Iteration: 1919; Percent complete: 48.0%; Average loss: 3.3216\n","Iteration: 1920; Percent complete: 48.0%; Average loss: 2.9912\n","Iteration: 1921; Percent complete: 48.0%; Average loss: 2.9606\n","Iteration: 1922; Percent complete: 48.0%; Average loss: 2.8928\n","Iteration: 1923; Percent complete: 48.1%; Average loss: 3.4039\n","Iteration: 1924; Percent complete: 48.1%; Average loss: 3.1715\n","Iteration: 1925; Percent complete: 48.1%; Average loss: 3.4897\n","Iteration: 1926; Percent complete: 48.1%; Average loss: 3.3158\n","Iteration: 1927; Percent complete: 48.2%; Average loss: 3.0276\n","Iteration: 1928; Percent complete: 48.2%; Average loss: 3.2131\n","Iteration: 1929; Percent complete: 48.2%; Average loss: 3.1619\n","Iteration: 1930; Percent complete: 48.2%; Average loss: 3.0880\n","Iteration: 1931; Percent complete: 48.3%; Average loss: 3.1014\n","Iteration: 1932; Percent complete: 48.3%; Average loss: 3.2036\n","Iteration: 1933; Percent complete: 48.3%; Average loss: 3.0937\n","Iteration: 1934; Percent complete: 48.4%; Average loss: 3.2684\n","Iteration: 1935; Percent complete: 48.4%; Average loss: 3.2758\n","Iteration: 1936; Percent complete: 48.4%; Average loss: 3.0778\n","Iteration: 1937; Percent complete: 48.4%; Average loss: 2.9064\n","Iteration: 1938; Percent complete: 48.4%; Average loss: 2.9660\n","Iteration: 1939; Percent complete: 48.5%; Average loss: 3.1093\n","Iteration: 1940; Percent complete: 48.5%; Average loss: 3.0151\n","Iteration: 1941; Percent complete: 48.5%; Average loss: 2.9418\n","Iteration: 1942; Percent complete: 48.5%; Average loss: 3.3218\n","Iteration: 1943; Percent complete: 48.6%; Average loss: 3.0812\n","Iteration: 1944; Percent complete: 48.6%; Average loss: 3.3052\n","Iteration: 1945; Percent complete: 48.6%; Average loss: 3.0489\n","Iteration: 1946; Percent complete: 48.6%; Average loss: 3.0967\n","Iteration: 1947; Percent complete: 48.7%; Average loss: 3.2906\n","Iteration: 1948; Percent complete: 48.7%; Average loss: 3.0965\n","Iteration: 1949; Percent complete: 48.7%; Average loss: 3.2798\n","Iteration: 1950; Percent complete: 48.8%; Average loss: 3.1707\n","Iteration: 1951; Percent complete: 48.8%; Average loss: 2.9927\n","Iteration: 1952; Percent complete: 48.8%; Average loss: 3.1917\n","Iteration: 1953; Percent complete: 48.8%; Average loss: 3.2831\n","Iteration: 1954; Percent complete: 48.9%; Average loss: 3.0678\n","Iteration: 1955; Percent complete: 48.9%; Average loss: 3.0393\n","Iteration: 1956; Percent complete: 48.9%; Average loss: 3.2321\n","Iteration: 1957; Percent complete: 48.9%; Average loss: 3.0613\n","Iteration: 1958; Percent complete: 48.9%; Average loss: 3.0652\n","Iteration: 1959; Percent complete: 49.0%; Average loss: 3.0579\n","Iteration: 1960; Percent complete: 49.0%; Average loss: 3.2708\n","Iteration: 1961; Percent complete: 49.0%; Average loss: 3.0985\n","Iteration: 1962; Percent complete: 49.0%; Average loss: 3.5663\n","Iteration: 1963; Percent complete: 49.1%; Average loss: 3.1546\n","Iteration: 1964; Percent complete: 49.1%; Average loss: 3.1814\n","Iteration: 1965; Percent complete: 49.1%; Average loss: 3.0700\n","Iteration: 1966; Percent complete: 49.1%; Average loss: 3.3869\n","Iteration: 1967; Percent complete: 49.2%; Average loss: 3.1939\n","Iteration: 1968; Percent complete: 49.2%; Average loss: 3.3145\n","Iteration: 1969; Percent complete: 49.2%; Average loss: 3.1948\n","Iteration: 1970; Percent complete: 49.2%; Average loss: 3.0522\n","Iteration: 1971; Percent complete: 49.3%; Average loss: 3.2951\n","Iteration: 1972; Percent complete: 49.3%; Average loss: 3.0979\n","Iteration: 1973; Percent complete: 49.3%; Average loss: 2.9677\n","Iteration: 1974; Percent complete: 49.4%; Average loss: 3.2751\n","Iteration: 1975; Percent complete: 49.4%; Average loss: 3.1069\n","Iteration: 1976; Percent complete: 49.4%; Average loss: 3.2909\n","Iteration: 1977; Percent complete: 49.4%; Average loss: 3.1442\n","Iteration: 1978; Percent complete: 49.5%; Average loss: 3.0128\n","Iteration: 1979; Percent complete: 49.5%; Average loss: 3.1223\n","Iteration: 1980; Percent complete: 49.5%; Average loss: 3.0759\n","Iteration: 1981; Percent complete: 49.5%; Average loss: 3.0791\n","Iteration: 1982; Percent complete: 49.5%; Average loss: 3.2592\n","Iteration: 1983; Percent complete: 49.6%; Average loss: 2.9671\n","Iteration: 1984; Percent complete: 49.6%; Average loss: 3.2818\n","Iteration: 1985; Percent complete: 49.6%; Average loss: 3.1251\n","Iteration: 1986; Percent complete: 49.6%; Average loss: 3.0924\n","Iteration: 1987; Percent complete: 49.7%; Average loss: 2.9361\n","Iteration: 1988; Percent complete: 49.7%; Average loss: 3.1080\n","Iteration: 1989; Percent complete: 49.7%; Average loss: 3.2752\n","Iteration: 1990; Percent complete: 49.8%; Average loss: 3.0236\n","Iteration: 1991; Percent complete: 49.8%; Average loss: 3.2833\n","Iteration: 1992; Percent complete: 49.8%; Average loss: 3.0837\n","Iteration: 1993; Percent complete: 49.8%; Average loss: 3.0059\n","Iteration: 1994; Percent complete: 49.9%; Average loss: 3.2554\n","Iteration: 1995; Percent complete: 49.9%; Average loss: 3.2110\n","Iteration: 1996; Percent complete: 49.9%; Average loss: 3.0886\n","Iteration: 1997; Percent complete: 49.9%; Average loss: 3.5671\n","Iteration: 1998; Percent complete: 50.0%; Average loss: 3.1667\n","Iteration: 1999; Percent complete: 50.0%; Average loss: 2.8814\n","Iteration: 2000; Percent complete: 50.0%; Average loss: 3.3756\n","Iteration: 2001; Percent complete: 50.0%; Average loss: 3.2413\n","Iteration: 2002; Percent complete: 50.0%; Average loss: 3.1192\n","Iteration: 2003; Percent complete: 50.1%; Average loss: 3.3340\n","Iteration: 2004; Percent complete: 50.1%; Average loss: 2.8729\n","Iteration: 2005; Percent complete: 50.1%; Average loss: 3.0814\n","Iteration: 2006; Percent complete: 50.1%; Average loss: 3.4137\n","Iteration: 2007; Percent complete: 50.2%; Average loss: 3.1544\n","Iteration: 2008; Percent complete: 50.2%; Average loss: 2.9188\n","Iteration: 2009; Percent complete: 50.2%; Average loss: 3.2621\n","Iteration: 2010; Percent complete: 50.2%; Average loss: 2.7892\n","Iteration: 2011; Percent complete: 50.3%; Average loss: 3.4035\n","Iteration: 2012; Percent complete: 50.3%; Average loss: 3.1214\n","Iteration: 2013; Percent complete: 50.3%; Average loss: 3.1486\n","Iteration: 2014; Percent complete: 50.3%; Average loss: 3.0227\n","Iteration: 2015; Percent complete: 50.4%; Average loss: 3.1359\n","Iteration: 2016; Percent complete: 50.4%; Average loss: 3.0809\n","Iteration: 2017; Percent complete: 50.4%; Average loss: 3.1338\n","Iteration: 2018; Percent complete: 50.4%; Average loss: 3.1182\n","Iteration: 2019; Percent complete: 50.5%; Average loss: 3.0016\n","Iteration: 2020; Percent complete: 50.5%; Average loss: 3.1944\n","Iteration: 2021; Percent complete: 50.5%; Average loss: 3.2879\n","Iteration: 2022; Percent complete: 50.5%; Average loss: 3.5805\n","Iteration: 2023; Percent complete: 50.6%; Average loss: 3.1736\n","Iteration: 2024; Percent complete: 50.6%; Average loss: 3.3817\n","Iteration: 2025; Percent complete: 50.6%; Average loss: 3.1111\n","Iteration: 2026; Percent complete: 50.6%; Average loss: 3.2715\n","Iteration: 2027; Percent complete: 50.7%; Average loss: 3.2220\n","Iteration: 2028; Percent complete: 50.7%; Average loss: 3.2466\n","Iteration: 2029; Percent complete: 50.7%; Average loss: 2.9971\n","Iteration: 2030; Percent complete: 50.7%; Average loss: 3.4336\n","Iteration: 2031; Percent complete: 50.8%; Average loss: 2.9908\n","Iteration: 2032; Percent complete: 50.8%; Average loss: 3.0952\n","Iteration: 2033; Percent complete: 50.8%; Average loss: 3.2148\n","Iteration: 2034; Percent complete: 50.8%; Average loss: 3.0894\n","Iteration: 2035; Percent complete: 50.9%; Average loss: 3.0541\n","Iteration: 2036; Percent complete: 50.9%; Average loss: 3.3210\n","Iteration: 2037; Percent complete: 50.9%; Average loss: 3.1378\n","Iteration: 2038; Percent complete: 50.9%; Average loss: 3.1511\n","Iteration: 2039; Percent complete: 51.0%; Average loss: 3.0163\n","Iteration: 2040; Percent complete: 51.0%; Average loss: 3.0495\n","Iteration: 2041; Percent complete: 51.0%; Average loss: 3.0996\n","Iteration: 2042; Percent complete: 51.0%; Average loss: 3.1341\n","Iteration: 2043; Percent complete: 51.1%; Average loss: 3.2819\n","Iteration: 2044; Percent complete: 51.1%; Average loss: 3.0815\n","Iteration: 2045; Percent complete: 51.1%; Average loss: 3.3877\n","Iteration: 2046; Percent complete: 51.1%; Average loss: 3.2184\n","Iteration: 2047; Percent complete: 51.2%; Average loss: 3.3320\n","Iteration: 2048; Percent complete: 51.2%; Average loss: 3.4282\n","Iteration: 2049; Percent complete: 51.2%; Average loss: 3.1353\n","Iteration: 2050; Percent complete: 51.2%; Average loss: 3.0299\n","Iteration: 2051; Percent complete: 51.3%; Average loss: 3.0625\n","Iteration: 2052; Percent complete: 51.3%; Average loss: 3.3839\n","Iteration: 2053; Percent complete: 51.3%; Average loss: 3.1590\n","Iteration: 2054; Percent complete: 51.3%; Average loss: 3.2414\n","Iteration: 2055; Percent complete: 51.4%; Average loss: 3.3393\n","Iteration: 2056; Percent complete: 51.4%; Average loss: 3.1856\n","Iteration: 2057; Percent complete: 51.4%; Average loss: 3.1232\n","Iteration: 2058; Percent complete: 51.4%; Average loss: 3.4446\n","Iteration: 2059; Percent complete: 51.5%; Average loss: 2.8392\n","Iteration: 2060; Percent complete: 51.5%; Average loss: 3.2996\n","Iteration: 2061; Percent complete: 51.5%; Average loss: 3.4201\n","Iteration: 2062; Percent complete: 51.5%; Average loss: 3.1072\n","Iteration: 2063; Percent complete: 51.6%; Average loss: 3.4718\n","Iteration: 2064; Percent complete: 51.6%; Average loss: 3.0702\n","Iteration: 2065; Percent complete: 51.6%; Average loss: 3.2865\n","Iteration: 2066; Percent complete: 51.6%; Average loss: 3.1080\n","Iteration: 2067; Percent complete: 51.7%; Average loss: 3.3000\n","Iteration: 2068; Percent complete: 51.7%; Average loss: 3.3896\n","Iteration: 2069; Percent complete: 51.7%; Average loss: 3.0559\n","Iteration: 2070; Percent complete: 51.7%; Average loss: 2.9412\n","Iteration: 2071; Percent complete: 51.8%; Average loss: 3.1513\n","Iteration: 2072; Percent complete: 51.8%; Average loss: 3.3264\n","Iteration: 2073; Percent complete: 51.8%; Average loss: 2.9242\n","Iteration: 2074; Percent complete: 51.8%; Average loss: 3.2279\n","Iteration: 2075; Percent complete: 51.9%; Average loss: 3.1801\n","Iteration: 2076; Percent complete: 51.9%; Average loss: 2.8873\n","Iteration: 2077; Percent complete: 51.9%; Average loss: 2.9457\n","Iteration: 2078; Percent complete: 51.9%; Average loss: 3.4400\n","Iteration: 2079; Percent complete: 52.0%; Average loss: 3.1141\n","Iteration: 2080; Percent complete: 52.0%; Average loss: 2.9740\n","Iteration: 2081; Percent complete: 52.0%; Average loss: 3.1987\n","Iteration: 2082; Percent complete: 52.0%; Average loss: 3.0449\n","Iteration: 2083; Percent complete: 52.1%; Average loss: 3.3206\n","Iteration: 2084; Percent complete: 52.1%; Average loss: 3.3289\n","Iteration: 2085; Percent complete: 52.1%; Average loss: 3.0655\n","Iteration: 2086; Percent complete: 52.1%; Average loss: 3.1610\n","Iteration: 2087; Percent complete: 52.2%; Average loss: 2.7785\n","Iteration: 2088; Percent complete: 52.2%; Average loss: 3.2158\n","Iteration: 2089; Percent complete: 52.2%; Average loss: 3.0953\n","Iteration: 2090; Percent complete: 52.2%; Average loss: 3.0559\n","Iteration: 2091; Percent complete: 52.3%; Average loss: 2.8939\n","Iteration: 2092; Percent complete: 52.3%; Average loss: 3.2090\n","Iteration: 2093; Percent complete: 52.3%; Average loss: 3.1061\n","Iteration: 2094; Percent complete: 52.3%; Average loss: 3.4012\n","Iteration: 2095; Percent complete: 52.4%; Average loss: 3.1046\n","Iteration: 2096; Percent complete: 52.4%; Average loss: 3.2618\n","Iteration: 2097; Percent complete: 52.4%; Average loss: 3.0316\n","Iteration: 2098; Percent complete: 52.4%; Average loss: 3.0979\n","Iteration: 2099; Percent complete: 52.5%; Average loss: 3.2674\n","Iteration: 2100; Percent complete: 52.5%; Average loss: 3.2148\n","Iteration: 2101; Percent complete: 52.5%; Average loss: 2.8420\n","Iteration: 2102; Percent complete: 52.5%; Average loss: 2.9489\n","Iteration: 2103; Percent complete: 52.6%; Average loss: 3.0468\n","Iteration: 2104; Percent complete: 52.6%; Average loss: 3.2028\n","Iteration: 2105; Percent complete: 52.6%; Average loss: 2.9406\n","Iteration: 2106; Percent complete: 52.6%; Average loss: 3.0176\n","Iteration: 2107; Percent complete: 52.7%; Average loss: 3.0751\n","Iteration: 2108; Percent complete: 52.7%; Average loss: 3.1425\n","Iteration: 2109; Percent complete: 52.7%; Average loss: 3.2325\n","Iteration: 2110; Percent complete: 52.8%; Average loss: 3.3942\n","Iteration: 2111; Percent complete: 52.8%; Average loss: 3.2402\n","Iteration: 2112; Percent complete: 52.8%; Average loss: 3.2656\n","Iteration: 2113; Percent complete: 52.8%; Average loss: 3.1256\n","Iteration: 2114; Percent complete: 52.8%; Average loss: 3.1271\n","Iteration: 2115; Percent complete: 52.9%; Average loss: 3.1704\n","Iteration: 2116; Percent complete: 52.9%; Average loss: 3.3466\n","Iteration: 2117; Percent complete: 52.9%; Average loss: 3.0055\n","Iteration: 2118; Percent complete: 52.9%; Average loss: 3.3525\n","Iteration: 2119; Percent complete: 53.0%; Average loss: 3.1248\n","Iteration: 2120; Percent complete: 53.0%; Average loss: 3.0215\n","Iteration: 2121; Percent complete: 53.0%; Average loss: 2.9810\n","Iteration: 2122; Percent complete: 53.0%; Average loss: 3.0790\n","Iteration: 2123; Percent complete: 53.1%; Average loss: 3.3329\n","Iteration: 2124; Percent complete: 53.1%; Average loss: 3.2117\n","Iteration: 2125; Percent complete: 53.1%; Average loss: 2.9607\n","Iteration: 2126; Percent complete: 53.1%; Average loss: 3.0818\n","Iteration: 2127; Percent complete: 53.2%; Average loss: 3.1673\n","Iteration: 2128; Percent complete: 53.2%; Average loss: 3.0309\n","Iteration: 2129; Percent complete: 53.2%; Average loss: 3.2967\n","Iteration: 2130; Percent complete: 53.2%; Average loss: 3.0962\n","Iteration: 2131; Percent complete: 53.3%; Average loss: 2.9891\n","Iteration: 2132; Percent complete: 53.3%; Average loss: 3.0436\n","Iteration: 2133; Percent complete: 53.3%; Average loss: 3.3463\n","Iteration: 2134; Percent complete: 53.3%; Average loss: 3.2877\n","Iteration: 2135; Percent complete: 53.4%; Average loss: 2.8871\n","Iteration: 2136; Percent complete: 53.4%; Average loss: 3.1975\n","Iteration: 2137; Percent complete: 53.4%; Average loss: 3.0007\n","Iteration: 2138; Percent complete: 53.4%; Average loss: 2.9321\n","Iteration: 2139; Percent complete: 53.5%; Average loss: 2.8843\n","Iteration: 2140; Percent complete: 53.5%; Average loss: 3.1538\n","Iteration: 2141; Percent complete: 53.5%; Average loss: 3.3909\n","Iteration: 2142; Percent complete: 53.5%; Average loss: 3.1605\n","Iteration: 2143; Percent complete: 53.6%; Average loss: 3.1700\n","Iteration: 2144; Percent complete: 53.6%; Average loss: 2.9007\n","Iteration: 2145; Percent complete: 53.6%; Average loss: 3.1163\n","Iteration: 2146; Percent complete: 53.6%; Average loss: 3.1067\n","Iteration: 2147; Percent complete: 53.7%; Average loss: 3.2822\n","Iteration: 2148; Percent complete: 53.7%; Average loss: 3.0893\n","Iteration: 2149; Percent complete: 53.7%; Average loss: 3.0994\n","Iteration: 2150; Percent complete: 53.8%; Average loss: 2.8877\n","Iteration: 2151; Percent complete: 53.8%; Average loss: 3.0810\n","Iteration: 2152; Percent complete: 53.8%; Average loss: 3.0532\n","Iteration: 2153; Percent complete: 53.8%; Average loss: 3.3358\n","Iteration: 2154; Percent complete: 53.8%; Average loss: 3.0382\n","Iteration: 2155; Percent complete: 53.9%; Average loss: 3.1762\n","Iteration: 2156; Percent complete: 53.9%; Average loss: 3.4274\n","Iteration: 2157; Percent complete: 53.9%; Average loss: 3.2920\n","Iteration: 2158; Percent complete: 53.9%; Average loss: 3.0957\n","Iteration: 2159; Percent complete: 54.0%; Average loss: 3.0380\n","Iteration: 2160; Percent complete: 54.0%; Average loss: 2.8601\n","Iteration: 2161; Percent complete: 54.0%; Average loss: 3.1252\n","Iteration: 2162; Percent complete: 54.0%; Average loss: 2.9785\n","Iteration: 2163; Percent complete: 54.1%; Average loss: 2.9222\n","Iteration: 2164; Percent complete: 54.1%; Average loss: 3.1478\n","Iteration: 2165; Percent complete: 54.1%; Average loss: 3.3152\n","Iteration: 2166; Percent complete: 54.1%; Average loss: 3.0644\n","Iteration: 2167; Percent complete: 54.2%; Average loss: 3.2608\n","Iteration: 2168; Percent complete: 54.2%; Average loss: 2.9579\n","Iteration: 2169; Percent complete: 54.2%; Average loss: 3.1471\n","Iteration: 2170; Percent complete: 54.2%; Average loss: 3.1531\n","Iteration: 2171; Percent complete: 54.3%; Average loss: 3.0220\n","Iteration: 2172; Percent complete: 54.3%; Average loss: 3.1407\n","Iteration: 2173; Percent complete: 54.3%; Average loss: 2.9452\n","Iteration: 2174; Percent complete: 54.4%; Average loss: 2.9862\n","Iteration: 2175; Percent complete: 54.4%; Average loss: 3.2810\n","Iteration: 2176; Percent complete: 54.4%; Average loss: 3.0422\n","Iteration: 2177; Percent complete: 54.4%; Average loss: 3.2073\n","Iteration: 2178; Percent complete: 54.4%; Average loss: 3.2095\n","Iteration: 2179; Percent complete: 54.5%; Average loss: 3.1263\n","Iteration: 2180; Percent complete: 54.5%; Average loss: 3.2929\n","Iteration: 2181; Percent complete: 54.5%; Average loss: 3.0251\n","Iteration: 2182; Percent complete: 54.5%; Average loss: 2.8800\n","Iteration: 2183; Percent complete: 54.6%; Average loss: 2.9930\n","Iteration: 2184; Percent complete: 54.6%; Average loss: 3.1235\n","Iteration: 2185; Percent complete: 54.6%; Average loss: 3.1041\n","Iteration: 2186; Percent complete: 54.6%; Average loss: 2.9607\n","Iteration: 2187; Percent complete: 54.7%; Average loss: 3.1064\n","Iteration: 2188; Percent complete: 54.7%; Average loss: 3.1141\n","Iteration: 2189; Percent complete: 54.7%; Average loss: 2.9779\n","Iteration: 2190; Percent complete: 54.8%; Average loss: 3.0304\n","Iteration: 2191; Percent complete: 54.8%; Average loss: 3.1125\n","Iteration: 2192; Percent complete: 54.8%; Average loss: 3.0672\n","Iteration: 2193; Percent complete: 54.8%; Average loss: 3.0780\n","Iteration: 2194; Percent complete: 54.9%; Average loss: 3.0565\n","Iteration: 2195; Percent complete: 54.9%; Average loss: 3.2915\n","Iteration: 2196; Percent complete: 54.9%; Average loss: 3.2661\n","Iteration: 2197; Percent complete: 54.9%; Average loss: 2.9820\n","Iteration: 2198; Percent complete: 54.9%; Average loss: 2.8698\n","Iteration: 2199; Percent complete: 55.0%; Average loss: 3.2302\n","Iteration: 2200; Percent complete: 55.0%; Average loss: 3.3934\n","Iteration: 2201; Percent complete: 55.0%; Average loss: 3.0246\n","Iteration: 2202; Percent complete: 55.0%; Average loss: 3.1221\n","Iteration: 2203; Percent complete: 55.1%; Average loss: 3.2326\n","Iteration: 2204; Percent complete: 55.1%; Average loss: 3.1499\n","Iteration: 2205; Percent complete: 55.1%; Average loss: 2.9911\n","Iteration: 2206; Percent complete: 55.1%; Average loss: 3.3007\n","Iteration: 2207; Percent complete: 55.2%; Average loss: 3.0243\n","Iteration: 2208; Percent complete: 55.2%; Average loss: 3.3412\n","Iteration: 2209; Percent complete: 55.2%; Average loss: 3.2361\n","Iteration: 2210; Percent complete: 55.2%; Average loss: 3.1265\n","Iteration: 2211; Percent complete: 55.3%; Average loss: 3.2062\n","Iteration: 2212; Percent complete: 55.3%; Average loss: 3.3316\n","Iteration: 2213; Percent complete: 55.3%; Average loss: 3.1721\n","Iteration: 2214; Percent complete: 55.4%; Average loss: 3.1284\n","Iteration: 2215; Percent complete: 55.4%; Average loss: 3.1229\n","Iteration: 2216; Percent complete: 55.4%; Average loss: 3.2188\n","Iteration: 2217; Percent complete: 55.4%; Average loss: 3.1512\n","Iteration: 2218; Percent complete: 55.5%; Average loss: 3.1132\n","Iteration: 2219; Percent complete: 55.5%; Average loss: 3.0570\n","Iteration: 2220; Percent complete: 55.5%; Average loss: 3.1244\n","Iteration: 2221; Percent complete: 55.5%; Average loss: 3.1061\n","Iteration: 2222; Percent complete: 55.5%; Average loss: 2.8604\n","Iteration: 2223; Percent complete: 55.6%; Average loss: 3.3493\n","Iteration: 2224; Percent complete: 55.6%; Average loss: 3.0923\n","Iteration: 2225; Percent complete: 55.6%; Average loss: 2.9821\n","Iteration: 2226; Percent complete: 55.6%; Average loss: 3.0740\n","Iteration: 2227; Percent complete: 55.7%; Average loss: 3.1944\n","Iteration: 2228; Percent complete: 55.7%; Average loss: 3.2821\n","Iteration: 2229; Percent complete: 55.7%; Average loss: 2.9750\n","Iteration: 2230; Percent complete: 55.8%; Average loss: 3.2225\n","Iteration: 2231; Percent complete: 55.8%; Average loss: 3.1366\n","Iteration: 2232; Percent complete: 55.8%; Average loss: 2.9537\n","Iteration: 2233; Percent complete: 55.8%; Average loss: 2.8608\n","Iteration: 2234; Percent complete: 55.9%; Average loss: 3.0939\n","Iteration: 2235; Percent complete: 55.9%; Average loss: 3.0246\n","Iteration: 2236; Percent complete: 55.9%; Average loss: 2.8550\n","Iteration: 2237; Percent complete: 55.9%; Average loss: 2.9696\n","Iteration: 2238; Percent complete: 56.0%; Average loss: 3.1076\n","Iteration: 2239; Percent complete: 56.0%; Average loss: 2.8301\n","Iteration: 2240; Percent complete: 56.0%; Average loss: 2.9904\n","Iteration: 2241; Percent complete: 56.0%; Average loss: 2.9941\n","Iteration: 2242; Percent complete: 56.0%; Average loss: 3.2376\n","Iteration: 2243; Percent complete: 56.1%; Average loss: 2.9912\n","Iteration: 2244; Percent complete: 56.1%; Average loss: 3.3065\n","Iteration: 2245; Percent complete: 56.1%; Average loss: 3.1961\n","Iteration: 2246; Percent complete: 56.1%; Average loss: 2.8777\n","Iteration: 2247; Percent complete: 56.2%; Average loss: 3.3806\n","Iteration: 2248; Percent complete: 56.2%; Average loss: 2.6990\n","Iteration: 2249; Percent complete: 56.2%; Average loss: 3.0649\n","Iteration: 2250; Percent complete: 56.2%; Average loss: 3.2014\n","Iteration: 2251; Percent complete: 56.3%; Average loss: 3.2315\n","Iteration: 2252; Percent complete: 56.3%; Average loss: 3.0883\n","Iteration: 2253; Percent complete: 56.3%; Average loss: 3.2401\n","Iteration: 2254; Percent complete: 56.4%; Average loss: 3.3471\n","Iteration: 2255; Percent complete: 56.4%; Average loss: 3.2219\n","Iteration: 2256; Percent complete: 56.4%; Average loss: 2.9324\n","Iteration: 2257; Percent complete: 56.4%; Average loss: 3.1160\n","Iteration: 2258; Percent complete: 56.5%; Average loss: 3.0617\n","Iteration: 2259; Percent complete: 56.5%; Average loss: 3.0557\n","Iteration: 2260; Percent complete: 56.5%; Average loss: 3.1053\n","Iteration: 2261; Percent complete: 56.5%; Average loss: 3.1712\n","Iteration: 2262; Percent complete: 56.5%; Average loss: 3.3133\n","Iteration: 2263; Percent complete: 56.6%; Average loss: 3.1311\n","Iteration: 2264; Percent complete: 56.6%; Average loss: 3.1323\n","Iteration: 2265; Percent complete: 56.6%; Average loss: 3.1489\n","Iteration: 2266; Percent complete: 56.6%; Average loss: 3.1312\n","Iteration: 2267; Percent complete: 56.7%; Average loss: 2.6940\n","Iteration: 2268; Percent complete: 56.7%; Average loss: 2.9859\n","Iteration: 2269; Percent complete: 56.7%; Average loss: 2.9451\n","Iteration: 2270; Percent complete: 56.8%; Average loss: 3.1158\n","Iteration: 2271; Percent complete: 56.8%; Average loss: 3.1570\n","Iteration: 2272; Percent complete: 56.8%; Average loss: 3.0610\n","Iteration: 2273; Percent complete: 56.8%; Average loss: 3.1699\n","Iteration: 2274; Percent complete: 56.9%; Average loss: 3.0287\n","Iteration: 2275; Percent complete: 56.9%; Average loss: 3.1515\n","Iteration: 2276; Percent complete: 56.9%; Average loss: 2.9446\n","Iteration: 2277; Percent complete: 56.9%; Average loss: 3.2736\n","Iteration: 2278; Percent complete: 57.0%; Average loss: 3.3687\n","Iteration: 2279; Percent complete: 57.0%; Average loss: 3.0281\n","Iteration: 2280; Percent complete: 57.0%; Average loss: 3.0229\n","Iteration: 2281; Percent complete: 57.0%; Average loss: 2.8469\n","Iteration: 2282; Percent complete: 57.0%; Average loss: 3.0679\n","Iteration: 2283; Percent complete: 57.1%; Average loss: 2.8747\n","Iteration: 2284; Percent complete: 57.1%; Average loss: 3.1818\n","Iteration: 2285; Percent complete: 57.1%; Average loss: 2.9678\n","Iteration: 2286; Percent complete: 57.1%; Average loss: 3.3656\n","Iteration: 2287; Percent complete: 57.2%; Average loss: 3.2065\n","Iteration: 2288; Percent complete: 57.2%; Average loss: 2.9174\n","Iteration: 2289; Percent complete: 57.2%; Average loss: 3.1164\n","Iteration: 2290; Percent complete: 57.2%; Average loss: 3.1029\n","Iteration: 2291; Percent complete: 57.3%; Average loss: 3.0888\n","Iteration: 2292; Percent complete: 57.3%; Average loss: 3.1147\n","Iteration: 2293; Percent complete: 57.3%; Average loss: 3.1029\n","Iteration: 2294; Percent complete: 57.4%; Average loss: 3.1665\n","Iteration: 2295; Percent complete: 57.4%; Average loss: 3.0924\n","Iteration: 2296; Percent complete: 57.4%; Average loss: 3.2747\n","Iteration: 2297; Percent complete: 57.4%; Average loss: 3.0867\n","Iteration: 2298; Percent complete: 57.5%; Average loss: 2.9506\n","Iteration: 2299; Percent complete: 57.5%; Average loss: 2.8252\n","Iteration: 2300; Percent complete: 57.5%; Average loss: 3.0415\n","Iteration: 2301; Percent complete: 57.5%; Average loss: 3.2345\n","Iteration: 2302; Percent complete: 57.6%; Average loss: 3.0805\n","Iteration: 2303; Percent complete: 57.6%; Average loss: 3.3843\n","Iteration: 2304; Percent complete: 57.6%; Average loss: 3.2762\n","Iteration: 2305; Percent complete: 57.6%; Average loss: 3.1566\n","Iteration: 2306; Percent complete: 57.6%; Average loss: 3.0942\n","Iteration: 2307; Percent complete: 57.7%; Average loss: 3.1230\n","Iteration: 2308; Percent complete: 57.7%; Average loss: 3.1185\n","Iteration: 2309; Percent complete: 57.7%; Average loss: 2.9689\n","Iteration: 2310; Percent complete: 57.8%; Average loss: 3.2298\n","Iteration: 2311; Percent complete: 57.8%; Average loss: 2.8643\n","Iteration: 2312; Percent complete: 57.8%; Average loss: 2.9598\n","Iteration: 2313; Percent complete: 57.8%; Average loss: 3.0089\n","Iteration: 2314; Percent complete: 57.9%; Average loss: 2.9031\n","Iteration: 2315; Percent complete: 57.9%; Average loss: 3.0456\n","Iteration: 2316; Percent complete: 57.9%; Average loss: 3.1096\n","Iteration: 2317; Percent complete: 57.9%; Average loss: 3.1445\n","Iteration: 2318; Percent complete: 58.0%; Average loss: 2.9141\n","Iteration: 2319; Percent complete: 58.0%; Average loss: 3.0265\n","Iteration: 2320; Percent complete: 58.0%; Average loss: 2.8722\n","Iteration: 2321; Percent complete: 58.0%; Average loss: 3.0921\n","Iteration: 2322; Percent complete: 58.1%; Average loss: 3.1889\n","Iteration: 2323; Percent complete: 58.1%; Average loss: 2.8809\n","Iteration: 2324; Percent complete: 58.1%; Average loss: 2.9477\n","Iteration: 2325; Percent complete: 58.1%; Average loss: 2.9464\n","Iteration: 2326; Percent complete: 58.1%; Average loss: 3.2199\n","Iteration: 2327; Percent complete: 58.2%; Average loss: 3.2816\n","Iteration: 2328; Percent complete: 58.2%; Average loss: 2.8962\n","Iteration: 2329; Percent complete: 58.2%; Average loss: 3.2167\n","Iteration: 2330; Percent complete: 58.2%; Average loss: 3.2034\n","Iteration: 2331; Percent complete: 58.3%; Average loss: 3.3457\n","Iteration: 2332; Percent complete: 58.3%; Average loss: 3.1063\n","Iteration: 2333; Percent complete: 58.3%; Average loss: 3.1814\n","Iteration: 2334; Percent complete: 58.4%; Average loss: 2.9439\n","Iteration: 2335; Percent complete: 58.4%; Average loss: 3.0218\n","Iteration: 2336; Percent complete: 58.4%; Average loss: 2.8461\n","Iteration: 2337; Percent complete: 58.4%; Average loss: 3.1260\n","Iteration: 2338; Percent complete: 58.5%; Average loss: 3.1207\n","Iteration: 2339; Percent complete: 58.5%; Average loss: 3.1813\n","Iteration: 2340; Percent complete: 58.5%; Average loss: 3.1042\n","Iteration: 2341; Percent complete: 58.5%; Average loss: 2.8064\n","Iteration: 2342; Percent complete: 58.6%; Average loss: 3.1308\n","Iteration: 2343; Percent complete: 58.6%; Average loss: 2.9909\n","Iteration: 2344; Percent complete: 58.6%; Average loss: 2.9932\n","Iteration: 2345; Percent complete: 58.6%; Average loss: 3.0909\n","Iteration: 2346; Percent complete: 58.7%; Average loss: 3.0429\n","Iteration: 2347; Percent complete: 58.7%; Average loss: 2.8331\n","Iteration: 2348; Percent complete: 58.7%; Average loss: 2.8992\n","Iteration: 2349; Percent complete: 58.7%; Average loss: 3.0746\n","Iteration: 2350; Percent complete: 58.8%; Average loss: 3.0448\n","Iteration: 2351; Percent complete: 58.8%; Average loss: 3.3180\n","Iteration: 2352; Percent complete: 58.8%; Average loss: 3.0667\n","Iteration: 2353; Percent complete: 58.8%; Average loss: 2.8933\n","Iteration: 2354; Percent complete: 58.9%; Average loss: 3.0408\n","Iteration: 2355; Percent complete: 58.9%; Average loss: 2.8965\n","Iteration: 2356; Percent complete: 58.9%; Average loss: 3.0755\n","Iteration: 2357; Percent complete: 58.9%; Average loss: 2.8976\n","Iteration: 2358; Percent complete: 59.0%; Average loss: 3.1118\n","Iteration: 2359; Percent complete: 59.0%; Average loss: 2.9531\n","Iteration: 2360; Percent complete: 59.0%; Average loss: 3.1608\n","Iteration: 2361; Percent complete: 59.0%; Average loss: 2.9047\n","Iteration: 2362; Percent complete: 59.1%; Average loss: 3.0628\n","Iteration: 2363; Percent complete: 59.1%; Average loss: 2.8531\n","Iteration: 2364; Percent complete: 59.1%; Average loss: 3.3225\n","Iteration: 2365; Percent complete: 59.1%; Average loss: 2.9675\n","Iteration: 2366; Percent complete: 59.2%; Average loss: 3.2318\n","Iteration: 2367; Percent complete: 59.2%; Average loss: 2.9086\n","Iteration: 2368; Percent complete: 59.2%; Average loss: 3.1289\n","Iteration: 2369; Percent complete: 59.2%; Average loss: 3.1262\n","Iteration: 2370; Percent complete: 59.2%; Average loss: 2.9872\n","Iteration: 2371; Percent complete: 59.3%; Average loss: 3.0541\n","Iteration: 2372; Percent complete: 59.3%; Average loss: 2.9846\n","Iteration: 2373; Percent complete: 59.3%; Average loss: 3.0902\n","Iteration: 2374; Percent complete: 59.4%; Average loss: 3.1456\n","Iteration: 2375; Percent complete: 59.4%; Average loss: 2.9349\n","Iteration: 2376; Percent complete: 59.4%; Average loss: 2.9518\n","Iteration: 2377; Percent complete: 59.4%; Average loss: 2.9715\n","Iteration: 2378; Percent complete: 59.5%; Average loss: 3.1441\n","Iteration: 2379; Percent complete: 59.5%; Average loss: 3.4142\n","Iteration: 2380; Percent complete: 59.5%; Average loss: 2.8673\n","Iteration: 2381; Percent complete: 59.5%; Average loss: 3.1046\n","Iteration: 2382; Percent complete: 59.6%; Average loss: 3.0956\n","Iteration: 2383; Percent complete: 59.6%; Average loss: 3.0006\n","Iteration: 2384; Percent complete: 59.6%; Average loss: 3.1832\n","Iteration: 2385; Percent complete: 59.6%; Average loss: 3.1179\n","Iteration: 2386; Percent complete: 59.7%; Average loss: 3.0724\n","Iteration: 2387; Percent complete: 59.7%; Average loss: 3.0420\n","Iteration: 2388; Percent complete: 59.7%; Average loss: 2.9452\n","Iteration: 2389; Percent complete: 59.7%; Average loss: 3.0826\n","Iteration: 2390; Percent complete: 59.8%; Average loss: 2.8930\n","Iteration: 2391; Percent complete: 59.8%; Average loss: 3.2715\n","Iteration: 2392; Percent complete: 59.8%; Average loss: 3.0019\n","Iteration: 2393; Percent complete: 59.8%; Average loss: 3.1712\n","Iteration: 2394; Percent complete: 59.9%; Average loss: 2.9568\n","Iteration: 2395; Percent complete: 59.9%; Average loss: 3.1445\n","Iteration: 2396; Percent complete: 59.9%; Average loss: 3.1061\n","Iteration: 2397; Percent complete: 59.9%; Average loss: 2.8932\n","Iteration: 2398; Percent complete: 60.0%; Average loss: 3.0422\n","Iteration: 2399; Percent complete: 60.0%; Average loss: 2.8691\n","Iteration: 2400; Percent complete: 60.0%; Average loss: 2.9981\n","Iteration: 2401; Percent complete: 60.0%; Average loss: 2.8618\n","Iteration: 2402; Percent complete: 60.1%; Average loss: 2.8610\n","Iteration: 2403; Percent complete: 60.1%; Average loss: 3.2111\n","Iteration: 2404; Percent complete: 60.1%; Average loss: 2.8664\n","Iteration: 2405; Percent complete: 60.1%; Average loss: 2.8749\n","Iteration: 2406; Percent complete: 60.2%; Average loss: 2.7888\n","Iteration: 2407; Percent complete: 60.2%; Average loss: 3.0866\n","Iteration: 2408; Percent complete: 60.2%; Average loss: 3.2579\n","Iteration: 2409; Percent complete: 60.2%; Average loss: 3.0265\n","Iteration: 2410; Percent complete: 60.2%; Average loss: 2.7687\n","Iteration: 2411; Percent complete: 60.3%; Average loss: 2.9711\n","Iteration: 2412; Percent complete: 60.3%; Average loss: 2.8979\n","Iteration: 2413; Percent complete: 60.3%; Average loss: 3.0318\n","Iteration: 2414; Percent complete: 60.4%; Average loss: 2.9682\n","Iteration: 2415; Percent complete: 60.4%; Average loss: 3.2041\n","Iteration: 2416; Percent complete: 60.4%; Average loss: 2.8237\n","Iteration: 2417; Percent complete: 60.4%; Average loss: 3.0607\n","Iteration: 2418; Percent complete: 60.5%; Average loss: 2.9706\n","Iteration: 2419; Percent complete: 60.5%; Average loss: 3.3001\n","Iteration: 2420; Percent complete: 60.5%; Average loss: 2.9505\n","Iteration: 2421; Percent complete: 60.5%; Average loss: 2.7656\n","Iteration: 2422; Percent complete: 60.6%; Average loss: 3.1361\n","Iteration: 2423; Percent complete: 60.6%; Average loss: 3.2755\n","Iteration: 2424; Percent complete: 60.6%; Average loss: 2.8783\n","Iteration: 2425; Percent complete: 60.6%; Average loss: 2.9898\n","Iteration: 2426; Percent complete: 60.7%; Average loss: 3.0379\n","Iteration: 2427; Percent complete: 60.7%; Average loss: 3.1052\n","Iteration: 2428; Percent complete: 60.7%; Average loss: 3.1807\n","Iteration: 2429; Percent complete: 60.7%; Average loss: 2.9411\n","Iteration: 2430; Percent complete: 60.8%; Average loss: 3.1624\n","Iteration: 2431; Percent complete: 60.8%; Average loss: 3.2220\n","Iteration: 2432; Percent complete: 60.8%; Average loss: 3.0548\n","Iteration: 2433; Percent complete: 60.8%; Average loss: 3.0670\n","Iteration: 2434; Percent complete: 60.9%; Average loss: 3.3731\n","Iteration: 2435; Percent complete: 60.9%; Average loss: 2.9379\n","Iteration: 2436; Percent complete: 60.9%; Average loss: 2.8615\n","Iteration: 2437; Percent complete: 60.9%; Average loss: 2.9770\n","Iteration: 2438; Percent complete: 61.0%; Average loss: 3.0991\n","Iteration: 2439; Percent complete: 61.0%; Average loss: 2.9143\n","Iteration: 2440; Percent complete: 61.0%; Average loss: 3.1803\n","Iteration: 2441; Percent complete: 61.0%; Average loss: 3.1278\n","Iteration: 2442; Percent complete: 61.1%; Average loss: 3.1906\n","Iteration: 2443; Percent complete: 61.1%; Average loss: 3.1232\n","Iteration: 2444; Percent complete: 61.1%; Average loss: 3.0271\n","Iteration: 2445; Percent complete: 61.1%; Average loss: 2.6952\n","Iteration: 2446; Percent complete: 61.2%; Average loss: 3.0021\n","Iteration: 2447; Percent complete: 61.2%; Average loss: 2.9544\n","Iteration: 2448; Percent complete: 61.2%; Average loss: 2.8822\n","Iteration: 2449; Percent complete: 61.2%; Average loss: 2.9641\n","Iteration: 2450; Percent complete: 61.3%; Average loss: 3.2232\n","Iteration: 2451; Percent complete: 61.3%; Average loss: 2.8349\n","Iteration: 2452; Percent complete: 61.3%; Average loss: 3.1113\n","Iteration: 2453; Percent complete: 61.3%; Average loss: 3.1259\n","Iteration: 2454; Percent complete: 61.4%; Average loss: 2.9713\n","Iteration: 2455; Percent complete: 61.4%; Average loss: 3.1987\n","Iteration: 2456; Percent complete: 61.4%; Average loss: 2.9205\n","Iteration: 2457; Percent complete: 61.4%; Average loss: 3.1762\n","Iteration: 2458; Percent complete: 61.5%; Average loss: 3.1053\n","Iteration: 2459; Percent complete: 61.5%; Average loss: 2.8414\n","Iteration: 2460; Percent complete: 61.5%; Average loss: 3.2236\n","Iteration: 2461; Percent complete: 61.5%; Average loss: 2.9950\n","Iteration: 2462; Percent complete: 61.6%; Average loss: 3.0867\n","Iteration: 2463; Percent complete: 61.6%; Average loss: 3.0013\n","Iteration: 2464; Percent complete: 61.6%; Average loss: 2.9368\n","Iteration: 2465; Percent complete: 61.6%; Average loss: 3.2315\n","Iteration: 2466; Percent complete: 61.7%; Average loss: 3.2523\n","Iteration: 2467; Percent complete: 61.7%; Average loss: 3.0258\n","Iteration: 2468; Percent complete: 61.7%; Average loss: 3.1471\n","Iteration: 2469; Percent complete: 61.7%; Average loss: 3.0931\n","Iteration: 2470; Percent complete: 61.8%; Average loss: 2.9812\n","Iteration: 2471; Percent complete: 61.8%; Average loss: 2.8363\n","Iteration: 2472; Percent complete: 61.8%; Average loss: 3.0126\n","Iteration: 2473; Percent complete: 61.8%; Average loss: 2.9571\n","Iteration: 2474; Percent complete: 61.9%; Average loss: 3.0998\n","Iteration: 2475; Percent complete: 61.9%; Average loss: 3.0094\n","Iteration: 2476; Percent complete: 61.9%; Average loss: 3.1715\n","Iteration: 2477; Percent complete: 61.9%; Average loss: 3.0117\n","Iteration: 2478; Percent complete: 62.0%; Average loss: 3.2747\n","Iteration: 2479; Percent complete: 62.0%; Average loss: 3.1114\n","Iteration: 2480; Percent complete: 62.0%; Average loss: 3.0816\n","Iteration: 2481; Percent complete: 62.0%; Average loss: 2.8723\n","Iteration: 2482; Percent complete: 62.1%; Average loss: 2.8493\n","Iteration: 2483; Percent complete: 62.1%; Average loss: 3.0167\n","Iteration: 2484; Percent complete: 62.1%; Average loss: 2.9085\n","Iteration: 2485; Percent complete: 62.1%; Average loss: 2.7456\n","Iteration: 2486; Percent complete: 62.2%; Average loss: 2.7615\n","Iteration: 2487; Percent complete: 62.2%; Average loss: 3.0792\n","Iteration: 2488; Percent complete: 62.2%; Average loss: 3.1006\n","Iteration: 2489; Percent complete: 62.2%; Average loss: 3.0142\n","Iteration: 2490; Percent complete: 62.3%; Average loss: 3.0804\n","Iteration: 2491; Percent complete: 62.3%; Average loss: 3.2778\n","Iteration: 2492; Percent complete: 62.3%; Average loss: 2.9302\n","Iteration: 2493; Percent complete: 62.3%; Average loss: 2.8825\n","Iteration: 2494; Percent complete: 62.4%; Average loss: 2.9822\n","Iteration: 2495; Percent complete: 62.4%; Average loss: 2.9249\n","Iteration: 2496; Percent complete: 62.4%; Average loss: 2.9489\n","Iteration: 2497; Percent complete: 62.4%; Average loss: 2.7845\n","Iteration: 2498; Percent complete: 62.5%; Average loss: 2.9504\n","Iteration: 2499; Percent complete: 62.5%; Average loss: 3.1600\n","Iteration: 2500; Percent complete: 62.5%; Average loss: 2.8225\n","Iteration: 2501; Percent complete: 62.5%; Average loss: 3.1078\n","Iteration: 2502; Percent complete: 62.5%; Average loss: 3.1957\n","Iteration: 2503; Percent complete: 62.6%; Average loss: 3.0100\n","Iteration: 2504; Percent complete: 62.6%; Average loss: 3.1423\n","Iteration: 2505; Percent complete: 62.6%; Average loss: 3.2177\n","Iteration: 2506; Percent complete: 62.6%; Average loss: 2.8332\n","Iteration: 2507; Percent complete: 62.7%; Average loss: 2.7696\n","Iteration: 2508; Percent complete: 62.7%; Average loss: 3.0803\n","Iteration: 2509; Percent complete: 62.7%; Average loss: 3.1984\n","Iteration: 2510; Percent complete: 62.7%; Average loss: 2.9952\n","Iteration: 2511; Percent complete: 62.8%; Average loss: 3.0179\n","Iteration: 2512; Percent complete: 62.8%; Average loss: 2.8350\n","Iteration: 2513; Percent complete: 62.8%; Average loss: 3.0103\n","Iteration: 2514; Percent complete: 62.8%; Average loss: 3.1946\n","Iteration: 2515; Percent complete: 62.9%; Average loss: 3.0919\n","Iteration: 2516; Percent complete: 62.9%; Average loss: 3.0572\n","Iteration: 2517; Percent complete: 62.9%; Average loss: 3.1022\n","Iteration: 2518; Percent complete: 62.9%; Average loss: 2.8749\n","Iteration: 2519; Percent complete: 63.0%; Average loss: 3.1295\n","Iteration: 2520; Percent complete: 63.0%; Average loss: 3.2456\n","Iteration: 2521; Percent complete: 63.0%; Average loss: 3.1041\n","Iteration: 2522; Percent complete: 63.0%; Average loss: 3.0890\n","Iteration: 2523; Percent complete: 63.1%; Average loss: 2.9283\n","Iteration: 2524; Percent complete: 63.1%; Average loss: 3.0685\n","Iteration: 2525; Percent complete: 63.1%; Average loss: 3.1837\n","Iteration: 2526; Percent complete: 63.1%; Average loss: 3.1366\n","Iteration: 2527; Percent complete: 63.2%; Average loss: 3.1609\n","Iteration: 2528; Percent complete: 63.2%; Average loss: 2.6892\n","Iteration: 2529; Percent complete: 63.2%; Average loss: 3.2601\n","Iteration: 2530; Percent complete: 63.2%; Average loss: 2.8114\n","Iteration: 2531; Percent complete: 63.3%; Average loss: 2.9857\n","Iteration: 2532; Percent complete: 63.3%; Average loss: 3.0049\n","Iteration: 2533; Percent complete: 63.3%; Average loss: 3.0681\n","Iteration: 2534; Percent complete: 63.3%; Average loss: 3.2018\n","Iteration: 2535; Percent complete: 63.4%; Average loss: 2.9225\n","Iteration: 2536; Percent complete: 63.4%; Average loss: 3.0188\n","Iteration: 2537; Percent complete: 63.4%; Average loss: 2.8155\n","Iteration: 2538; Percent complete: 63.4%; Average loss: 3.1415\n","Iteration: 2539; Percent complete: 63.5%; Average loss: 3.2081\n","Iteration: 2540; Percent complete: 63.5%; Average loss: 2.6614\n","Iteration: 2541; Percent complete: 63.5%; Average loss: 3.0638\n","Iteration: 2542; Percent complete: 63.5%; Average loss: 2.9084\n","Iteration: 2543; Percent complete: 63.6%; Average loss: 2.9032\n","Iteration: 2544; Percent complete: 63.6%; Average loss: 3.1166\n","Iteration: 2545; Percent complete: 63.6%; Average loss: 3.3125\n","Iteration: 2546; Percent complete: 63.6%; Average loss: 3.0515\n","Iteration: 2547; Percent complete: 63.7%; Average loss: 2.9239\n","Iteration: 2548; Percent complete: 63.7%; Average loss: 3.0116\n","Iteration: 2549; Percent complete: 63.7%; Average loss: 3.1081\n","Iteration: 2550; Percent complete: 63.7%; Average loss: 3.3625\n","Iteration: 2551; Percent complete: 63.8%; Average loss: 3.0103\n","Iteration: 2552; Percent complete: 63.8%; Average loss: 2.6961\n","Iteration: 2553; Percent complete: 63.8%; Average loss: 2.7568\n","Iteration: 2554; Percent complete: 63.8%; Average loss: 3.2867\n","Iteration: 2555; Percent complete: 63.9%; Average loss: 3.0219\n","Iteration: 2556; Percent complete: 63.9%; Average loss: 2.7718\n","Iteration: 2557; Percent complete: 63.9%; Average loss: 2.8877\n","Iteration: 2558; Percent complete: 63.9%; Average loss: 2.8823\n","Iteration: 2559; Percent complete: 64.0%; Average loss: 2.9486\n","Iteration: 2560; Percent complete: 64.0%; Average loss: 2.8618\n","Iteration: 2561; Percent complete: 64.0%; Average loss: 3.0866\n","Iteration: 2562; Percent complete: 64.0%; Average loss: 2.8876\n","Iteration: 2563; Percent complete: 64.1%; Average loss: 2.7787\n","Iteration: 2564; Percent complete: 64.1%; Average loss: 3.1089\n","Iteration: 2565; Percent complete: 64.1%; Average loss: 2.9527\n","Iteration: 2566; Percent complete: 64.1%; Average loss: 2.7488\n","Iteration: 2567; Percent complete: 64.2%; Average loss: 3.0978\n","Iteration: 2568; Percent complete: 64.2%; Average loss: 3.0913\n","Iteration: 2569; Percent complete: 64.2%; Average loss: 3.2051\n","Iteration: 2570; Percent complete: 64.2%; Average loss: 3.0198\n","Iteration: 2571; Percent complete: 64.3%; Average loss: 2.9833\n","Iteration: 2572; Percent complete: 64.3%; Average loss: 3.1840\n","Iteration: 2573; Percent complete: 64.3%; Average loss: 3.0619\n","Iteration: 2574; Percent complete: 64.3%; Average loss: 3.2416\n","Iteration: 2575; Percent complete: 64.4%; Average loss: 3.1120\n","Iteration: 2576; Percent complete: 64.4%; Average loss: 2.9413\n","Iteration: 2577; Percent complete: 64.4%; Average loss: 3.1098\n","Iteration: 2578; Percent complete: 64.5%; Average loss: 3.0288\n","Iteration: 2579; Percent complete: 64.5%; Average loss: 2.9875\n","Iteration: 2580; Percent complete: 64.5%; Average loss: 3.1395\n","Iteration: 2581; Percent complete: 64.5%; Average loss: 2.9169\n","Iteration: 2582; Percent complete: 64.5%; Average loss: 3.0887\n","Iteration: 2583; Percent complete: 64.6%; Average loss: 3.1549\n","Iteration: 2584; Percent complete: 64.6%; Average loss: 3.0792\n","Iteration: 2585; Percent complete: 64.6%; Average loss: 3.1035\n","Iteration: 2586; Percent complete: 64.6%; Average loss: 3.1847\n","Iteration: 2587; Percent complete: 64.7%; Average loss: 3.0919\n","Iteration: 2588; Percent complete: 64.7%; Average loss: 3.2319\n","Iteration: 2589; Percent complete: 64.7%; Average loss: 3.0950\n","Iteration: 2590; Percent complete: 64.8%; Average loss: 2.7958\n","Iteration: 2591; Percent complete: 64.8%; Average loss: 3.1227\n","Iteration: 2592; Percent complete: 64.8%; Average loss: 2.9983\n","Iteration: 2593; Percent complete: 64.8%; Average loss: 2.9405\n","Iteration: 2594; Percent complete: 64.8%; Average loss: 3.0699\n","Iteration: 2595; Percent complete: 64.9%; Average loss: 2.8409\n","Iteration: 2596; Percent complete: 64.9%; Average loss: 3.0620\n","Iteration: 2597; Percent complete: 64.9%; Average loss: 3.0742\n","Iteration: 2598; Percent complete: 65.0%; Average loss: 2.7414\n","Iteration: 2599; Percent complete: 65.0%; Average loss: 2.8713\n","Iteration: 2600; Percent complete: 65.0%; Average loss: 2.8868\n","Iteration: 2601; Percent complete: 65.0%; Average loss: 3.1591\n","Iteration: 2602; Percent complete: 65.0%; Average loss: 3.1607\n","Iteration: 2603; Percent complete: 65.1%; Average loss: 3.1006\n","Iteration: 2604; Percent complete: 65.1%; Average loss: 2.9589\n","Iteration: 2605; Percent complete: 65.1%; Average loss: 2.8122\n","Iteration: 2606; Percent complete: 65.1%; Average loss: 3.0893\n","Iteration: 2607; Percent complete: 65.2%; Average loss: 2.8250\n","Iteration: 2608; Percent complete: 65.2%; Average loss: 3.1850\n","Iteration: 2609; Percent complete: 65.2%; Average loss: 2.9469\n","Iteration: 2610; Percent complete: 65.2%; Average loss: 3.1244\n","Iteration: 2611; Percent complete: 65.3%; Average loss: 2.9004\n","Iteration: 2612; Percent complete: 65.3%; Average loss: 3.0772\n","Iteration: 2613; Percent complete: 65.3%; Average loss: 3.0414\n","Iteration: 2614; Percent complete: 65.3%; Average loss: 3.2645\n","Iteration: 2615; Percent complete: 65.4%; Average loss: 3.1281\n","Iteration: 2616; Percent complete: 65.4%; Average loss: 3.1683\n","Iteration: 2617; Percent complete: 65.4%; Average loss: 2.7684\n","Iteration: 2618; Percent complete: 65.5%; Average loss: 2.8572\n","Iteration: 2619; Percent complete: 65.5%; Average loss: 3.1864\n","Iteration: 2620; Percent complete: 65.5%; Average loss: 2.7416\n","Iteration: 2621; Percent complete: 65.5%; Average loss: 2.9621\n","Iteration: 2622; Percent complete: 65.5%; Average loss: 3.0202\n","Iteration: 2623; Percent complete: 65.6%; Average loss: 2.9413\n","Iteration: 2624; Percent complete: 65.6%; Average loss: 2.8154\n","Iteration: 2625; Percent complete: 65.6%; Average loss: 2.9385\n","Iteration: 2626; Percent complete: 65.6%; Average loss: 3.0374\n","Iteration: 2627; Percent complete: 65.7%; Average loss: 3.0038\n","Iteration: 2628; Percent complete: 65.7%; Average loss: 2.9689\n","Iteration: 2629; Percent complete: 65.7%; Average loss: 3.2785\n","Iteration: 2630; Percent complete: 65.8%; Average loss: 2.9083\n","Iteration: 2631; Percent complete: 65.8%; Average loss: 3.2039\n","Iteration: 2632; Percent complete: 65.8%; Average loss: 3.2428\n","Iteration: 2633; Percent complete: 65.8%; Average loss: 3.1994\n","Iteration: 2634; Percent complete: 65.8%; Average loss: 3.0489\n","Iteration: 2635; Percent complete: 65.9%; Average loss: 3.1964\n","Iteration: 2636; Percent complete: 65.9%; Average loss: 2.9531\n","Iteration: 2637; Percent complete: 65.9%; Average loss: 3.1318\n","Iteration: 2638; Percent complete: 66.0%; Average loss: 3.0333\n","Iteration: 2639; Percent complete: 66.0%; Average loss: 2.9840\n","Iteration: 2640; Percent complete: 66.0%; Average loss: 2.9161\n","Iteration: 2641; Percent complete: 66.0%; Average loss: 3.0198\n","Iteration: 2642; Percent complete: 66.0%; Average loss: 3.0088\n","Iteration: 2643; Percent complete: 66.1%; Average loss: 2.9611\n","Iteration: 2644; Percent complete: 66.1%; Average loss: 3.0325\n","Iteration: 2645; Percent complete: 66.1%; Average loss: 3.0977\n","Iteration: 2646; Percent complete: 66.1%; Average loss: 2.9874\n","Iteration: 2647; Percent complete: 66.2%; Average loss: 2.9978\n","Iteration: 2648; Percent complete: 66.2%; Average loss: 2.9874\n","Iteration: 2649; Percent complete: 66.2%; Average loss: 3.1996\n","Iteration: 2650; Percent complete: 66.2%; Average loss: 3.1996\n","Iteration: 2651; Percent complete: 66.3%; Average loss: 2.6363\n","Iteration: 2652; Percent complete: 66.3%; Average loss: 2.8429\n","Iteration: 2653; Percent complete: 66.3%; Average loss: 2.9015\n","Iteration: 2654; Percent complete: 66.3%; Average loss: 3.1648\n","Iteration: 2655; Percent complete: 66.4%; Average loss: 2.7480\n","Iteration: 2656; Percent complete: 66.4%; Average loss: 3.0549\n","Iteration: 2657; Percent complete: 66.4%; Average loss: 2.7950\n","Iteration: 2658; Percent complete: 66.5%; Average loss: 2.8874\n","Iteration: 2659; Percent complete: 66.5%; Average loss: 3.0347\n","Iteration: 2660; Percent complete: 66.5%; Average loss: 2.9785\n","Iteration: 2661; Percent complete: 66.5%; Average loss: 2.9328\n","Iteration: 2662; Percent complete: 66.5%; Average loss: 2.9624\n","Iteration: 2663; Percent complete: 66.6%; Average loss: 3.0191\n","Iteration: 2664; Percent complete: 66.6%; Average loss: 2.9892\n","Iteration: 2665; Percent complete: 66.6%; Average loss: 2.7850\n","Iteration: 2666; Percent complete: 66.6%; Average loss: 2.7574\n","Iteration: 2667; Percent complete: 66.7%; Average loss: 2.8991\n","Iteration: 2668; Percent complete: 66.7%; Average loss: 2.6920\n","Iteration: 2669; Percent complete: 66.7%; Average loss: 2.8069\n","Iteration: 2670; Percent complete: 66.8%; Average loss: 3.0270\n","Iteration: 2671; Percent complete: 66.8%; Average loss: 2.9658\n","Iteration: 2672; Percent complete: 66.8%; Average loss: 3.1372\n","Iteration: 2673; Percent complete: 66.8%; Average loss: 3.2170\n","Iteration: 2674; Percent complete: 66.8%; Average loss: 3.3731\n","Iteration: 2675; Percent complete: 66.9%; Average loss: 3.1457\n","Iteration: 2676; Percent complete: 66.9%; Average loss: 2.9366\n","Iteration: 2677; Percent complete: 66.9%; Average loss: 3.2895\n","Iteration: 2678; Percent complete: 67.0%; Average loss: 2.9114\n","Iteration: 2679; Percent complete: 67.0%; Average loss: 2.7945\n","Iteration: 2680; Percent complete: 67.0%; Average loss: 3.1163\n","Iteration: 2681; Percent complete: 67.0%; Average loss: 3.1660\n","Iteration: 2682; Percent complete: 67.0%; Average loss: 3.0664\n","Iteration: 2683; Percent complete: 67.1%; Average loss: 2.9889\n","Iteration: 2684; Percent complete: 67.1%; Average loss: 2.9928\n","Iteration: 2685; Percent complete: 67.1%; Average loss: 2.9037\n","Iteration: 2686; Percent complete: 67.2%; Average loss: 2.7743\n","Iteration: 2687; Percent complete: 67.2%; Average loss: 3.0910\n","Iteration: 2688; Percent complete: 67.2%; Average loss: 3.1397\n","Iteration: 2689; Percent complete: 67.2%; Average loss: 2.8963\n","Iteration: 2690; Percent complete: 67.2%; Average loss: 3.0810\n","Iteration: 2691; Percent complete: 67.3%; Average loss: 2.8264\n","Iteration: 2692; Percent complete: 67.3%; Average loss: 2.9718\n","Iteration: 2693; Percent complete: 67.3%; Average loss: 2.7906\n","Iteration: 2694; Percent complete: 67.3%; Average loss: 3.1580\n","Iteration: 2695; Percent complete: 67.4%; Average loss: 3.1633\n","Iteration: 2696; Percent complete: 67.4%; Average loss: 3.2210\n","Iteration: 2697; Percent complete: 67.4%; Average loss: 3.1471\n","Iteration: 2698; Percent complete: 67.5%; Average loss: 2.9879\n","Iteration: 2699; Percent complete: 67.5%; Average loss: 2.7971\n","Iteration: 2700; Percent complete: 67.5%; Average loss: 2.8880\n","Iteration: 2701; Percent complete: 67.5%; Average loss: 2.9480\n","Iteration: 2702; Percent complete: 67.5%; Average loss: 3.0276\n","Iteration: 2703; Percent complete: 67.6%; Average loss: 2.9700\n","Iteration: 2704; Percent complete: 67.6%; Average loss: 3.0776\n","Iteration: 2705; Percent complete: 67.6%; Average loss: 3.0057\n","Iteration: 2706; Percent complete: 67.7%; Average loss: 3.1166\n","Iteration: 2707; Percent complete: 67.7%; Average loss: 3.0768\n","Iteration: 2708; Percent complete: 67.7%; Average loss: 3.0122\n","Iteration: 2709; Percent complete: 67.7%; Average loss: 2.9563\n","Iteration: 2710; Percent complete: 67.8%; Average loss: 3.2512\n","Iteration: 2711; Percent complete: 67.8%; Average loss: 3.1201\n","Iteration: 2712; Percent complete: 67.8%; Average loss: 3.1616\n","Iteration: 2713; Percent complete: 67.8%; Average loss: 3.0125\n","Iteration: 2714; Percent complete: 67.8%; Average loss: 3.0338\n","Iteration: 2715; Percent complete: 67.9%; Average loss: 2.9933\n","Iteration: 2716; Percent complete: 67.9%; Average loss: 2.8909\n","Iteration: 2717; Percent complete: 67.9%; Average loss: 2.8875\n","Iteration: 2718; Percent complete: 68.0%; Average loss: 3.0022\n","Iteration: 2719; Percent complete: 68.0%; Average loss: 2.7069\n","Iteration: 2720; Percent complete: 68.0%; Average loss: 2.7081\n","Iteration: 2721; Percent complete: 68.0%; Average loss: 3.1360\n","Iteration: 2722; Percent complete: 68.0%; Average loss: 2.9489\n","Iteration: 2723; Percent complete: 68.1%; Average loss: 3.1751\n","Iteration: 2724; Percent complete: 68.1%; Average loss: 3.1807\n","Iteration: 2725; Percent complete: 68.1%; Average loss: 2.9573\n","Iteration: 2726; Percent complete: 68.2%; Average loss: 2.8948\n","Iteration: 2727; Percent complete: 68.2%; Average loss: 2.9119\n","Iteration: 2728; Percent complete: 68.2%; Average loss: 2.9946\n","Iteration: 2729; Percent complete: 68.2%; Average loss: 2.7751\n","Iteration: 2730; Percent complete: 68.2%; Average loss: 3.0672\n","Iteration: 2731; Percent complete: 68.3%; Average loss: 2.8234\n","Iteration: 2732; Percent complete: 68.3%; Average loss: 3.1189\n","Iteration: 2733; Percent complete: 68.3%; Average loss: 2.7425\n","Iteration: 2734; Percent complete: 68.3%; Average loss: 2.9110\n","Iteration: 2735; Percent complete: 68.4%; Average loss: 2.8613\n","Iteration: 2736; Percent complete: 68.4%; Average loss: 3.0209\n","Iteration: 2737; Percent complete: 68.4%; Average loss: 2.8138\n","Iteration: 2738; Percent complete: 68.5%; Average loss: 2.9201\n","Iteration: 2739; Percent complete: 68.5%; Average loss: 2.9513\n","Iteration: 2740; Percent complete: 68.5%; Average loss: 2.7779\n","Iteration: 2741; Percent complete: 68.5%; Average loss: 2.7900\n","Iteration: 2742; Percent complete: 68.5%; Average loss: 3.1641\n","Iteration: 2743; Percent complete: 68.6%; Average loss: 2.9361\n","Iteration: 2744; Percent complete: 68.6%; Average loss: 3.0199\n","Iteration: 2745; Percent complete: 68.6%; Average loss: 2.8963\n","Iteration: 2746; Percent complete: 68.7%; Average loss: 2.9570\n","Iteration: 2747; Percent complete: 68.7%; Average loss: 2.5414\n","Iteration: 2748; Percent complete: 68.7%; Average loss: 2.9082\n","Iteration: 2749; Percent complete: 68.7%; Average loss: 2.6539\n","Iteration: 2750; Percent complete: 68.8%; Average loss: 2.9851\n","Iteration: 2751; Percent complete: 68.8%; Average loss: 3.0940\n","Iteration: 2752; Percent complete: 68.8%; Average loss: 2.7888\n","Iteration: 2753; Percent complete: 68.8%; Average loss: 3.2185\n","Iteration: 2754; Percent complete: 68.8%; Average loss: 2.8347\n","Iteration: 2755; Percent complete: 68.9%; Average loss: 2.9100\n","Iteration: 2756; Percent complete: 68.9%; Average loss: 2.8586\n","Iteration: 2757; Percent complete: 68.9%; Average loss: 2.9620\n","Iteration: 2758; Percent complete: 69.0%; Average loss: 2.8563\n","Iteration: 2759; Percent complete: 69.0%; Average loss: 2.8181\n","Iteration: 2760; Percent complete: 69.0%; Average loss: 2.8304\n","Iteration: 2761; Percent complete: 69.0%; Average loss: 2.8680\n","Iteration: 2762; Percent complete: 69.0%; Average loss: 2.9835\n","Iteration: 2763; Percent complete: 69.1%; Average loss: 3.0166\n","Iteration: 2764; Percent complete: 69.1%; Average loss: 2.9433\n","Iteration: 2765; Percent complete: 69.1%; Average loss: 3.0914\n","Iteration: 2766; Percent complete: 69.2%; Average loss: 2.8011\n","Iteration: 2767; Percent complete: 69.2%; Average loss: 3.0291\n","Iteration: 2768; Percent complete: 69.2%; Average loss: 2.9037\n","Iteration: 2769; Percent complete: 69.2%; Average loss: 3.0638\n","Iteration: 2770; Percent complete: 69.2%; Average loss: 3.0578\n","Iteration: 2771; Percent complete: 69.3%; Average loss: 3.1275\n","Iteration: 2772; Percent complete: 69.3%; Average loss: 2.8856\n","Iteration: 2773; Percent complete: 69.3%; Average loss: 2.8815\n","Iteration: 2774; Percent complete: 69.3%; Average loss: 2.9525\n","Iteration: 2775; Percent complete: 69.4%; Average loss: 2.8209\n","Iteration: 2776; Percent complete: 69.4%; Average loss: 3.0501\n","Iteration: 2777; Percent complete: 69.4%; Average loss: 2.8684\n","Iteration: 2778; Percent complete: 69.5%; Average loss: 3.0864\n","Iteration: 2779; Percent complete: 69.5%; Average loss: 2.9549\n","Iteration: 2780; Percent complete: 69.5%; Average loss: 2.8438\n","Iteration: 2781; Percent complete: 69.5%; Average loss: 2.8415\n","Iteration: 2782; Percent complete: 69.5%; Average loss: 2.7824\n","Iteration: 2783; Percent complete: 69.6%; Average loss: 3.0852\n","Iteration: 2784; Percent complete: 69.6%; Average loss: 2.8224\n","Iteration: 2785; Percent complete: 69.6%; Average loss: 2.9073\n","Iteration: 2786; Percent complete: 69.7%; Average loss: 2.9855\n","Iteration: 2787; Percent complete: 69.7%; Average loss: 2.9677\n","Iteration: 2788; Percent complete: 69.7%; Average loss: 3.3027\n","Iteration: 2789; Percent complete: 69.7%; Average loss: 3.0047\n","Iteration: 2790; Percent complete: 69.8%; Average loss: 2.8897\n","Iteration: 2791; Percent complete: 69.8%; Average loss: 2.8959\n","Iteration: 2792; Percent complete: 69.8%; Average loss: 3.2278\n","Iteration: 2793; Percent complete: 69.8%; Average loss: 3.0879\n","Iteration: 2794; Percent complete: 69.8%; Average loss: 2.9052\n","Iteration: 2795; Percent complete: 69.9%; Average loss: 2.9200\n","Iteration: 2796; Percent complete: 69.9%; Average loss: 2.9960\n","Iteration: 2797; Percent complete: 69.9%; Average loss: 2.5463\n","Iteration: 2798; Percent complete: 70.0%; Average loss: 2.6206\n","Iteration: 2799; Percent complete: 70.0%; Average loss: 2.9359\n","Iteration: 2800; Percent complete: 70.0%; Average loss: 2.9618\n","Iteration: 2801; Percent complete: 70.0%; Average loss: 2.8432\n","Iteration: 2802; Percent complete: 70.0%; Average loss: 2.9040\n","Iteration: 2803; Percent complete: 70.1%; Average loss: 2.9287\n","Iteration: 2804; Percent complete: 70.1%; Average loss: 2.8066\n","Iteration: 2805; Percent complete: 70.1%; Average loss: 3.0781\n","Iteration: 2806; Percent complete: 70.2%; Average loss: 2.7781\n","Iteration: 2807; Percent complete: 70.2%; Average loss: 3.2952\n","Iteration: 2808; Percent complete: 70.2%; Average loss: 3.0034\n","Iteration: 2809; Percent complete: 70.2%; Average loss: 2.9832\n","Iteration: 2810; Percent complete: 70.2%; Average loss: 2.9012\n","Iteration: 2811; Percent complete: 70.3%; Average loss: 2.5863\n","Iteration: 2812; Percent complete: 70.3%; Average loss: 2.9356\n","Iteration: 2813; Percent complete: 70.3%; Average loss: 2.9002\n","Iteration: 2814; Percent complete: 70.3%; Average loss: 3.1004\n","Iteration: 2815; Percent complete: 70.4%; Average loss: 2.9623\n","Iteration: 2816; Percent complete: 70.4%; Average loss: 3.0766\n","Iteration: 2817; Percent complete: 70.4%; Average loss: 2.8821\n","Iteration: 2818; Percent complete: 70.5%; Average loss: 2.8511\n","Iteration: 2819; Percent complete: 70.5%; Average loss: 2.9457\n","Iteration: 2820; Percent complete: 70.5%; Average loss: 2.8318\n","Iteration: 2821; Percent complete: 70.5%; Average loss: 3.0359\n","Iteration: 2822; Percent complete: 70.5%; Average loss: 3.0699\n","Iteration: 2823; Percent complete: 70.6%; Average loss: 2.9706\n","Iteration: 2824; Percent complete: 70.6%; Average loss: 2.8071\n","Iteration: 2825; Percent complete: 70.6%; Average loss: 2.8504\n","Iteration: 2826; Percent complete: 70.7%; Average loss: 2.9725\n","Iteration: 2827; Percent complete: 70.7%; Average loss: 2.7361\n","Iteration: 2828; Percent complete: 70.7%; Average loss: 3.1837\n","Iteration: 2829; Percent complete: 70.7%; Average loss: 2.7079\n","Iteration: 2830; Percent complete: 70.8%; Average loss: 3.0187\n","Iteration: 2831; Percent complete: 70.8%; Average loss: 2.6677\n","Iteration: 2832; Percent complete: 70.8%; Average loss: 2.9618\n","Iteration: 2833; Percent complete: 70.8%; Average loss: 2.9094\n","Iteration: 2834; Percent complete: 70.9%; Average loss: 2.9344\n","Iteration: 2835; Percent complete: 70.9%; Average loss: 2.7875\n","Iteration: 2836; Percent complete: 70.9%; Average loss: 2.7426\n","Iteration: 2837; Percent complete: 70.9%; Average loss: 2.5968\n","Iteration: 2838; Percent complete: 71.0%; Average loss: 2.6317\n","Iteration: 2839; Percent complete: 71.0%; Average loss: 2.9453\n","Iteration: 2840; Percent complete: 71.0%; Average loss: 2.9811\n","Iteration: 2841; Percent complete: 71.0%; Average loss: 2.7394\n","Iteration: 2842; Percent complete: 71.0%; Average loss: 2.9221\n","Iteration: 2843; Percent complete: 71.1%; Average loss: 3.1374\n","Iteration: 2844; Percent complete: 71.1%; Average loss: 2.7252\n","Iteration: 2845; Percent complete: 71.1%; Average loss: 3.0016\n","Iteration: 2846; Percent complete: 71.2%; Average loss: 2.7887\n","Iteration: 2847; Percent complete: 71.2%; Average loss: 3.0417\n","Iteration: 2848; Percent complete: 71.2%; Average loss: 2.9176\n","Iteration: 2849; Percent complete: 71.2%; Average loss: 2.8109\n","Iteration: 2850; Percent complete: 71.2%; Average loss: 2.7380\n","Iteration: 2851; Percent complete: 71.3%; Average loss: 2.9909\n","Iteration: 2852; Percent complete: 71.3%; Average loss: 2.9551\n","Iteration: 2853; Percent complete: 71.3%; Average loss: 2.9716\n","Iteration: 2854; Percent complete: 71.4%; Average loss: 2.9902\n","Iteration: 2855; Percent complete: 71.4%; Average loss: 2.8690\n","Iteration: 2856; Percent complete: 71.4%; Average loss: 3.1203\n","Iteration: 2857; Percent complete: 71.4%; Average loss: 3.0099\n","Iteration: 2858; Percent complete: 71.5%; Average loss: 2.9722\n","Iteration: 2859; Percent complete: 71.5%; Average loss: 2.9032\n","Iteration: 2860; Percent complete: 71.5%; Average loss: 2.8478\n","Iteration: 2861; Percent complete: 71.5%; Average loss: 2.9576\n","Iteration: 2862; Percent complete: 71.5%; Average loss: 3.0005\n","Iteration: 2863; Percent complete: 71.6%; Average loss: 2.8720\n","Iteration: 2864; Percent complete: 71.6%; Average loss: 2.9815\n","Iteration: 2865; Percent complete: 71.6%; Average loss: 2.9245\n","Iteration: 2866; Percent complete: 71.7%; Average loss: 2.9234\n","Iteration: 2867; Percent complete: 71.7%; Average loss: 3.0139\n","Iteration: 2868; Percent complete: 71.7%; Average loss: 3.0720\n","Iteration: 2869; Percent complete: 71.7%; Average loss: 2.8686\n","Iteration: 2870; Percent complete: 71.8%; Average loss: 2.6254\n","Iteration: 2871; Percent complete: 71.8%; Average loss: 2.9856\n","Iteration: 2872; Percent complete: 71.8%; Average loss: 2.9076\n","Iteration: 2873; Percent complete: 71.8%; Average loss: 2.8395\n","Iteration: 2874; Percent complete: 71.9%; Average loss: 2.9960\n","Iteration: 2875; Percent complete: 71.9%; Average loss: 2.9067\n","Iteration: 2876; Percent complete: 71.9%; Average loss: 3.0682\n","Iteration: 2877; Percent complete: 71.9%; Average loss: 2.8739\n","Iteration: 2878; Percent complete: 72.0%; Average loss: 2.8596\n","Iteration: 2879; Percent complete: 72.0%; Average loss: 2.7205\n","Iteration: 2880; Percent complete: 72.0%; Average loss: 2.8890\n","Iteration: 2881; Percent complete: 72.0%; Average loss: 2.9558\n","Iteration: 2882; Percent complete: 72.0%; Average loss: 2.9908\n","Iteration: 2883; Percent complete: 72.1%; Average loss: 2.6685\n","Iteration: 2884; Percent complete: 72.1%; Average loss: 2.8039\n","Iteration: 2885; Percent complete: 72.1%; Average loss: 2.8988\n","Iteration: 2886; Percent complete: 72.2%; Average loss: 2.9196\n","Iteration: 2887; Percent complete: 72.2%; Average loss: 2.8104\n","Iteration: 2888; Percent complete: 72.2%; Average loss: 2.6521\n","Iteration: 2889; Percent complete: 72.2%; Average loss: 2.9960\n","Iteration: 2890; Percent complete: 72.2%; Average loss: 2.9655\n","Iteration: 2891; Percent complete: 72.3%; Average loss: 3.1574\n","Iteration: 2892; Percent complete: 72.3%; Average loss: 2.9770\n","Iteration: 2893; Percent complete: 72.3%; Average loss: 2.9368\n","Iteration: 2894; Percent complete: 72.4%; Average loss: 2.8910\n","Iteration: 2895; Percent complete: 72.4%; Average loss: 2.7292\n","Iteration: 2896; Percent complete: 72.4%; Average loss: 2.7641\n","Iteration: 2897; Percent complete: 72.4%; Average loss: 2.7411\n","Iteration: 2898; Percent complete: 72.5%; Average loss: 3.0646\n","Iteration: 2899; Percent complete: 72.5%; Average loss: 2.9945\n","Iteration: 2900; Percent complete: 72.5%; Average loss: 2.9144\n","Iteration: 2901; Percent complete: 72.5%; Average loss: 3.0329\n","Iteration: 2902; Percent complete: 72.5%; Average loss: 3.1213\n","Iteration: 2903; Percent complete: 72.6%; Average loss: 2.8020\n","Iteration: 2904; Percent complete: 72.6%; Average loss: 3.0054\n","Iteration: 2905; Percent complete: 72.6%; Average loss: 2.8390\n","Iteration: 2906; Percent complete: 72.7%; Average loss: 2.8926\n","Iteration: 2907; Percent complete: 72.7%; Average loss: 2.7924\n","Iteration: 2908; Percent complete: 72.7%; Average loss: 2.9422\n","Iteration: 2909; Percent complete: 72.7%; Average loss: 3.0580\n","Iteration: 2910; Percent complete: 72.8%; Average loss: 2.8267\n","Iteration: 2911; Percent complete: 72.8%; Average loss: 3.0096\n","Iteration: 2912; Percent complete: 72.8%; Average loss: 2.7704\n","Iteration: 2913; Percent complete: 72.8%; Average loss: 2.9782\n","Iteration: 2914; Percent complete: 72.9%; Average loss: 2.8764\n","Iteration: 2915; Percent complete: 72.9%; Average loss: 3.1405\n","Iteration: 2916; Percent complete: 72.9%; Average loss: 3.0231\n","Iteration: 2917; Percent complete: 72.9%; Average loss: 2.9045\n","Iteration: 2918; Percent complete: 73.0%; Average loss: 3.1015\n","Iteration: 2919; Percent complete: 73.0%; Average loss: 2.9376\n","Iteration: 2920; Percent complete: 73.0%; Average loss: 3.1045\n","Iteration: 2921; Percent complete: 73.0%; Average loss: 2.7446\n","Iteration: 2922; Percent complete: 73.0%; Average loss: 2.9669\n","Iteration: 2923; Percent complete: 73.1%; Average loss: 3.0442\n","Iteration: 2924; Percent complete: 73.1%; Average loss: 2.9050\n","Iteration: 2925; Percent complete: 73.1%; Average loss: 2.8568\n","Iteration: 2926; Percent complete: 73.2%; Average loss: 2.9518\n","Iteration: 2927; Percent complete: 73.2%; Average loss: 3.2656\n","Iteration: 2928; Percent complete: 73.2%; Average loss: 2.9451\n","Iteration: 2929; Percent complete: 73.2%; Average loss: 3.0407\n","Iteration: 2930; Percent complete: 73.2%; Average loss: 2.9807\n","Iteration: 2931; Percent complete: 73.3%; Average loss: 2.8489\n","Iteration: 2932; Percent complete: 73.3%; Average loss: 2.8679\n","Iteration: 2933; Percent complete: 73.3%; Average loss: 3.1212\n","Iteration: 2934; Percent complete: 73.4%; Average loss: 2.8973\n","Iteration: 2935; Percent complete: 73.4%; Average loss: 2.9637\n","Iteration: 2936; Percent complete: 73.4%; Average loss: 2.9589\n","Iteration: 2937; Percent complete: 73.4%; Average loss: 2.9732\n","Iteration: 2938; Percent complete: 73.5%; Average loss: 3.0736\n","Iteration: 2939; Percent complete: 73.5%; Average loss: 3.0543\n","Iteration: 2940; Percent complete: 73.5%; Average loss: 2.8632\n","Iteration: 2941; Percent complete: 73.5%; Average loss: 2.9779\n","Iteration: 2942; Percent complete: 73.6%; Average loss: 2.9059\n","Iteration: 2943; Percent complete: 73.6%; Average loss: 2.8913\n","Iteration: 2944; Percent complete: 73.6%; Average loss: 2.8711\n","Iteration: 2945; Percent complete: 73.6%; Average loss: 3.2580\n","Iteration: 2946; Percent complete: 73.7%; Average loss: 2.7121\n","Iteration: 2947; Percent complete: 73.7%; Average loss: 2.9202\n","Iteration: 2948; Percent complete: 73.7%; Average loss: 2.9836\n","Iteration: 2949; Percent complete: 73.7%; Average loss: 3.2676\n","Iteration: 2950; Percent complete: 73.8%; Average loss: 2.9496\n","Iteration: 2951; Percent complete: 73.8%; Average loss: 2.7548\n","Iteration: 2952; Percent complete: 73.8%; Average loss: 3.0158\n","Iteration: 2953; Percent complete: 73.8%; Average loss: 3.0207\n","Iteration: 2954; Percent complete: 73.9%; Average loss: 3.1965\n","Iteration: 2955; Percent complete: 73.9%; Average loss: 2.9950\n","Iteration: 2956; Percent complete: 73.9%; Average loss: 2.8651\n","Iteration: 2957; Percent complete: 73.9%; Average loss: 2.7861\n","Iteration: 2958; Percent complete: 74.0%; Average loss: 2.6840\n","Iteration: 2959; Percent complete: 74.0%; Average loss: 2.8487\n","Iteration: 2960; Percent complete: 74.0%; Average loss: 3.2578\n","Iteration: 2961; Percent complete: 74.0%; Average loss: 2.9384\n","Iteration: 2962; Percent complete: 74.1%; Average loss: 2.9022\n","Iteration: 2963; Percent complete: 74.1%; Average loss: 2.9146\n","Iteration: 2964; Percent complete: 74.1%; Average loss: 2.8927\n","Iteration: 2965; Percent complete: 74.1%; Average loss: 2.9887\n","Iteration: 2966; Percent complete: 74.2%; Average loss: 2.9135\n","Iteration: 2967; Percent complete: 74.2%; Average loss: 2.9492\n","Iteration: 2968; Percent complete: 74.2%; Average loss: 2.9292\n","Iteration: 2969; Percent complete: 74.2%; Average loss: 2.9560\n","Iteration: 2970; Percent complete: 74.2%; Average loss: 2.9017\n","Iteration: 2971; Percent complete: 74.3%; Average loss: 2.8892\n","Iteration: 2972; Percent complete: 74.3%; Average loss: 2.9901\n","Iteration: 2973; Percent complete: 74.3%; Average loss: 2.7758\n","Iteration: 2974; Percent complete: 74.4%; Average loss: 2.8430\n","Iteration: 2975; Percent complete: 74.4%; Average loss: 2.8181\n","Iteration: 2976; Percent complete: 74.4%; Average loss: 3.1210\n","Iteration: 2977; Percent complete: 74.4%; Average loss: 2.7305\n","Iteration: 2978; Percent complete: 74.5%; Average loss: 2.8383\n","Iteration: 2979; Percent complete: 74.5%; Average loss: 2.9398\n","Iteration: 2980; Percent complete: 74.5%; Average loss: 3.2102\n","Iteration: 2981; Percent complete: 74.5%; Average loss: 2.5497\n","Iteration: 2982; Percent complete: 74.6%; Average loss: 2.8780\n","Iteration: 2983; Percent complete: 74.6%; Average loss: 2.9328\n","Iteration: 2984; Percent complete: 74.6%; Average loss: 2.5330\n","Iteration: 2985; Percent complete: 74.6%; Average loss: 2.6194\n","Iteration: 2986; Percent complete: 74.7%; Average loss: 2.9876\n","Iteration: 2987; Percent complete: 74.7%; Average loss: 2.5888\n","Iteration: 2988; Percent complete: 74.7%; Average loss: 2.7289\n","Iteration: 2989; Percent complete: 74.7%; Average loss: 2.8564\n","Iteration: 2990; Percent complete: 74.8%; Average loss: 2.8064\n","Iteration: 2991; Percent complete: 74.8%; Average loss: 2.6690\n","Iteration: 2992; Percent complete: 74.8%; Average loss: 3.1497\n","Iteration: 2993; Percent complete: 74.8%; Average loss: 2.8254\n","Iteration: 2994; Percent complete: 74.9%; Average loss: 2.9563\n","Iteration: 2995; Percent complete: 74.9%; Average loss: 2.8567\n","Iteration: 2996; Percent complete: 74.9%; Average loss: 3.0155\n","Iteration: 2997; Percent complete: 74.9%; Average loss: 3.0196\n","Iteration: 2998; Percent complete: 75.0%; Average loss: 2.7191\n","Iteration: 2999; Percent complete: 75.0%; Average loss: 3.0123\n","Iteration: 3000; Percent complete: 75.0%; Average loss: 2.9269\n","Iteration: 3001; Percent complete: 75.0%; Average loss: 2.6807\n","Iteration: 3002; Percent complete: 75.0%; Average loss: 2.7970\n","Iteration: 3003; Percent complete: 75.1%; Average loss: 3.0971\n","Iteration: 3004; Percent complete: 75.1%; Average loss: 2.8227\n","Iteration: 3005; Percent complete: 75.1%; Average loss: 2.9389\n","Iteration: 3006; Percent complete: 75.1%; Average loss: 2.8465\n","Iteration: 3007; Percent complete: 75.2%; Average loss: 3.0603\n","Iteration: 3008; Percent complete: 75.2%; Average loss: 3.1079\n","Iteration: 3009; Percent complete: 75.2%; Average loss: 2.8750\n","Iteration: 3010; Percent complete: 75.2%; Average loss: 2.8170\n","Iteration: 3011; Percent complete: 75.3%; Average loss: 3.0349\n","Iteration: 3012; Percent complete: 75.3%; Average loss: 2.7563\n","Iteration: 3013; Percent complete: 75.3%; Average loss: 2.8674\n","Iteration: 3014; Percent complete: 75.3%; Average loss: 2.8390\n","Iteration: 3015; Percent complete: 75.4%; Average loss: 2.8009\n","Iteration: 3016; Percent complete: 75.4%; Average loss: 2.8964\n","Iteration: 3017; Percent complete: 75.4%; Average loss: 2.8071\n","Iteration: 3018; Percent complete: 75.4%; Average loss: 2.8546\n","Iteration: 3019; Percent complete: 75.5%; Average loss: 2.8179\n","Iteration: 3020; Percent complete: 75.5%; Average loss: 2.7590\n","Iteration: 3021; Percent complete: 75.5%; Average loss: 2.7509\n","Iteration: 3022; Percent complete: 75.5%; Average loss: 2.7451\n","Iteration: 3023; Percent complete: 75.6%; Average loss: 2.9343\n","Iteration: 3024; Percent complete: 75.6%; Average loss: 2.7099\n","Iteration: 3025; Percent complete: 75.6%; Average loss: 3.0489\n","Iteration: 3026; Percent complete: 75.6%; Average loss: 2.6099\n","Iteration: 3027; Percent complete: 75.7%; Average loss: 2.7516\n","Iteration: 3028; Percent complete: 75.7%; Average loss: 3.1583\n","Iteration: 3029; Percent complete: 75.7%; Average loss: 2.8114\n","Iteration: 3030; Percent complete: 75.8%; Average loss: 2.6636\n","Iteration: 3031; Percent complete: 75.8%; Average loss: 2.9828\n","Iteration: 3032; Percent complete: 75.8%; Average loss: 2.9148\n","Iteration: 3033; Percent complete: 75.8%; Average loss: 2.7679\n","Iteration: 3034; Percent complete: 75.8%; Average loss: 2.9485\n","Iteration: 3035; Percent complete: 75.9%; Average loss: 2.7276\n","Iteration: 3036; Percent complete: 75.9%; Average loss: 2.8337\n","Iteration: 3037; Percent complete: 75.9%; Average loss: 2.9240\n","Iteration: 3038; Percent complete: 75.9%; Average loss: 3.0324\n","Iteration: 3039; Percent complete: 76.0%; Average loss: 3.0348\n","Iteration: 3040; Percent complete: 76.0%; Average loss: 2.8052\n","Iteration: 3041; Percent complete: 76.0%; Average loss: 2.8887\n","Iteration: 3042; Percent complete: 76.0%; Average loss: 2.9104\n","Iteration: 3043; Percent complete: 76.1%; Average loss: 2.5796\n","Iteration: 3044; Percent complete: 76.1%; Average loss: 2.7507\n","Iteration: 3045; Percent complete: 76.1%; Average loss: 2.9846\n","Iteration: 3046; Percent complete: 76.1%; Average loss: 2.9727\n","Iteration: 3047; Percent complete: 76.2%; Average loss: 2.8636\n","Iteration: 3048; Percent complete: 76.2%; Average loss: 2.8318\n","Iteration: 3049; Percent complete: 76.2%; Average loss: 2.7675\n","Iteration: 3050; Percent complete: 76.2%; Average loss: 2.9618\n","Iteration: 3051; Percent complete: 76.3%; Average loss: 3.0302\n","Iteration: 3052; Percent complete: 76.3%; Average loss: 2.9998\n","Iteration: 3053; Percent complete: 76.3%; Average loss: 2.6702\n","Iteration: 3054; Percent complete: 76.3%; Average loss: 2.9456\n","Iteration: 3055; Percent complete: 76.4%; Average loss: 2.8905\n","Iteration: 3056; Percent complete: 76.4%; Average loss: 2.9093\n","Iteration: 3057; Percent complete: 76.4%; Average loss: 2.5708\n","Iteration: 3058; Percent complete: 76.4%; Average loss: 2.9303\n","Iteration: 3059; Percent complete: 76.5%; Average loss: 2.8573\n","Iteration: 3060; Percent complete: 76.5%; Average loss: 3.0518\n","Iteration: 3061; Percent complete: 76.5%; Average loss: 2.8157\n","Iteration: 3062; Percent complete: 76.5%; Average loss: 2.8220\n","Iteration: 3063; Percent complete: 76.6%; Average loss: 2.8932\n","Iteration: 3064; Percent complete: 76.6%; Average loss: 2.8158\n","Iteration: 3065; Percent complete: 76.6%; Average loss: 2.6332\n","Iteration: 3066; Percent complete: 76.6%; Average loss: 2.7955\n","Iteration: 3067; Percent complete: 76.7%; Average loss: 2.7252\n","Iteration: 3068; Percent complete: 76.7%; Average loss: 2.9083\n","Iteration: 3069; Percent complete: 76.7%; Average loss: 2.7617\n","Iteration: 3070; Percent complete: 76.8%; Average loss: 3.0452\n","Iteration: 3071; Percent complete: 76.8%; Average loss: 2.8524\n","Iteration: 3072; Percent complete: 76.8%; Average loss: 2.7189\n","Iteration: 3073; Percent complete: 76.8%; Average loss: 3.0555\n","Iteration: 3074; Percent complete: 76.8%; Average loss: 3.0991\n","Iteration: 3075; Percent complete: 76.9%; Average loss: 2.8923\n","Iteration: 3076; Percent complete: 76.9%; Average loss: 2.7796\n","Iteration: 3077; Percent complete: 76.9%; Average loss: 2.9033\n","Iteration: 3078; Percent complete: 77.0%; Average loss: 2.9310\n","Iteration: 3079; Percent complete: 77.0%; Average loss: 2.8426\n","Iteration: 3080; Percent complete: 77.0%; Average loss: 2.9795\n","Iteration: 3081; Percent complete: 77.0%; Average loss: 3.0117\n","Iteration: 3082; Percent complete: 77.0%; Average loss: 2.9149\n","Iteration: 3083; Percent complete: 77.1%; Average loss: 3.0353\n","Iteration: 3084; Percent complete: 77.1%; Average loss: 2.8067\n","Iteration: 3085; Percent complete: 77.1%; Average loss: 3.0251\n","Iteration: 3086; Percent complete: 77.1%; Average loss: 2.8087\n","Iteration: 3087; Percent complete: 77.2%; Average loss: 2.8721\n","Iteration: 3088; Percent complete: 77.2%; Average loss: 2.7746\n","Iteration: 3089; Percent complete: 77.2%; Average loss: 2.8972\n","Iteration: 3090; Percent complete: 77.2%; Average loss: 2.9187\n","Iteration: 3091; Percent complete: 77.3%; Average loss: 2.8955\n","Iteration: 3092; Percent complete: 77.3%; Average loss: 2.5722\n","Iteration: 3093; Percent complete: 77.3%; Average loss: 3.0195\n","Iteration: 3094; Percent complete: 77.3%; Average loss: 3.1547\n","Iteration: 3095; Percent complete: 77.4%; Average loss: 2.7883\n","Iteration: 3096; Percent complete: 77.4%; Average loss: 2.5516\n","Iteration: 3097; Percent complete: 77.4%; Average loss: 2.9189\n","Iteration: 3098; Percent complete: 77.5%; Average loss: 2.8598\n","Iteration: 3099; Percent complete: 77.5%; Average loss: 2.5705\n","Iteration: 3100; Percent complete: 77.5%; Average loss: 2.6155\n","Iteration: 3101; Percent complete: 77.5%; Average loss: 2.5772\n","Iteration: 3102; Percent complete: 77.5%; Average loss: 3.0475\n","Iteration: 3103; Percent complete: 77.6%; Average loss: 2.8481\n","Iteration: 3104; Percent complete: 77.6%; Average loss: 3.0580\n","Iteration: 3105; Percent complete: 77.6%; Average loss: 2.7113\n","Iteration: 3106; Percent complete: 77.6%; Average loss: 2.8920\n","Iteration: 3107; Percent complete: 77.7%; Average loss: 2.8886\n","Iteration: 3108; Percent complete: 77.7%; Average loss: 2.8075\n","Iteration: 3109; Percent complete: 77.7%; Average loss: 2.6664\n","Iteration: 3110; Percent complete: 77.8%; Average loss: 2.9109\n","Iteration: 3111; Percent complete: 77.8%; Average loss: 2.7450\n","Iteration: 3112; Percent complete: 77.8%; Average loss: 2.7746\n","Iteration: 3113; Percent complete: 77.8%; Average loss: 2.9898\n","Iteration: 3114; Percent complete: 77.8%; Average loss: 2.7450\n","Iteration: 3115; Percent complete: 77.9%; Average loss: 2.7402\n","Iteration: 3116; Percent complete: 77.9%; Average loss: 2.9593\n","Iteration: 3117; Percent complete: 77.9%; Average loss: 2.8257\n","Iteration: 3118; Percent complete: 78.0%; Average loss: 2.8594\n","Iteration: 3119; Percent complete: 78.0%; Average loss: 2.8528\n","Iteration: 3120; Percent complete: 78.0%; Average loss: 2.6746\n","Iteration: 3121; Percent complete: 78.0%; Average loss: 2.9805\n","Iteration: 3122; Percent complete: 78.0%; Average loss: 2.9576\n","Iteration: 3123; Percent complete: 78.1%; Average loss: 2.6350\n","Iteration: 3124; Percent complete: 78.1%; Average loss: 2.8944\n","Iteration: 3125; Percent complete: 78.1%; Average loss: 2.7744\n","Iteration: 3126; Percent complete: 78.1%; Average loss: 2.8035\n","Iteration: 3127; Percent complete: 78.2%; Average loss: 2.8732\n","Iteration: 3128; Percent complete: 78.2%; Average loss: 2.8164\n","Iteration: 3129; Percent complete: 78.2%; Average loss: 2.7786\n","Iteration: 3130; Percent complete: 78.2%; Average loss: 3.2083\n","Iteration: 3131; Percent complete: 78.3%; Average loss: 2.7962\n","Iteration: 3132; Percent complete: 78.3%; Average loss: 2.9252\n","Iteration: 3133; Percent complete: 78.3%; Average loss: 2.8498\n","Iteration: 3134; Percent complete: 78.3%; Average loss: 2.7712\n","Iteration: 3135; Percent complete: 78.4%; Average loss: 2.7498\n","Iteration: 3136; Percent complete: 78.4%; Average loss: 3.0620\n","Iteration: 3137; Percent complete: 78.4%; Average loss: 2.8223\n","Iteration: 3138; Percent complete: 78.5%; Average loss: 2.8259\n","Iteration: 3139; Percent complete: 78.5%; Average loss: 2.9177\n","Iteration: 3140; Percent complete: 78.5%; Average loss: 3.2247\n","Iteration: 3141; Percent complete: 78.5%; Average loss: 2.9306\n","Iteration: 3142; Percent complete: 78.5%; Average loss: 3.0377\n","Iteration: 3143; Percent complete: 78.6%; Average loss: 3.0114\n","Iteration: 3144; Percent complete: 78.6%; Average loss: 2.8632\n","Iteration: 3145; Percent complete: 78.6%; Average loss: 2.8981\n","Iteration: 3146; Percent complete: 78.6%; Average loss: 2.8792\n","Iteration: 3147; Percent complete: 78.7%; Average loss: 3.2047\n","Iteration: 3148; Percent complete: 78.7%; Average loss: 2.8241\n","Iteration: 3149; Percent complete: 78.7%; Average loss: 2.8980\n","Iteration: 3150; Percent complete: 78.8%; Average loss: 2.6803\n","Iteration: 3151; Percent complete: 78.8%; Average loss: 2.8181\n","Iteration: 3152; Percent complete: 78.8%; Average loss: 2.9184\n","Iteration: 3153; Percent complete: 78.8%; Average loss: 2.6551\n","Iteration: 3154; Percent complete: 78.8%; Average loss: 2.9822\n","Iteration: 3155; Percent complete: 78.9%; Average loss: 2.9213\n","Iteration: 3156; Percent complete: 78.9%; Average loss: 2.9587\n","Iteration: 3157; Percent complete: 78.9%; Average loss: 2.8673\n","Iteration: 3158; Percent complete: 79.0%; Average loss: 2.7735\n","Iteration: 3159; Percent complete: 79.0%; Average loss: 2.8443\n","Iteration: 3160; Percent complete: 79.0%; Average loss: 2.8959\n","Iteration: 3161; Percent complete: 79.0%; Average loss: 2.7441\n","Iteration: 3162; Percent complete: 79.0%; Average loss: 3.0409\n","Iteration: 3163; Percent complete: 79.1%; Average loss: 2.7505\n","Iteration: 3164; Percent complete: 79.1%; Average loss: 2.8829\n","Iteration: 3165; Percent complete: 79.1%; Average loss: 3.0218\n","Iteration: 3166; Percent complete: 79.1%; Average loss: 2.9141\n","Iteration: 3167; Percent complete: 79.2%; Average loss: 2.9187\n","Iteration: 3168; Percent complete: 79.2%; Average loss: 2.8994\n","Iteration: 3169; Percent complete: 79.2%; Average loss: 2.9216\n","Iteration: 3170; Percent complete: 79.2%; Average loss: 3.0036\n","Iteration: 3171; Percent complete: 79.3%; Average loss: 2.7321\n","Iteration: 3172; Percent complete: 79.3%; Average loss: 2.7459\n","Iteration: 3173; Percent complete: 79.3%; Average loss: 2.7832\n","Iteration: 3174; Percent complete: 79.3%; Average loss: 2.7393\n","Iteration: 3175; Percent complete: 79.4%; Average loss: 2.7195\n","Iteration: 3176; Percent complete: 79.4%; Average loss: 2.7928\n","Iteration: 3177; Percent complete: 79.4%; Average loss: 2.9068\n","Iteration: 3178; Percent complete: 79.5%; Average loss: 2.9069\n","Iteration: 3179; Percent complete: 79.5%; Average loss: 2.8454\n","Iteration: 3180; Percent complete: 79.5%; Average loss: 2.6126\n","Iteration: 3181; Percent complete: 79.5%; Average loss: 2.6366\n","Iteration: 3182; Percent complete: 79.5%; Average loss: 3.0212\n","Iteration: 3183; Percent complete: 79.6%; Average loss: 2.7895\n","Iteration: 3184; Percent complete: 79.6%; Average loss: 3.0608\n","Iteration: 3185; Percent complete: 79.6%; Average loss: 2.9356\n","Iteration: 3186; Percent complete: 79.7%; Average loss: 2.5395\n","Iteration: 3187; Percent complete: 79.7%; Average loss: 2.6116\n","Iteration: 3188; Percent complete: 79.7%; Average loss: 2.7376\n","Iteration: 3189; Percent complete: 79.7%; Average loss: 3.0175\n","Iteration: 3190; Percent complete: 79.8%; Average loss: 2.8893\n","Iteration: 3191; Percent complete: 79.8%; Average loss: 2.5443\n","Iteration: 3192; Percent complete: 79.8%; Average loss: 2.7824\n","Iteration: 3193; Percent complete: 79.8%; Average loss: 2.8952\n","Iteration: 3194; Percent complete: 79.8%; Average loss: 2.8266\n","Iteration: 3195; Percent complete: 79.9%; Average loss: 2.5906\n","Iteration: 3196; Percent complete: 79.9%; Average loss: 2.8003\n","Iteration: 3197; Percent complete: 79.9%; Average loss: 2.7710\n","Iteration: 3198; Percent complete: 80.0%; Average loss: 2.7662\n","Iteration: 3199; Percent complete: 80.0%; Average loss: 2.7492\n","Iteration: 3200; Percent complete: 80.0%; Average loss: 3.0185\n","Iteration: 3201; Percent complete: 80.0%; Average loss: 2.8949\n","Iteration: 3202; Percent complete: 80.0%; Average loss: 2.8682\n","Iteration: 3203; Percent complete: 80.1%; Average loss: 3.0698\n","Iteration: 3204; Percent complete: 80.1%; Average loss: 2.8690\n","Iteration: 3205; Percent complete: 80.1%; Average loss: 2.8211\n","Iteration: 3206; Percent complete: 80.2%; Average loss: 2.8221\n","Iteration: 3207; Percent complete: 80.2%; Average loss: 2.9031\n","Iteration: 3208; Percent complete: 80.2%; Average loss: 2.7675\n","Iteration: 3209; Percent complete: 80.2%; Average loss: 2.7174\n","Iteration: 3210; Percent complete: 80.2%; Average loss: 2.7089\n","Iteration: 3211; Percent complete: 80.3%; Average loss: 2.9291\n","Iteration: 3212; Percent complete: 80.3%; Average loss: 2.7726\n","Iteration: 3213; Percent complete: 80.3%; Average loss: 2.9963\n","Iteration: 3214; Percent complete: 80.3%; Average loss: 2.8575\n","Iteration: 3215; Percent complete: 80.4%; Average loss: 2.7973\n","Iteration: 3216; Percent complete: 80.4%; Average loss: 2.5132\n","Iteration: 3217; Percent complete: 80.4%; Average loss: 2.7245\n","Iteration: 3218; Percent complete: 80.5%; Average loss: 3.0660\n","Iteration: 3219; Percent complete: 80.5%; Average loss: 2.6845\n","Iteration: 3220; Percent complete: 80.5%; Average loss: 2.7763\n","Iteration: 3221; Percent complete: 80.5%; Average loss: 2.7947\n","Iteration: 3222; Percent complete: 80.5%; Average loss: 3.0700\n","Iteration: 3223; Percent complete: 80.6%; Average loss: 2.6565\n","Iteration: 3224; Percent complete: 80.6%; Average loss: 2.8338\n","Iteration: 3225; Percent complete: 80.6%; Average loss: 3.0298\n","Iteration: 3226; Percent complete: 80.7%; Average loss: 2.9373\n","Iteration: 3227; Percent complete: 80.7%; Average loss: 2.8619\n","Iteration: 3228; Percent complete: 80.7%; Average loss: 2.8401\n","Iteration: 3229; Percent complete: 80.7%; Average loss: 2.7525\n","Iteration: 3230; Percent complete: 80.8%; Average loss: 2.7592\n","Iteration: 3231; Percent complete: 80.8%; Average loss: 2.8483\n","Iteration: 3232; Percent complete: 80.8%; Average loss: 2.8586\n","Iteration: 3233; Percent complete: 80.8%; Average loss: 2.8782\n","Iteration: 3234; Percent complete: 80.8%; Average loss: 2.8050\n","Iteration: 3235; Percent complete: 80.9%; Average loss: 2.7010\n","Iteration: 3236; Percent complete: 80.9%; Average loss: 2.6851\n","Iteration: 3237; Percent complete: 80.9%; Average loss: 2.7459\n","Iteration: 3238; Percent complete: 81.0%; Average loss: 3.1282\n","Iteration: 3239; Percent complete: 81.0%; Average loss: 2.8586\n","Iteration: 3240; Percent complete: 81.0%; Average loss: 3.0323\n","Iteration: 3241; Percent complete: 81.0%; Average loss: 2.7643\n","Iteration: 3242; Percent complete: 81.0%; Average loss: 2.5583\n","Iteration: 3243; Percent complete: 81.1%; Average loss: 2.9195\n","Iteration: 3244; Percent complete: 81.1%; Average loss: 3.0392\n","Iteration: 3245; Percent complete: 81.1%; Average loss: 2.8248\n","Iteration: 3246; Percent complete: 81.2%; Average loss: 2.8474\n","Iteration: 3247; Percent complete: 81.2%; Average loss: 2.7212\n","Iteration: 3248; Percent complete: 81.2%; Average loss: 2.7513\n","Iteration: 3249; Percent complete: 81.2%; Average loss: 2.6135\n","Iteration: 3250; Percent complete: 81.2%; Average loss: 2.6170\n","Iteration: 3251; Percent complete: 81.3%; Average loss: 2.9964\n","Iteration: 3252; Percent complete: 81.3%; Average loss: 2.6406\n","Iteration: 3253; Percent complete: 81.3%; Average loss: 2.9957\n","Iteration: 3254; Percent complete: 81.3%; Average loss: 2.8895\n","Iteration: 3255; Percent complete: 81.4%; Average loss: 2.9159\n","Iteration: 3256; Percent complete: 81.4%; Average loss: 2.8166\n","Iteration: 3257; Percent complete: 81.4%; Average loss: 2.9979\n","Iteration: 3258; Percent complete: 81.5%; Average loss: 2.6527\n","Iteration: 3259; Percent complete: 81.5%; Average loss: 2.8918\n","Iteration: 3260; Percent complete: 81.5%; Average loss: 2.7584\n","Iteration: 3261; Percent complete: 81.5%; Average loss: 2.8635\n","Iteration: 3262; Percent complete: 81.5%; Average loss: 2.8702\n","Iteration: 3263; Percent complete: 81.6%; Average loss: 2.7139\n","Iteration: 3264; Percent complete: 81.6%; Average loss: 2.7764\n","Iteration: 3265; Percent complete: 81.6%; Average loss: 2.7228\n","Iteration: 3266; Percent complete: 81.7%; Average loss: 2.9242\n","Iteration: 3267; Percent complete: 81.7%; Average loss: 3.0013\n","Iteration: 3268; Percent complete: 81.7%; Average loss: 2.9569\n","Iteration: 3269; Percent complete: 81.7%; Average loss: 3.1165\n","Iteration: 3270; Percent complete: 81.8%; Average loss: 2.6213\n","Iteration: 3271; Percent complete: 81.8%; Average loss: 2.8032\n","Iteration: 3272; Percent complete: 81.8%; Average loss: 2.7705\n","Iteration: 3273; Percent complete: 81.8%; Average loss: 2.5527\n","Iteration: 3274; Percent complete: 81.8%; Average loss: 2.9763\n","Iteration: 3275; Percent complete: 81.9%; Average loss: 2.8749\n","Iteration: 3276; Percent complete: 81.9%; Average loss: 2.6411\n","Iteration: 3277; Percent complete: 81.9%; Average loss: 2.9778\n","Iteration: 3278; Percent complete: 82.0%; Average loss: 2.9464\n","Iteration: 3279; Percent complete: 82.0%; Average loss: 2.8714\n","Iteration: 3280; Percent complete: 82.0%; Average loss: 2.9059\n","Iteration: 3281; Percent complete: 82.0%; Average loss: 2.8524\n","Iteration: 3282; Percent complete: 82.0%; Average loss: 2.7161\n","Iteration: 3283; Percent complete: 82.1%; Average loss: 2.8846\n","Iteration: 3284; Percent complete: 82.1%; Average loss: 2.7798\n","Iteration: 3285; Percent complete: 82.1%; Average loss: 2.8044\n","Iteration: 3286; Percent complete: 82.2%; Average loss: 2.8333\n","Iteration: 3287; Percent complete: 82.2%; Average loss: 3.0333\n","Iteration: 3288; Percent complete: 82.2%; Average loss: 2.8318\n","Iteration: 3289; Percent complete: 82.2%; Average loss: 2.8647\n","Iteration: 3290; Percent complete: 82.2%; Average loss: 2.9295\n","Iteration: 3291; Percent complete: 82.3%; Average loss: 3.1218\n","Iteration: 3292; Percent complete: 82.3%; Average loss: 3.0348\n","Iteration: 3293; Percent complete: 82.3%; Average loss: 2.9540\n","Iteration: 3294; Percent complete: 82.3%; Average loss: 2.8949\n","Iteration: 3295; Percent complete: 82.4%; Average loss: 2.9152\n","Iteration: 3296; Percent complete: 82.4%; Average loss: 2.9505\n","Iteration: 3297; Percent complete: 82.4%; Average loss: 2.5834\n","Iteration: 3298; Percent complete: 82.5%; Average loss: 2.7212\n","Iteration: 3299; Percent complete: 82.5%; Average loss: 2.8309\n","Iteration: 3300; Percent complete: 82.5%; Average loss: 2.5609\n","Iteration: 3301; Percent complete: 82.5%; Average loss: 2.7956\n","Iteration: 3302; Percent complete: 82.5%; Average loss: 2.7904\n","Iteration: 3303; Percent complete: 82.6%; Average loss: 2.8467\n","Iteration: 3304; Percent complete: 82.6%; Average loss: 2.5918\n","Iteration: 3305; Percent complete: 82.6%; Average loss: 3.0985\n","Iteration: 3306; Percent complete: 82.7%; Average loss: 2.8737\n","Iteration: 3307; Percent complete: 82.7%; Average loss: 2.9437\n","Iteration: 3308; Percent complete: 82.7%; Average loss: 2.7538\n","Iteration: 3309; Percent complete: 82.7%; Average loss: 2.7736\n","Iteration: 3310; Percent complete: 82.8%; Average loss: 2.8911\n","Iteration: 3311; Percent complete: 82.8%; Average loss: 2.8603\n","Iteration: 3312; Percent complete: 82.8%; Average loss: 3.0237\n","Iteration: 3313; Percent complete: 82.8%; Average loss: 2.6852\n","Iteration: 3314; Percent complete: 82.8%; Average loss: 2.7717\n","Iteration: 3315; Percent complete: 82.9%; Average loss: 2.6780\n","Iteration: 3316; Percent complete: 82.9%; Average loss: 2.6028\n","Iteration: 3317; Percent complete: 82.9%; Average loss: 2.6634\n","Iteration: 3318; Percent complete: 83.0%; Average loss: 2.9670\n","Iteration: 3319; Percent complete: 83.0%; Average loss: 3.0991\n","Iteration: 3320; Percent complete: 83.0%; Average loss: 2.7574\n","Iteration: 3321; Percent complete: 83.0%; Average loss: 2.7254\n","Iteration: 3322; Percent complete: 83.0%; Average loss: 2.8342\n","Iteration: 3323; Percent complete: 83.1%; Average loss: 2.7257\n","Iteration: 3324; Percent complete: 83.1%; Average loss: 2.8625\n","Iteration: 3325; Percent complete: 83.1%; Average loss: 2.7021\n","Iteration: 3326; Percent complete: 83.2%; Average loss: 2.7326\n","Iteration: 3327; Percent complete: 83.2%; Average loss: 2.8548\n","Iteration: 3328; Percent complete: 83.2%; Average loss: 3.1208\n","Iteration: 3329; Percent complete: 83.2%; Average loss: 2.7882\n","Iteration: 3330; Percent complete: 83.2%; Average loss: 2.6551\n","Iteration: 3331; Percent complete: 83.3%; Average loss: 2.8607\n","Iteration: 3332; Percent complete: 83.3%; Average loss: 2.7024\n","Iteration: 3333; Percent complete: 83.3%; Average loss: 2.9027\n","Iteration: 3334; Percent complete: 83.4%; Average loss: 2.9085\n","Iteration: 3335; Percent complete: 83.4%; Average loss: 2.7671\n","Iteration: 3336; Percent complete: 83.4%; Average loss: 2.7357\n","Iteration: 3337; Percent complete: 83.4%; Average loss: 2.8896\n","Iteration: 3338; Percent complete: 83.5%; Average loss: 2.8893\n","Iteration: 3339; Percent complete: 83.5%; Average loss: 2.6441\n","Iteration: 3340; Percent complete: 83.5%; Average loss: 2.7735\n","Iteration: 3341; Percent complete: 83.5%; Average loss: 3.0078\n","Iteration: 3342; Percent complete: 83.5%; Average loss: 2.8769\n","Iteration: 3343; Percent complete: 83.6%; Average loss: 2.8571\n","Iteration: 3344; Percent complete: 83.6%; Average loss: 2.8424\n","Iteration: 3345; Percent complete: 83.6%; Average loss: 2.7506\n","Iteration: 3346; Percent complete: 83.7%; Average loss: 2.6440\n","Iteration: 3347; Percent complete: 83.7%; Average loss: 2.8354\n","Iteration: 3348; Percent complete: 83.7%; Average loss: 2.7777\n","Iteration: 3349; Percent complete: 83.7%; Average loss: 2.8361\n","Iteration: 3350; Percent complete: 83.8%; Average loss: 2.7929\n","Iteration: 3351; Percent complete: 83.8%; Average loss: 2.5729\n","Iteration: 3352; Percent complete: 83.8%; Average loss: 2.9700\n","Iteration: 3353; Percent complete: 83.8%; Average loss: 2.6119\n","Iteration: 3354; Percent complete: 83.9%; Average loss: 3.0064\n","Iteration: 3355; Percent complete: 83.9%; Average loss: 2.7974\n","Iteration: 3356; Percent complete: 83.9%; Average loss: 2.5673\n","Iteration: 3357; Percent complete: 83.9%; Average loss: 3.1415\n","Iteration: 3358; Percent complete: 84.0%; Average loss: 2.7267\n","Iteration: 3359; Percent complete: 84.0%; Average loss: 2.7575\n","Iteration: 3360; Percent complete: 84.0%; Average loss: 2.6594\n","Iteration: 3361; Percent complete: 84.0%; Average loss: 2.6928\n","Iteration: 3362; Percent complete: 84.0%; Average loss: 2.5891\n","Iteration: 3363; Percent complete: 84.1%; Average loss: 2.7488\n","Iteration: 3364; Percent complete: 84.1%; Average loss: 2.7121\n","Iteration: 3365; Percent complete: 84.1%; Average loss: 2.6673\n","Iteration: 3366; Percent complete: 84.2%; Average loss: 2.8689\n","Iteration: 3367; Percent complete: 84.2%; Average loss: 2.7188\n","Iteration: 3368; Percent complete: 84.2%; Average loss: 2.7196\n","Iteration: 3369; Percent complete: 84.2%; Average loss: 2.7211\n","Iteration: 3370; Percent complete: 84.2%; Average loss: 2.6624\n","Iteration: 3371; Percent complete: 84.3%; Average loss: 2.8164\n","Iteration: 3372; Percent complete: 84.3%; Average loss: 2.9444\n","Iteration: 3373; Percent complete: 84.3%; Average loss: 2.4951\n","Iteration: 3374; Percent complete: 84.4%; Average loss: 3.0010\n","Iteration: 3375; Percent complete: 84.4%; Average loss: 2.5442\n","Iteration: 3376; Percent complete: 84.4%; Average loss: 2.8349\n","Iteration: 3377; Percent complete: 84.4%; Average loss: 2.6276\n","Iteration: 3378; Percent complete: 84.5%; Average loss: 2.7823\n","Iteration: 3379; Percent complete: 84.5%; Average loss: 2.6401\n","Iteration: 3380; Percent complete: 84.5%; Average loss: 2.7697\n","Iteration: 3381; Percent complete: 84.5%; Average loss: 2.8112\n","Iteration: 3382; Percent complete: 84.5%; Average loss: 2.6280\n","Iteration: 3383; Percent complete: 84.6%; Average loss: 2.8835\n","Iteration: 3384; Percent complete: 84.6%; Average loss: 2.7927\n","Iteration: 3385; Percent complete: 84.6%; Average loss: 2.7398\n","Iteration: 3386; Percent complete: 84.7%; Average loss: 2.8575\n","Iteration: 3387; Percent complete: 84.7%; Average loss: 2.6707\n","Iteration: 3388; Percent complete: 84.7%; Average loss: 2.7413\n","Iteration: 3389; Percent complete: 84.7%; Average loss: 3.0837\n","Iteration: 3390; Percent complete: 84.8%; Average loss: 2.5767\n","Iteration: 3391; Percent complete: 84.8%; Average loss: 3.1012\n","Iteration: 3392; Percent complete: 84.8%; Average loss: 2.7987\n","Iteration: 3393; Percent complete: 84.8%; Average loss: 2.7213\n","Iteration: 3394; Percent complete: 84.9%; Average loss: 2.8480\n","Iteration: 3395; Percent complete: 84.9%; Average loss: 2.7062\n","Iteration: 3396; Percent complete: 84.9%; Average loss: 2.7015\n","Iteration: 3397; Percent complete: 84.9%; Average loss: 2.5969\n","Iteration: 3398; Percent complete: 85.0%; Average loss: 2.7975\n","Iteration: 3399; Percent complete: 85.0%; Average loss: 2.8903\n","Iteration: 3400; Percent complete: 85.0%; Average loss: 2.4392\n","Iteration: 3401; Percent complete: 85.0%; Average loss: 2.6778\n","Iteration: 3402; Percent complete: 85.0%; Average loss: 2.8168\n","Iteration: 3403; Percent complete: 85.1%; Average loss: 2.8626\n","Iteration: 3404; Percent complete: 85.1%; Average loss: 2.7888\n","Iteration: 3405; Percent complete: 85.1%; Average loss: 2.8067\n","Iteration: 3406; Percent complete: 85.2%; Average loss: 2.6001\n","Iteration: 3407; Percent complete: 85.2%; Average loss: 2.8009\n","Iteration: 3408; Percent complete: 85.2%; Average loss: 2.7099\n","Iteration: 3409; Percent complete: 85.2%; Average loss: 2.9238\n","Iteration: 3410; Percent complete: 85.2%; Average loss: 2.7664\n","Iteration: 3411; Percent complete: 85.3%; Average loss: 2.8836\n","Iteration: 3412; Percent complete: 85.3%; Average loss: 2.7511\n","Iteration: 3413; Percent complete: 85.3%; Average loss: 2.8005\n","Iteration: 3414; Percent complete: 85.4%; Average loss: 2.7717\n","Iteration: 3415; Percent complete: 85.4%; Average loss: 2.6970\n","Iteration: 3416; Percent complete: 85.4%; Average loss: 2.8639\n","Iteration: 3417; Percent complete: 85.4%; Average loss: 2.6898\n","Iteration: 3418; Percent complete: 85.5%; Average loss: 3.0879\n","Iteration: 3419; Percent complete: 85.5%; Average loss: 2.6982\n","Iteration: 3420; Percent complete: 85.5%; Average loss: 2.7517\n","Iteration: 3421; Percent complete: 85.5%; Average loss: 2.9118\n","Iteration: 3422; Percent complete: 85.5%; Average loss: 2.8431\n","Iteration: 3423; Percent complete: 85.6%; Average loss: 2.6015\n","Iteration: 3424; Percent complete: 85.6%; Average loss: 3.0014\n","Iteration: 3425; Percent complete: 85.6%; Average loss: 2.5882\n","Iteration: 3426; Percent complete: 85.7%; Average loss: 2.9089\n","Iteration: 3427; Percent complete: 85.7%; Average loss: 2.6181\n","Iteration: 3428; Percent complete: 85.7%; Average loss: 2.9721\n","Iteration: 3429; Percent complete: 85.7%; Average loss: 2.7091\n","Iteration: 3430; Percent complete: 85.8%; Average loss: 2.6947\n","Iteration: 3431; Percent complete: 85.8%; Average loss: 2.6217\n","Iteration: 3432; Percent complete: 85.8%; Average loss: 2.7206\n","Iteration: 3433; Percent complete: 85.8%; Average loss: 2.4887\n","Iteration: 3434; Percent complete: 85.9%; Average loss: 2.7779\n","Iteration: 3435; Percent complete: 85.9%; Average loss: 2.7617\n","Iteration: 3436; Percent complete: 85.9%; Average loss: 2.7061\n","Iteration: 3437; Percent complete: 85.9%; Average loss: 2.6569\n","Iteration: 3438; Percent complete: 86.0%; Average loss: 2.9703\n","Iteration: 3439; Percent complete: 86.0%; Average loss: 2.9264\n","Iteration: 3440; Percent complete: 86.0%; Average loss: 2.5963\n","Iteration: 3441; Percent complete: 86.0%; Average loss: 2.9488\n","Iteration: 3442; Percent complete: 86.1%; Average loss: 2.8998\n","Iteration: 3443; Percent complete: 86.1%; Average loss: 2.7642\n","Iteration: 3444; Percent complete: 86.1%; Average loss: 2.8513\n","Iteration: 3445; Percent complete: 86.1%; Average loss: 2.7607\n","Iteration: 3446; Percent complete: 86.2%; Average loss: 2.7088\n","Iteration: 3447; Percent complete: 86.2%; Average loss: 2.8583\n","Iteration: 3448; Percent complete: 86.2%; Average loss: 2.6041\n","Iteration: 3449; Percent complete: 86.2%; Average loss: 2.8061\n","Iteration: 3450; Percent complete: 86.2%; Average loss: 2.5506\n","Iteration: 3451; Percent complete: 86.3%; Average loss: 2.7411\n","Iteration: 3452; Percent complete: 86.3%; Average loss: 2.8813\n","Iteration: 3453; Percent complete: 86.3%; Average loss: 2.7889\n","Iteration: 3454; Percent complete: 86.4%; Average loss: 2.7328\n","Iteration: 3455; Percent complete: 86.4%; Average loss: 3.0038\n","Iteration: 3456; Percent complete: 86.4%; Average loss: 2.8471\n","Iteration: 3457; Percent complete: 86.4%; Average loss: 2.5410\n","Iteration: 3458; Percent complete: 86.5%; Average loss: 2.8030\n","Iteration: 3459; Percent complete: 86.5%; Average loss: 2.8446\n","Iteration: 3460; Percent complete: 86.5%; Average loss: 2.7160\n","Iteration: 3461; Percent complete: 86.5%; Average loss: 3.0324\n","Iteration: 3462; Percent complete: 86.6%; Average loss: 2.5417\n","Iteration: 3463; Percent complete: 86.6%; Average loss: 2.9422\n","Iteration: 3464; Percent complete: 86.6%; Average loss: 2.7685\n","Iteration: 3465; Percent complete: 86.6%; Average loss: 2.8035\n","Iteration: 3466; Percent complete: 86.7%; Average loss: 2.7549\n","Iteration: 3467; Percent complete: 86.7%; Average loss: 2.7893\n","Iteration: 3468; Percent complete: 86.7%; Average loss: 2.7555\n","Iteration: 3469; Percent complete: 86.7%; Average loss: 2.6870\n","Iteration: 3470; Percent complete: 86.8%; Average loss: 2.7490\n","Iteration: 3471; Percent complete: 86.8%; Average loss: 2.7497\n","Iteration: 3472; Percent complete: 86.8%; Average loss: 2.7800\n","Iteration: 3473; Percent complete: 86.8%; Average loss: 2.6128\n","Iteration: 3474; Percent complete: 86.9%; Average loss: 2.8411\n","Iteration: 3475; Percent complete: 86.9%; Average loss: 2.7842\n","Iteration: 3476; Percent complete: 86.9%; Average loss: 2.7515\n","Iteration: 3477; Percent complete: 86.9%; Average loss: 2.9838\n","Iteration: 3478; Percent complete: 87.0%; Average loss: 2.7398\n","Iteration: 3479; Percent complete: 87.0%; Average loss: 2.4800\n","Iteration: 3480; Percent complete: 87.0%; Average loss: 2.5071\n","Iteration: 3481; Percent complete: 87.0%; Average loss: 2.6302\n","Iteration: 3482; Percent complete: 87.1%; Average loss: 2.3920\n","Iteration: 3483; Percent complete: 87.1%; Average loss: 2.5100\n","Iteration: 3484; Percent complete: 87.1%; Average loss: 2.9615\n","Iteration: 3485; Percent complete: 87.1%; Average loss: 2.6715\n","Iteration: 3486; Percent complete: 87.2%; Average loss: 2.9318\n","Iteration: 3487; Percent complete: 87.2%; Average loss: 2.6530\n","Iteration: 3488; Percent complete: 87.2%; Average loss: 2.7048\n","Iteration: 3489; Percent complete: 87.2%; Average loss: 2.8452\n","Iteration: 3490; Percent complete: 87.2%; Average loss: 2.8029\n","Iteration: 3491; Percent complete: 87.3%; Average loss: 2.4795\n","Iteration: 3492; Percent complete: 87.3%; Average loss: 2.7239\n","Iteration: 3493; Percent complete: 87.3%; Average loss: 2.7713\n","Iteration: 3494; Percent complete: 87.4%; Average loss: 2.7418\n","Iteration: 3495; Percent complete: 87.4%; Average loss: 2.8463\n","Iteration: 3496; Percent complete: 87.4%; Average loss: 2.5519\n","Iteration: 3497; Percent complete: 87.4%; Average loss: 2.6324\n","Iteration: 3498; Percent complete: 87.5%; Average loss: 2.8859\n","Iteration: 3499; Percent complete: 87.5%; Average loss: 2.8286\n","Iteration: 3500; Percent complete: 87.5%; Average loss: 2.6890\n","Iteration: 3501; Percent complete: 87.5%; Average loss: 2.8536\n","Iteration: 3502; Percent complete: 87.5%; Average loss: 2.5661\n","Iteration: 3503; Percent complete: 87.6%; Average loss: 3.0931\n","Iteration: 3504; Percent complete: 87.6%; Average loss: 2.7734\n","Iteration: 3505; Percent complete: 87.6%; Average loss: 2.7271\n","Iteration: 3506; Percent complete: 87.6%; Average loss: 2.6646\n","Iteration: 3507; Percent complete: 87.7%; Average loss: 2.8048\n","Iteration: 3508; Percent complete: 87.7%; Average loss: 2.8413\n","Iteration: 3509; Percent complete: 87.7%; Average loss: 2.5540\n","Iteration: 3510; Percent complete: 87.8%; Average loss: 2.6394\n","Iteration: 3511; Percent complete: 87.8%; Average loss: 2.8381\n","Iteration: 3512; Percent complete: 87.8%; Average loss: 2.5600\n","Iteration: 3513; Percent complete: 87.8%; Average loss: 2.6484\n","Iteration: 3514; Percent complete: 87.8%; Average loss: 2.7581\n","Iteration: 3515; Percent complete: 87.9%; Average loss: 2.9775\n","Iteration: 3516; Percent complete: 87.9%; Average loss: 2.8276\n","Iteration: 3517; Percent complete: 87.9%; Average loss: 2.7889\n","Iteration: 3518; Percent complete: 87.9%; Average loss: 2.5396\n","Iteration: 3519; Percent complete: 88.0%; Average loss: 2.8440\n","Iteration: 3520; Percent complete: 88.0%; Average loss: 2.7585\n","Iteration: 3521; Percent complete: 88.0%; Average loss: 2.6849\n","Iteration: 3522; Percent complete: 88.0%; Average loss: 2.7654\n","Iteration: 3523; Percent complete: 88.1%; Average loss: 2.7691\n","Iteration: 3524; Percent complete: 88.1%; Average loss: 2.7489\n","Iteration: 3525; Percent complete: 88.1%; Average loss: 2.7226\n","Iteration: 3526; Percent complete: 88.1%; Average loss: 2.8438\n","Iteration: 3527; Percent complete: 88.2%; Average loss: 2.5736\n","Iteration: 3528; Percent complete: 88.2%; Average loss: 2.6895\n","Iteration: 3529; Percent complete: 88.2%; Average loss: 2.8896\n","Iteration: 3530; Percent complete: 88.2%; Average loss: 2.5487\n","Iteration: 3531; Percent complete: 88.3%; Average loss: 2.6942\n","Iteration: 3532; Percent complete: 88.3%; Average loss: 2.8454\n","Iteration: 3533; Percent complete: 88.3%; Average loss: 2.6835\n","Iteration: 3534; Percent complete: 88.3%; Average loss: 2.7557\n","Iteration: 3535; Percent complete: 88.4%; Average loss: 2.7178\n","Iteration: 3536; Percent complete: 88.4%; Average loss: 2.5861\n","Iteration: 3537; Percent complete: 88.4%; Average loss: 2.4027\n","Iteration: 3538; Percent complete: 88.4%; Average loss: 2.6073\n","Iteration: 3539; Percent complete: 88.5%; Average loss: 2.6739\n","Iteration: 3540; Percent complete: 88.5%; Average loss: 2.9753\n","Iteration: 3541; Percent complete: 88.5%; Average loss: 2.8636\n","Iteration: 3542; Percent complete: 88.5%; Average loss: 2.8133\n","Iteration: 3543; Percent complete: 88.6%; Average loss: 2.8939\n","Iteration: 3544; Percent complete: 88.6%; Average loss: 2.5703\n","Iteration: 3545; Percent complete: 88.6%; Average loss: 2.9346\n","Iteration: 3546; Percent complete: 88.6%; Average loss: 2.7432\n","Iteration: 3547; Percent complete: 88.7%; Average loss: 2.4453\n","Iteration: 3548; Percent complete: 88.7%; Average loss: 2.7724\n","Iteration: 3549; Percent complete: 88.7%; Average loss: 2.7425\n","Iteration: 3550; Percent complete: 88.8%; Average loss: 2.8257\n","Iteration: 3551; Percent complete: 88.8%; Average loss: 2.6997\n","Iteration: 3552; Percent complete: 88.8%; Average loss: 2.9181\n","Iteration: 3553; Percent complete: 88.8%; Average loss: 2.6667\n","Iteration: 3554; Percent complete: 88.8%; Average loss: 2.6355\n","Iteration: 3555; Percent complete: 88.9%; Average loss: 2.9634\n","Iteration: 3556; Percent complete: 88.9%; Average loss: 3.0833\n","Iteration: 3557; Percent complete: 88.9%; Average loss: 2.7134\n","Iteration: 3558; Percent complete: 88.9%; Average loss: 2.6294\n","Iteration: 3559; Percent complete: 89.0%; Average loss: 2.8990\n","Iteration: 3560; Percent complete: 89.0%; Average loss: 2.6652\n","Iteration: 3561; Percent complete: 89.0%; Average loss: 2.6531\n","Iteration: 3562; Percent complete: 89.0%; Average loss: 2.7725\n","Iteration: 3563; Percent complete: 89.1%; Average loss: 2.7744\n","Iteration: 3564; Percent complete: 89.1%; Average loss: 2.7065\n","Iteration: 3565; Percent complete: 89.1%; Average loss: 2.7436\n","Iteration: 3566; Percent complete: 89.1%; Average loss: 2.7800\n","Iteration: 3567; Percent complete: 89.2%; Average loss: 2.8260\n","Iteration: 3568; Percent complete: 89.2%; Average loss: 2.6048\n","Iteration: 3569; Percent complete: 89.2%; Average loss: 2.6887\n","Iteration: 3570; Percent complete: 89.2%; Average loss: 2.7954\n","Iteration: 3571; Percent complete: 89.3%; Average loss: 2.6993\n","Iteration: 3572; Percent complete: 89.3%; Average loss: 2.7300\n","Iteration: 3573; Percent complete: 89.3%; Average loss: 2.6683\n","Iteration: 3574; Percent complete: 89.3%; Average loss: 2.7311\n","Iteration: 3575; Percent complete: 89.4%; Average loss: 2.6892\n","Iteration: 3576; Percent complete: 89.4%; Average loss: 2.5132\n","Iteration: 3577; Percent complete: 89.4%; Average loss: 2.7003\n","Iteration: 3578; Percent complete: 89.5%; Average loss: 2.8072\n","Iteration: 3579; Percent complete: 89.5%; Average loss: 2.7571\n","Iteration: 3580; Percent complete: 89.5%; Average loss: 2.5983\n","Iteration: 3581; Percent complete: 89.5%; Average loss: 2.5700\n","Iteration: 3582; Percent complete: 89.5%; Average loss: 2.6139\n","Iteration: 3583; Percent complete: 89.6%; Average loss: 2.7427\n","Iteration: 3584; Percent complete: 89.6%; Average loss: 2.7250\n","Iteration: 3585; Percent complete: 89.6%; Average loss: 2.5314\n","Iteration: 3586; Percent complete: 89.6%; Average loss: 2.6482\n","Iteration: 3587; Percent complete: 89.7%; Average loss: 2.7682\n","Iteration: 3588; Percent complete: 89.7%; Average loss: 2.7449\n","Iteration: 3589; Percent complete: 89.7%; Average loss: 2.5546\n","Iteration: 3590; Percent complete: 89.8%; Average loss: 2.5628\n","Iteration: 3591; Percent complete: 89.8%; Average loss: 2.7883\n","Iteration: 3592; Percent complete: 89.8%; Average loss: 2.7242\n","Iteration: 3593; Percent complete: 89.8%; Average loss: 2.7379\n","Iteration: 3594; Percent complete: 89.8%; Average loss: 2.6885\n","Iteration: 3595; Percent complete: 89.9%; Average loss: 2.7438\n","Iteration: 3596; Percent complete: 89.9%; Average loss: 2.7052\n","Iteration: 3597; Percent complete: 89.9%; Average loss: 2.5927\n","Iteration: 3598; Percent complete: 90.0%; Average loss: 2.8088\n","Iteration: 3599; Percent complete: 90.0%; Average loss: 2.5227\n","Iteration: 3600; Percent complete: 90.0%; Average loss: 2.7440\n","Iteration: 3601; Percent complete: 90.0%; Average loss: 2.7180\n","Iteration: 3602; Percent complete: 90.0%; Average loss: 2.8074\n","Iteration: 3603; Percent complete: 90.1%; Average loss: 2.4979\n","Iteration: 3604; Percent complete: 90.1%; Average loss: 2.6088\n","Iteration: 3605; Percent complete: 90.1%; Average loss: 2.4904\n","Iteration: 3606; Percent complete: 90.1%; Average loss: 2.5353\n","Iteration: 3607; Percent complete: 90.2%; Average loss: 2.7888\n","Iteration: 3608; Percent complete: 90.2%; Average loss: 2.8283\n","Iteration: 3609; Percent complete: 90.2%; Average loss: 2.6162\n","Iteration: 3610; Percent complete: 90.2%; Average loss: 2.8303\n","Iteration: 3611; Percent complete: 90.3%; Average loss: 2.7998\n","Iteration: 3612; Percent complete: 90.3%; Average loss: 2.5591\n","Iteration: 3613; Percent complete: 90.3%; Average loss: 2.9135\n","Iteration: 3614; Percent complete: 90.3%; Average loss: 2.9356\n","Iteration: 3615; Percent complete: 90.4%; Average loss: 2.6244\n","Iteration: 3616; Percent complete: 90.4%; Average loss: 2.5284\n","Iteration: 3617; Percent complete: 90.4%; Average loss: 2.6163\n","Iteration: 3618; Percent complete: 90.5%; Average loss: 3.0272\n","Iteration: 3619; Percent complete: 90.5%; Average loss: 2.9524\n","Iteration: 3620; Percent complete: 90.5%; Average loss: 2.6513\n","Iteration: 3621; Percent complete: 90.5%; Average loss: 2.8025\n","Iteration: 3622; Percent complete: 90.5%; Average loss: 2.8018\n","Iteration: 3623; Percent complete: 90.6%; Average loss: 2.8279\n","Iteration: 3624; Percent complete: 90.6%; Average loss: 2.7518\n","Iteration: 3625; Percent complete: 90.6%; Average loss: 2.5620\n","Iteration: 3626; Percent complete: 90.6%; Average loss: 2.6157\n","Iteration: 3627; Percent complete: 90.7%; Average loss: 2.7248\n","Iteration: 3628; Percent complete: 90.7%; Average loss: 2.6372\n","Iteration: 3629; Percent complete: 90.7%; Average loss: 2.6612\n","Iteration: 3630; Percent complete: 90.8%; Average loss: 2.5112\n","Iteration: 3631; Percent complete: 90.8%; Average loss: 2.7509\n","Iteration: 3632; Percent complete: 90.8%; Average loss: 2.4107\n","Iteration: 3633; Percent complete: 90.8%; Average loss: 3.1101\n","Iteration: 3634; Percent complete: 90.8%; Average loss: 2.7302\n","Iteration: 3635; Percent complete: 90.9%; Average loss: 3.0567\n","Iteration: 3636; Percent complete: 90.9%; Average loss: 2.7469\n","Iteration: 3637; Percent complete: 90.9%; Average loss: 2.6302\n","Iteration: 3638; Percent complete: 91.0%; Average loss: 2.7999\n","Iteration: 3639; Percent complete: 91.0%; Average loss: 2.4880\n","Iteration: 3640; Percent complete: 91.0%; Average loss: 2.9067\n","Iteration: 3641; Percent complete: 91.0%; Average loss: 2.7419\n","Iteration: 3642; Percent complete: 91.0%; Average loss: 2.7758\n","Iteration: 3643; Percent complete: 91.1%; Average loss: 2.8646\n","Iteration: 3644; Percent complete: 91.1%; Average loss: 2.6836\n","Iteration: 3645; Percent complete: 91.1%; Average loss: 2.8516\n","Iteration: 3646; Percent complete: 91.1%; Average loss: 2.6654\n","Iteration: 3647; Percent complete: 91.2%; Average loss: 2.7262\n","Iteration: 3648; Percent complete: 91.2%; Average loss: 2.4940\n","Iteration: 3649; Percent complete: 91.2%; Average loss: 2.5670\n","Iteration: 3650; Percent complete: 91.2%; Average loss: 2.8029\n","Iteration: 3651; Percent complete: 91.3%; Average loss: 2.5845\n","Iteration: 3652; Percent complete: 91.3%; Average loss: 2.6953\n","Iteration: 3653; Percent complete: 91.3%; Average loss: 2.6973\n","Iteration: 3654; Percent complete: 91.3%; Average loss: 2.6898\n","Iteration: 3655; Percent complete: 91.4%; Average loss: 2.7537\n","Iteration: 3656; Percent complete: 91.4%; Average loss: 2.7885\n","Iteration: 3657; Percent complete: 91.4%; Average loss: 2.7388\n","Iteration: 3658; Percent complete: 91.5%; Average loss: 2.7701\n","Iteration: 3659; Percent complete: 91.5%; Average loss: 2.6232\n","Iteration: 3660; Percent complete: 91.5%; Average loss: 2.7985\n","Iteration: 3661; Percent complete: 91.5%; Average loss: 2.6097\n","Iteration: 3662; Percent complete: 91.5%; Average loss: 2.5707\n","Iteration: 3663; Percent complete: 91.6%; Average loss: 2.9114\n","Iteration: 3664; Percent complete: 91.6%; Average loss: 2.6586\n","Iteration: 3665; Percent complete: 91.6%; Average loss: 2.9197\n","Iteration: 3666; Percent complete: 91.6%; Average loss: 2.8488\n","Iteration: 3667; Percent complete: 91.7%; Average loss: 2.6566\n","Iteration: 3668; Percent complete: 91.7%; Average loss: 2.5245\n","Iteration: 3669; Percent complete: 91.7%; Average loss: 2.7063\n","Iteration: 3670; Percent complete: 91.8%; Average loss: 2.6876\n","Iteration: 3671; Percent complete: 91.8%; Average loss: 2.8102\n","Iteration: 3672; Percent complete: 91.8%; Average loss: 2.7445\n","Iteration: 3673; Percent complete: 91.8%; Average loss: 2.5706\n","Iteration: 3674; Percent complete: 91.8%; Average loss: 2.6744\n","Iteration: 3675; Percent complete: 91.9%; Average loss: 2.6352\n","Iteration: 3676; Percent complete: 91.9%; Average loss: 2.8258\n","Iteration: 3677; Percent complete: 91.9%; Average loss: 2.7009\n","Iteration: 3678; Percent complete: 92.0%; Average loss: 2.6423\n","Iteration: 3679; Percent complete: 92.0%; Average loss: 2.6394\n","Iteration: 3680; Percent complete: 92.0%; Average loss: 2.8554\n","Iteration: 3681; Percent complete: 92.0%; Average loss: 2.8380\n","Iteration: 3682; Percent complete: 92.0%; Average loss: 2.7491\n","Iteration: 3683; Percent complete: 92.1%; Average loss: 2.6597\n","Iteration: 3684; Percent complete: 92.1%; Average loss: 2.6997\n","Iteration: 3685; Percent complete: 92.1%; Average loss: 2.8030\n","Iteration: 3686; Percent complete: 92.2%; Average loss: 2.6292\n","Iteration: 3687; Percent complete: 92.2%; Average loss: 2.6423\n","Iteration: 3688; Percent complete: 92.2%; Average loss: 2.7838\n","Iteration: 3689; Percent complete: 92.2%; Average loss: 2.5863\n","Iteration: 3690; Percent complete: 92.2%; Average loss: 2.6416\n","Iteration: 3691; Percent complete: 92.3%; Average loss: 2.6987\n","Iteration: 3692; Percent complete: 92.3%; Average loss: 2.5530\n","Iteration: 3693; Percent complete: 92.3%; Average loss: 2.7430\n","Iteration: 3694; Percent complete: 92.3%; Average loss: 2.7129\n","Iteration: 3695; Percent complete: 92.4%; Average loss: 2.8054\n","Iteration: 3696; Percent complete: 92.4%; Average loss: 2.7303\n","Iteration: 3697; Percent complete: 92.4%; Average loss: 2.6291\n","Iteration: 3698; Percent complete: 92.5%; Average loss: 2.9055\n","Iteration: 3699; Percent complete: 92.5%; Average loss: 2.5334\n","Iteration: 3700; Percent complete: 92.5%; Average loss: 2.8540\n","Iteration: 3701; Percent complete: 92.5%; Average loss: 2.9145\n","Iteration: 3702; Percent complete: 92.5%; Average loss: 2.8634\n","Iteration: 3703; Percent complete: 92.6%; Average loss: 2.6903\n","Iteration: 3704; Percent complete: 92.6%; Average loss: 2.5922\n","Iteration: 3705; Percent complete: 92.6%; Average loss: 2.6472\n","Iteration: 3706; Percent complete: 92.7%; Average loss: 2.7798\n","Iteration: 3707; Percent complete: 92.7%; Average loss: 2.6700\n","Iteration: 3708; Percent complete: 92.7%; Average loss: 2.6797\n","Iteration: 3709; Percent complete: 92.7%; Average loss: 2.5088\n","Iteration: 3710; Percent complete: 92.8%; Average loss: 2.6247\n","Iteration: 3711; Percent complete: 92.8%; Average loss: 2.6857\n","Iteration: 3712; Percent complete: 92.8%; Average loss: 2.7180\n","Iteration: 3713; Percent complete: 92.8%; Average loss: 2.7692\n","Iteration: 3714; Percent complete: 92.8%; Average loss: 2.8202\n","Iteration: 3715; Percent complete: 92.9%; Average loss: 2.6595\n","Iteration: 3716; Percent complete: 92.9%; Average loss: 2.7302\n","Iteration: 3717; Percent complete: 92.9%; Average loss: 2.5657\n","Iteration: 3718; Percent complete: 93.0%; Average loss: 2.3582\n","Iteration: 3719; Percent complete: 93.0%; Average loss: 2.8415\n","Iteration: 3720; Percent complete: 93.0%; Average loss: 2.6617\n","Iteration: 3721; Percent complete: 93.0%; Average loss: 2.4921\n","Iteration: 3722; Percent complete: 93.0%; Average loss: 2.8183\n","Iteration: 3723; Percent complete: 93.1%; Average loss: 2.5711\n","Iteration: 3724; Percent complete: 93.1%; Average loss: 2.8626\n","Iteration: 3725; Percent complete: 93.1%; Average loss: 2.5653\n","Iteration: 3726; Percent complete: 93.2%; Average loss: 2.5217\n","Iteration: 3727; Percent complete: 93.2%; Average loss: 2.6495\n","Iteration: 3728; Percent complete: 93.2%; Average loss: 2.7264\n","Iteration: 3729; Percent complete: 93.2%; Average loss: 2.5829\n","Iteration: 3730; Percent complete: 93.2%; Average loss: 2.7161\n","Iteration: 3731; Percent complete: 93.3%; Average loss: 2.6146\n","Iteration: 3732; Percent complete: 93.3%; Average loss: 2.6784\n","Iteration: 3733; Percent complete: 93.3%; Average loss: 2.4301\n","Iteration: 3734; Percent complete: 93.3%; Average loss: 2.7322\n","Iteration: 3735; Percent complete: 93.4%; Average loss: 2.7287\n","Iteration: 3736; Percent complete: 93.4%; Average loss: 2.5718\n","Iteration: 3737; Percent complete: 93.4%; Average loss: 2.9788\n","Iteration: 3738; Percent complete: 93.5%; Average loss: 2.5804\n","Iteration: 3739; Percent complete: 93.5%; Average loss: 2.7058\n","Iteration: 3740; Percent complete: 93.5%; Average loss: 2.6111\n","Iteration: 3741; Percent complete: 93.5%; Average loss: 2.6528\n","Iteration: 3742; Percent complete: 93.5%; Average loss: 2.7228\n","Iteration: 3743; Percent complete: 93.6%; Average loss: 2.6115\n","Iteration: 3744; Percent complete: 93.6%; Average loss: 2.5462\n","Iteration: 3745; Percent complete: 93.6%; Average loss: 2.4581\n","Iteration: 3746; Percent complete: 93.7%; Average loss: 2.7189\n","Iteration: 3747; Percent complete: 93.7%; Average loss: 2.4052\n","Iteration: 3748; Percent complete: 93.7%; Average loss: 3.0105\n","Iteration: 3749; Percent complete: 93.7%; Average loss: 2.7405\n","Iteration: 3750; Percent complete: 93.8%; Average loss: 3.0505\n","Iteration: 3751; Percent complete: 93.8%; Average loss: 2.9108\n","Iteration: 3752; Percent complete: 93.8%; Average loss: 2.8368\n","Iteration: 3753; Percent complete: 93.8%; Average loss: 2.6234\n","Iteration: 3754; Percent complete: 93.8%; Average loss: 2.8419\n","Iteration: 3755; Percent complete: 93.9%; Average loss: 2.7835\n","Iteration: 3756; Percent complete: 93.9%; Average loss: 2.6872\n","Iteration: 3757; Percent complete: 93.9%; Average loss: 2.5912\n","Iteration: 3758; Percent complete: 94.0%; Average loss: 2.6078\n","Iteration: 3759; Percent complete: 94.0%; Average loss: 2.4902\n","Iteration: 3760; Percent complete: 94.0%; Average loss: 2.5000\n","Iteration: 3761; Percent complete: 94.0%; Average loss: 2.5404\n","Iteration: 3762; Percent complete: 94.0%; Average loss: 2.5103\n","Iteration: 3763; Percent complete: 94.1%; Average loss: 2.8164\n","Iteration: 3764; Percent complete: 94.1%; Average loss: 2.7557\n","Iteration: 3765; Percent complete: 94.1%; Average loss: 2.7540\n","Iteration: 3766; Percent complete: 94.2%; Average loss: 2.6202\n","Iteration: 3767; Percent complete: 94.2%; Average loss: 2.3642\n","Iteration: 3768; Percent complete: 94.2%; Average loss: 2.8044\n","Iteration: 3769; Percent complete: 94.2%; Average loss: 2.4751\n","Iteration: 3770; Percent complete: 94.2%; Average loss: 2.6425\n","Iteration: 3771; Percent complete: 94.3%; Average loss: 2.7379\n","Iteration: 3772; Percent complete: 94.3%; Average loss: 2.6958\n","Iteration: 3773; Percent complete: 94.3%; Average loss: 2.4869\n","Iteration: 3774; Percent complete: 94.3%; Average loss: 2.5511\n","Iteration: 3775; Percent complete: 94.4%; Average loss: 2.9561\n","Iteration: 3776; Percent complete: 94.4%; Average loss: 2.8045\n","Iteration: 3777; Percent complete: 94.4%; Average loss: 2.7178\n","Iteration: 3778; Percent complete: 94.5%; Average loss: 2.6578\n","Iteration: 3779; Percent complete: 94.5%; Average loss: 2.7748\n","Iteration: 3780; Percent complete: 94.5%; Average loss: 2.6588\n","Iteration: 3781; Percent complete: 94.5%; Average loss: 2.7530\n","Iteration: 3782; Percent complete: 94.5%; Average loss: 2.6518\n","Iteration: 3783; Percent complete: 94.6%; Average loss: 2.8608\n","Iteration: 3784; Percent complete: 94.6%; Average loss: 2.8856\n","Iteration: 3785; Percent complete: 94.6%; Average loss: 2.9011\n","Iteration: 3786; Percent complete: 94.7%; Average loss: 2.5300\n","Iteration: 3787; Percent complete: 94.7%; Average loss: 2.7898\n","Iteration: 3788; Percent complete: 94.7%; Average loss: 2.6724\n","Iteration: 3789; Percent complete: 94.7%; Average loss: 2.8102\n","Iteration: 3790; Percent complete: 94.8%; Average loss: 2.7743\n","Iteration: 3791; Percent complete: 94.8%; Average loss: 2.6223\n","Iteration: 3792; Percent complete: 94.8%; Average loss: 2.6212\n","Iteration: 3793; Percent complete: 94.8%; Average loss: 2.6316\n","Iteration: 3794; Percent complete: 94.8%; Average loss: 2.6621\n","Iteration: 3795; Percent complete: 94.9%; Average loss: 2.5516\n","Iteration: 3796; Percent complete: 94.9%; Average loss: 2.9068\n","Iteration: 3797; Percent complete: 94.9%; Average loss: 2.7831\n","Iteration: 3798; Percent complete: 95.0%; Average loss: 2.6339\n","Iteration: 3799; Percent complete: 95.0%; Average loss: 2.7152\n","Iteration: 3800; Percent complete: 95.0%; Average loss: 2.6826\n","Iteration: 3801; Percent complete: 95.0%; Average loss: 2.8022\n","Iteration: 3802; Percent complete: 95.0%; Average loss: 2.8290\n","Iteration: 3803; Percent complete: 95.1%; Average loss: 2.6679\n","Iteration: 3804; Percent complete: 95.1%; Average loss: 2.5555\n","Iteration: 3805; Percent complete: 95.1%; Average loss: 2.7073\n","Iteration: 3806; Percent complete: 95.2%; Average loss: 2.7795\n","Iteration: 3807; Percent complete: 95.2%; Average loss: 2.7643\n","Iteration: 3808; Percent complete: 95.2%; Average loss: 2.7390\n","Iteration: 3809; Percent complete: 95.2%; Average loss: 2.7354\n","Iteration: 3810; Percent complete: 95.2%; Average loss: 2.7340\n","Iteration: 3811; Percent complete: 95.3%; Average loss: 2.5738\n","Iteration: 3812; Percent complete: 95.3%; Average loss: 2.7610\n","Iteration: 3813; Percent complete: 95.3%; Average loss: 2.6762\n","Iteration: 3814; Percent complete: 95.3%; Average loss: 2.6258\n","Iteration: 3815; Percent complete: 95.4%; Average loss: 2.6816\n","Iteration: 3816; Percent complete: 95.4%; Average loss: 2.7164\n","Iteration: 3817; Percent complete: 95.4%; Average loss: 2.5617\n","Iteration: 3818; Percent complete: 95.5%; Average loss: 2.6400\n","Iteration: 3819; Percent complete: 95.5%; Average loss: 2.7571\n","Iteration: 3820; Percent complete: 95.5%; Average loss: 2.6589\n","Iteration: 3821; Percent complete: 95.5%; Average loss: 2.6477\n","Iteration: 3822; Percent complete: 95.5%; Average loss: 2.7589\n","Iteration: 3823; Percent complete: 95.6%; Average loss: 2.7986\n","Iteration: 3824; Percent complete: 95.6%; Average loss: 2.4581\n","Iteration: 3825; Percent complete: 95.6%; Average loss: 2.6407\n","Iteration: 3826; Percent complete: 95.7%; Average loss: 2.5345\n","Iteration: 3827; Percent complete: 95.7%; Average loss: 2.6914\n","Iteration: 3828; Percent complete: 95.7%; Average loss: 2.3852\n","Iteration: 3829; Percent complete: 95.7%; Average loss: 2.6563\n","Iteration: 3830; Percent complete: 95.8%; Average loss: 2.5536\n","Iteration: 3831; Percent complete: 95.8%; Average loss: 2.3282\n","Iteration: 3832; Percent complete: 95.8%; Average loss: 2.5302\n","Iteration: 3833; Percent complete: 95.8%; Average loss: 2.7210\n","Iteration: 3834; Percent complete: 95.9%; Average loss: 2.5132\n","Iteration: 3835; Percent complete: 95.9%; Average loss: 2.4638\n","Iteration: 3836; Percent complete: 95.9%; Average loss: 2.6989\n","Iteration: 3837; Percent complete: 95.9%; Average loss: 2.4366\n","Iteration: 3838; Percent complete: 96.0%; Average loss: 2.4530\n","Iteration: 3839; Percent complete: 96.0%; Average loss: 2.7406\n","Iteration: 3840; Percent complete: 96.0%; Average loss: 2.6262\n","Iteration: 3841; Percent complete: 96.0%; Average loss: 2.7750\n","Iteration: 3842; Percent complete: 96.0%; Average loss: 2.8052\n","Iteration: 3843; Percent complete: 96.1%; Average loss: 2.7837\n","Iteration: 3844; Percent complete: 96.1%; Average loss: 2.5782\n","Iteration: 3845; Percent complete: 96.1%; Average loss: 2.5426\n","Iteration: 3846; Percent complete: 96.2%; Average loss: 2.4560\n","Iteration: 3847; Percent complete: 96.2%; Average loss: 2.5542\n","Iteration: 3848; Percent complete: 96.2%; Average loss: 2.8804\n","Iteration: 3849; Percent complete: 96.2%; Average loss: 2.7457\n","Iteration: 3850; Percent complete: 96.2%; Average loss: 2.6769\n","Iteration: 3851; Percent complete: 96.3%; Average loss: 2.5558\n","Iteration: 3852; Percent complete: 96.3%; Average loss: 2.8662\n","Iteration: 3853; Percent complete: 96.3%; Average loss: 2.5310\n","Iteration: 3854; Percent complete: 96.4%; Average loss: 2.5929\n","Iteration: 3855; Percent complete: 96.4%; Average loss: 2.5514\n","Iteration: 3856; Percent complete: 96.4%; Average loss: 2.7519\n","Iteration: 3857; Percent complete: 96.4%; Average loss: 2.4313\n","Iteration: 3858; Percent complete: 96.5%; Average loss: 2.8344\n","Iteration: 3859; Percent complete: 96.5%; Average loss: 2.5770\n","Iteration: 3860; Percent complete: 96.5%; Average loss: 2.6924\n","Iteration: 3861; Percent complete: 96.5%; Average loss: 2.5581\n","Iteration: 3862; Percent complete: 96.5%; Average loss: 2.6845\n","Iteration: 3863; Percent complete: 96.6%; Average loss: 2.6329\n","Iteration: 3864; Percent complete: 96.6%; Average loss: 2.5853\n","Iteration: 3865; Percent complete: 96.6%; Average loss: 2.6420\n","Iteration: 3866; Percent complete: 96.7%; Average loss: 2.7671\n","Iteration: 3867; Percent complete: 96.7%; Average loss: 2.6067\n","Iteration: 3868; Percent complete: 96.7%; Average loss: 2.7407\n","Iteration: 3869; Percent complete: 96.7%; Average loss: 2.8304\n","Iteration: 3870; Percent complete: 96.8%; Average loss: 2.6607\n","Iteration: 3871; Percent complete: 96.8%; Average loss: 2.7225\n","Iteration: 3872; Percent complete: 96.8%; Average loss: 2.6920\n","Iteration: 3873; Percent complete: 96.8%; Average loss: 2.5377\n","Iteration: 3874; Percent complete: 96.9%; Average loss: 2.6006\n","Iteration: 3875; Percent complete: 96.9%; Average loss: 2.5291\n","Iteration: 3876; Percent complete: 96.9%; Average loss: 2.5959\n","Iteration: 3877; Percent complete: 96.9%; Average loss: 2.7665\n","Iteration: 3878; Percent complete: 97.0%; Average loss: 2.9462\n","Iteration: 3879; Percent complete: 97.0%; Average loss: 2.7532\n","Iteration: 3880; Percent complete: 97.0%; Average loss: 2.6526\n","Iteration: 3881; Percent complete: 97.0%; Average loss: 2.9390\n","Iteration: 3882; Percent complete: 97.0%; Average loss: 2.7278\n","Iteration: 3883; Percent complete: 97.1%; Average loss: 2.7252\n","Iteration: 3884; Percent complete: 97.1%; Average loss: 2.6373\n","Iteration: 3885; Percent complete: 97.1%; Average loss: 2.7086\n","Iteration: 3886; Percent complete: 97.2%; Average loss: 2.6001\n","Iteration: 3887; Percent complete: 97.2%; Average loss: 2.3930\n","Iteration: 3888; Percent complete: 97.2%; Average loss: 2.5950\n","Iteration: 3889; Percent complete: 97.2%; Average loss: 2.5970\n","Iteration: 3890; Percent complete: 97.2%; Average loss: 2.9269\n","Iteration: 3891; Percent complete: 97.3%; Average loss: 2.7235\n","Iteration: 3892; Percent complete: 97.3%; Average loss: 2.8146\n","Iteration: 3893; Percent complete: 97.3%; Average loss: 2.6205\n","Iteration: 3894; Percent complete: 97.4%; Average loss: 2.7128\n","Iteration: 3895; Percent complete: 97.4%; Average loss: 2.9435\n","Iteration: 3896; Percent complete: 97.4%; Average loss: 2.7187\n","Iteration: 3897; Percent complete: 97.4%; Average loss: 2.5918\n","Iteration: 3898; Percent complete: 97.5%; Average loss: 2.7549\n","Iteration: 3899; Percent complete: 97.5%; Average loss: 2.7263\n","Iteration: 3900; Percent complete: 97.5%; Average loss: 2.6230\n","Iteration: 3901; Percent complete: 97.5%; Average loss: 2.6958\n","Iteration: 3902; Percent complete: 97.5%; Average loss: 2.4993\n","Iteration: 3903; Percent complete: 97.6%; Average loss: 2.5530\n","Iteration: 3904; Percent complete: 97.6%; Average loss: 2.6294\n","Iteration: 3905; Percent complete: 97.6%; Average loss: 2.7021\n","Iteration: 3906; Percent complete: 97.7%; Average loss: 2.7444\n","Iteration: 3907; Percent complete: 97.7%; Average loss: 2.5858\n","Iteration: 3908; Percent complete: 97.7%; Average loss: 2.6046\n","Iteration: 3909; Percent complete: 97.7%; Average loss: 2.4865\n","Iteration: 3910; Percent complete: 97.8%; Average loss: 2.8063\n","Iteration: 3911; Percent complete: 97.8%; Average loss: 2.6302\n","Iteration: 3912; Percent complete: 97.8%; Average loss: 2.4194\n","Iteration: 3913; Percent complete: 97.8%; Average loss: 2.7260\n","Iteration: 3914; Percent complete: 97.9%; Average loss: 2.8107\n","Iteration: 3915; Percent complete: 97.9%; Average loss: 2.4184\n","Iteration: 3916; Percent complete: 97.9%; Average loss: 2.5056\n","Iteration: 3917; Percent complete: 97.9%; Average loss: 2.7187\n","Iteration: 3918; Percent complete: 98.0%; Average loss: 2.7032\n","Iteration: 3919; Percent complete: 98.0%; Average loss: 2.4210\n","Iteration: 3920; Percent complete: 98.0%; Average loss: 2.4900\n","Iteration: 3921; Percent complete: 98.0%; Average loss: 2.3807\n","Iteration: 3922; Percent complete: 98.0%; Average loss: 2.5232\n","Iteration: 3923; Percent complete: 98.1%; Average loss: 2.8847\n","Iteration: 3924; Percent complete: 98.1%; Average loss: 2.6389\n","Iteration: 3925; Percent complete: 98.1%; Average loss: 2.6933\n","Iteration: 3926; Percent complete: 98.2%; Average loss: 2.6093\n","Iteration: 3927; Percent complete: 98.2%; Average loss: 2.6796\n","Iteration: 3928; Percent complete: 98.2%; Average loss: 2.5894\n","Iteration: 3929; Percent complete: 98.2%; Average loss: 2.7468\n","Iteration: 3930; Percent complete: 98.2%; Average loss: 2.6974\n","Iteration: 3931; Percent complete: 98.3%; Average loss: 2.7879\n","Iteration: 3932; Percent complete: 98.3%; Average loss: 2.6025\n","Iteration: 3933; Percent complete: 98.3%; Average loss: 2.6741\n","Iteration: 3934; Percent complete: 98.4%; Average loss: 2.4882\n","Iteration: 3935; Percent complete: 98.4%; Average loss: 2.7683\n","Iteration: 3936; Percent complete: 98.4%; Average loss: 2.7660\n","Iteration: 3937; Percent complete: 98.4%; Average loss: 2.6877\n","Iteration: 3938; Percent complete: 98.5%; Average loss: 2.5510\n","Iteration: 3939; Percent complete: 98.5%; Average loss: 2.6563\n","Iteration: 3940; Percent complete: 98.5%; Average loss: 2.3319\n","Iteration: 3941; Percent complete: 98.5%; Average loss: 2.4566\n","Iteration: 3942; Percent complete: 98.6%; Average loss: 2.7055\n","Iteration: 3943; Percent complete: 98.6%; Average loss: 2.2388\n","Iteration: 3944; Percent complete: 98.6%; Average loss: 2.4024\n","Iteration: 3945; Percent complete: 98.6%; Average loss: 2.6416\n","Iteration: 3946; Percent complete: 98.7%; Average loss: 2.4965\n","Iteration: 3947; Percent complete: 98.7%; Average loss: 2.3177\n","Iteration: 3948; Percent complete: 98.7%; Average loss: 2.7398\n","Iteration: 3949; Percent complete: 98.7%; Average loss: 2.8704\n","Iteration: 3950; Percent complete: 98.8%; Average loss: 2.5305\n","Iteration: 3951; Percent complete: 98.8%; Average loss: 2.5160\n","Iteration: 3952; Percent complete: 98.8%; Average loss: 2.5147\n","Iteration: 3953; Percent complete: 98.8%; Average loss: 2.7737\n","Iteration: 3954; Percent complete: 98.9%; Average loss: 2.6978\n","Iteration: 3955; Percent complete: 98.9%; Average loss: 2.5516\n","Iteration: 3956; Percent complete: 98.9%; Average loss: 2.6590\n","Iteration: 3957; Percent complete: 98.9%; Average loss: 2.5429\n","Iteration: 3958; Percent complete: 99.0%; Average loss: 2.4602\n","Iteration: 3959; Percent complete: 99.0%; Average loss: 2.6978\n","Iteration: 3960; Percent complete: 99.0%; Average loss: 2.8485\n","Iteration: 3961; Percent complete: 99.0%; Average loss: 2.5316\n","Iteration: 3962; Percent complete: 99.1%; Average loss: 2.8547\n","Iteration: 3963; Percent complete: 99.1%; Average loss: 2.8175\n","Iteration: 3964; Percent complete: 99.1%; Average loss: 2.5576\n","Iteration: 3965; Percent complete: 99.1%; Average loss: 2.8527\n","Iteration: 3966; Percent complete: 99.2%; Average loss: 2.4410\n","Iteration: 3967; Percent complete: 99.2%; Average loss: 2.4694\n","Iteration: 3968; Percent complete: 99.2%; Average loss: 2.3086\n","Iteration: 3969; Percent complete: 99.2%; Average loss: 2.5812\n","Iteration: 3970; Percent complete: 99.2%; Average loss: 2.5271\n","Iteration: 3971; Percent complete: 99.3%; Average loss: 2.8846\n","Iteration: 3972; Percent complete: 99.3%; Average loss: 2.6860\n","Iteration: 3973; Percent complete: 99.3%; Average loss: 2.6586\n","Iteration: 3974; Percent complete: 99.4%; Average loss: 2.7977\n","Iteration: 3975; Percent complete: 99.4%; Average loss: 2.7217\n","Iteration: 3976; Percent complete: 99.4%; Average loss: 2.5198\n","Iteration: 3977; Percent complete: 99.4%; Average loss: 2.5765\n","Iteration: 3978; Percent complete: 99.5%; Average loss: 2.6110\n","Iteration: 3979; Percent complete: 99.5%; Average loss: 2.8455\n","Iteration: 3980; Percent complete: 99.5%; Average loss: 2.6600\n","Iteration: 3981; Percent complete: 99.5%; Average loss: 2.5068\n","Iteration: 3982; Percent complete: 99.6%; Average loss: 2.8169\n","Iteration: 3983; Percent complete: 99.6%; Average loss: 2.4689\n","Iteration: 3984; Percent complete: 99.6%; Average loss: 2.5696\n","Iteration: 3985; Percent complete: 99.6%; Average loss: 2.5294\n","Iteration: 3986; Percent complete: 99.7%; Average loss: 2.7056\n","Iteration: 3987; Percent complete: 99.7%; Average loss: 2.7148\n","Iteration: 3988; Percent complete: 99.7%; Average loss: 2.7626\n","Iteration: 3989; Percent complete: 99.7%; Average loss: 2.6867\n","Iteration: 3990; Percent complete: 99.8%; Average loss: 2.5453\n","Iteration: 3991; Percent complete: 99.8%; Average loss: 2.5991\n","Iteration: 3992; Percent complete: 99.8%; Average loss: 2.7127\n","Iteration: 3993; Percent complete: 99.8%; Average loss: 2.7597\n","Iteration: 3994; Percent complete: 99.9%; Average loss: 2.7170\n","Iteration: 3995; Percent complete: 99.9%; Average loss: 2.3631\n","Iteration: 3996; Percent complete: 99.9%; Average loss: 2.7382\n","Iteration: 3997; Percent complete: 99.9%; Average loss: 2.6142\n","Iteration: 3998; Percent complete: 100.0%; Average loss: 2.5942\n","Iteration: 3999; Percent complete: 100.0%; Average loss: 2.5004\n","Iteration: 4000; Percent complete: 100.0%; Average loss: 2.8483\n"]}]},{"cell_type":"markdown","source":["\n","##### Explanation of Key Parts:\n","- **Training Parameters**:\n","  - **`clip`**: Controls the gradient clipping threshold, helping to prevent exploding gradients in RNNs.\n","  - **`teacher_forcing_ratio`**: Sets the probability of using the true target word as input at each decoding step. A ratio of `1.0` means teacher forcing is always used, which can help with faster convergence initially.\n","  - **`learning_rate`**: Specifies the base learning rate for the encoder optimizer.\n","  - **`decoder_learning_ratio`**: Multiplies the decoder’s learning rate to allow the decoder to learn faster than the encoder, which can be helpful in certain sequence-to-sequence tasks.\n","  - **`n_iteration`**: Total number of training iterations to perform.\n","  - **`print_every` and `save_every`**: Set the intervals for printing training progress and saving checkpoints, respectively.\n","\n","- **Optimizers**:\n","  - **`encoder_optimizer` and `decoder_optimizer`**: Use the Adam optimizer, a commonly used optimizer in deep learning, with separate learning rates for the encoder and decoder. The decoder’s learning rate is set higher by multiplying it by `decoder_learning_ratio`.\n","\n","- **Starting the Training Loop**:\n","  - **`trainIters`**: This function runs the main training loop defined previously, handling batch loading, training iterations, and saving checkpoints. The function uses the configured parameters and passes in necessary arguments, including model names, vocabulary, encoder/decoder models, optimizers, embedding, and directories.\n"],"metadata":{"id":"VOCMIpMeT57J"}},{"cell_type":"markdown","source":["### **Section 4: Evaluation and Inference**\n","\n","After training, this section defines methods for using the model to generate responses. We implement a greedy decoding method for inference, evaluate the model's response to sample inputs, and set up an interactive chat function.\n","\n"],"metadata":{"id":"U-wrsBP4VnXB"}},{"cell_type":"markdown","source":["load the model trained"],"metadata":{"id":"OJD-qSILT19v"}},{"cell_type":"code","source":["\n","import torch\n","import os\n","\n","# Load the final trained model checkpoint\n","checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/nlp_pro_babu/checkpoints/cb_model/movie-corpus/2-2_500/4000_checkpoint.tar\"\n","checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","\n","# Load encoder and decoder states\n","encoder.load_state_dict(checkpoint['en'])\n","decoder.load_state_dict(checkpoint['de'])\n","encoder.eval()\n","decoder.eval()\n","\n","print(\"Checkpoint loaded successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36oDX20bT4AV","executionInfo":{"status":"ok","timestamp":1731464426158,"user_tz":0,"elapsed":1254,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"b6f80c31-e8b3-4222-c10d-8c56690fb0f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-20-e55c5941c055>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n"]},{"output_type":"stream","name":"stdout","text":["Checkpoint loaded successfully!\n"]}]},{"cell_type":"markdown","source":["#### **4.1 Greedy Decoding**\n","\n","- **Objective**: Use a greedy decoding strategy to generate a response, choosing the word with the highest probability at each decoding step.\n","- **Explanation**:\n","  - Greedy decoding selects the most probable word at each step based on the decoder's output.\n","  - The process continues iteratively until either an `EOS` (end-of-sequence) token is generated or a maximum response length is reached.\n","  - This simple approach can be effective, but it doesn’t explore alternative sequences (like beam search would), which can sometimes lead to less diverse responses.\n","\n"],"metadata":{"id":"X1jc3Bl-VnTm"}},{"cell_type":"code","source":["\n","class GreedySearchDecoder(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(GreedySearchDecoder, self).__init__()\n","        self.encoder = encoder  # Encoder model\n","        self.decoder = decoder  # Decoder model\n","\n","    def forward(self, input_seq, input_length, max_length):\n","        \"\"\"\n","        Perform greedy decoding to generate a response.\n","        Args:\n","            input_seq: The input sequence (encoded as word indices).\n","            input_length: Length of the input sequence.\n","            max_length: Maximum length of the generated response.\n","        Returns:\n","            all_tokens: Tensor of word tokens generated in the response.\n","            all_scores: Tensor of scores for each generated token.\n","        \"\"\"\n","        # Forward input through encoder to get encoder outputs and hidden state\n","        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n","\n","        # Use encoder's final hidden state as the initial hidden state for the decoder\n","        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n","\n","        # Initialize the decoder input with the SOS token\n","        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n","\n","        # Initialize tensors to store generated tokens and their scores\n","        all_tokens = torch.zeros([0], device=device, dtype=torch.long)  # Stores generated word indices\n","        all_scores = torch.zeros([0], device=device)  # Stores scores of generated words\n","\n","        # Iteratively generate tokens one at a time up to max_length\n","        for _ in range(max_length):\n","            # Forward pass through the decoder with the current input and hidden state\n","            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n","\n","            # Select the word with the highest score (greedy selection)\n","            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n","\n","            # Append the selected token and score to the results\n","            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n","            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n","\n","            # Prepare the selected token as the next input (reshape for batch compatibility)\n","            decoder_input = torch.unsqueeze(decoder_input, 0)\n","\n","        # Return all generated tokens and their corresponding scores\n","        return all_tokens, all_scores\n"],"metadata":{"id":"V7EvcbDMZUCG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **4.2 Evaluation Function**\n","\n","- **Objective**: Define a function that takes an input sentence, processes it into the required format, and generates a response using the encoder-decoder model with a greedy decoding strategy.\n","- **Explanation**:\n","  - The function formats the input sentence as a batch of word indices compatible with the model’s requirements.\n","  - It then uses the `GreedySearchDecoder` to decode the response token by token.\n","  - Finally, the function converts the decoded word indices back into words, so the response can be read in a human-readable format.\n","\n"],"metadata":{"id":"Y322pb2zVnQ3"}},{"cell_type":"code","source":["def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n","    \"\"\"\n","    Generate a response for an input sentence using the encoder-decoder model.\n","\n","    Args:\n","        encoder (nn.Module): The trained encoder model.\n","        decoder (nn.Module): The trained decoder model.\n","        searcher (GreedySearchDecoder): The searcher object implementing greedy decoding.\n","        voc (Voc): Vocabulary object containing word-to-index mappings.\n","        sentence (str): The input sentence for which a response is to be generated.\n","        max_length (int): Maximum length of the generated response.\n","\n","    Returns:\n","        decoded_words (list of str): The generated response as a list of words.\n","    \"\"\"\n","    # This function tokenizes the input sentence, maps each word to its index in the vocabulary,\n","    # and appends an EOS token at the end.\n","    indexes_batch = [indexesFromSentence(voc, sentence)]  # List of indices for the sentence\n","\n","    # This tensor is used to indicate the length of each sentence in the batch. Here, the batch\n","    # size is 1 (we are only processing one sentence at a time during evaluation).\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])  # Sentence length as a tensor\n","\n","    # The `LongTensor` tensor represents the sentence as a sequence of indices, where each index\n","    # corresponds to a word in the vocabulary. We transpose the tensor to match the model's input\n","    # shape requirements (seq_len, batch_size).\n","    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)  # Shape: (seq_len, 1)\n","\n","    # The searcher uses the encoder to process the input sentence and then iteratively decodes\n","    # each token in the response sequence using the decoder.\n","    tokens, scores = searcher(input_batch, lengths, max_length)  # Decoding output tokens and scores\n","\n","    # After decoding, the tokens are indices of words in the vocabulary. We map each token back\n","    # to its corresponding word using `index2word`. This converts the response from a list of\n","    # word indices to a list of words.\n","    decoded_words = [voc.index2word[token.item()] for token in tokens]\n","\n","    return decoded_words\n"],"metadata":{"id":"JgF5rRX1ZqzA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **4.3 Interactive Chat Function**\n","\n","- **Objective**: Enable users to chat interactively with the model by entering text and receiving generated responses. The chat loop runs until the user types \"q\" or \"quit\".\n","  \n","- **Explanation**:\n","  - This function continuously prompts the user for input, processes it, generates a response, and prints the model’s response.\n","  - It performs text normalization to ensure consistent formatting, evaluates the input through the model, and gracefully handles unknown words (those not in the vocabulary).\n","\n"],"metadata":{"id":"iLwWhIKsVnOL"}},{"cell_type":"code","source":["\n","def evaluateInput(encoder, decoder, searcher, voc):\n","    \"\"\"\n","    Interactively chat with the model.\n","\n","    Args:\n","        encoder (nn.Module): The trained encoder model.\n","        decoder (nn.Module): The trained decoder model.\n","        searcher (GreedySearchDecoder): The searcher object implementing greedy decoding.\n","        voc (Voc): Vocabulary object containing word-to-index mappings.\n","    \"\"\"\n","    input_sentence = ''  # Initialize the input sentence variable\n","\n","    # Start interactive loop for user to chat with the model\n","    while True:\n","        try:\n","            # Get input sentence from the user\n","            input_sentence = input('> ')\n","\n","            # Check for exit conditions: if user types 'q' or 'quit', end the chat\n","            if input_sentence == 'q' or input_sentence == 'quit':\n","                break\n","\n","            # Normalize the input sentence for consistent formatting\n","            # Normalization usually involves converting text to lowercase, removing punctuation, etc.\n","            input_sentence = normalizeString(input_sentence)\n","\n","            # Generate a response using the evaluate function\n","            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n","\n","            # Filter out 'EOS' and 'PAD' tokens from the output for cleaner display\n","            output_words = [word for word in output_words if word not in ('EOS', 'PAD')]\n","\n","            # Join the list of words into a single string and print as the bot's response\n","            print('Bot:', ' '.join(output_words))\n","\n","        except KeyError:\n","            # Handle cases where a word in the input sentence is not found in the vocabulary\n","            print(\"Error: Encountered unknown word.\")\n"],"metadata":{"id":"gKWpkz_5Z3OF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### **4.4 Running the Chatbot**\n","\n","- **Objective**: Initialize the models, set up the `GreedySearchDecoder`, and start the interactive chat session.\n","- **Explanation**: This code prepares the encoder and decoder for inference mode, initializes the searcher, and starts the chat function.\n"],"metadata":{"id":"Kctgy8mIVnLS"}},{"cell_type":"code","source":["# Set models to evaluation mode\n","encoder.eval()\n","decoder.eval()\n","\n","# Initialize the Greedy Search Decoder\n","searcher = GreedySearchDecoder(encoder, decoder)\n","\n","# Start chatting with the model\n","print(\"Chatbot is ready! Type 'quit' to exit.\")\n","evaluateInput(encoder, decoder, searcher, voc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NhQ7QJocaGRF","executionInfo":{"status":"ok","timestamp":1731343186020,"user_tz":0,"elapsed":65002,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"52b7f8a5-c001-40b9-d986-7fe594faf0c8"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Chatbot is ready! Type 'quit' to exit.\n","> hello?\n","Bot: hello . . . . .\n","> where am I?\n","Bot: in the lighthouse . the door . .\n","> who are you?\n","Bot: i m in my mood . . .\n","> how are you doing?\n","Bot: i m fine . the door . .\n","> you're under arrest\n","Bot: i m not . . ! !\n","> i'm just kidding\n","Bot: you re a gentleman ! ! !\n","> where are you from?\n","Bot: i m in italy . the park .\n","> it's time for me to leave\n","Bot: you re going to stay here the park .\n","> good bye\n","Bot: good morning . . . !\n","> quit\n"]}]},{"cell_type":"markdown","source":["### 1. Evaluation Matrices\n"],"metadata":{"id":"yFkwX03DY9G8"}},{"cell_type":"markdown","source":["Load the model trained from the directory"],"metadata":{"id":"pWOvxVaID95c"}},{"cell_type":"code","source":["# Load the checkpoint from the specified path\n","checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","\n","# Load encoder and decoder states\n","encoder.load_state_dict(checkpoint['en'])\n","decoder.load_state_dict(checkpoint['de'])\n","encoder.eval()\n","decoder.eval()\n","\n","print(\"Checkpoint loaded successfully!\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gq6SzIzD9Q_","executionInfo":{"status":"ok","timestamp":1731464442206,"user_tz":0,"elapsed":507,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"40ebca63-f912-4dda-e432-19790cecd1d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-23-b55268b2dd60>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n"]},{"output_type":"stream","name":"stdout","text":["Checkpoint loaded successfully!\n"]}]},{"cell_type":"markdown","source":["#### **1.1 BLEU Score**\n","The **BLEU (Bilingual Evaluation Understudy)** score is used to evaluate the quality of machine-translated text to a reference text. It can also be used to compare chatbot responses with expected (target) responses.\n","\n"],"metadata":{"id":"0Qo-Ibd7_PWf"}},{"cell_type":"code","source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","import matplotlib.pyplot as plt\n","\n","def evaluate_bleu_score(reference_sentences, candidate_sentences):\n","    \"\"\"\n","    Evaluates the BLEU score of chatbot responses compared to reference sentences\n","    and visualizes the scores for each response.\n","\n","    Parameters:\n","    reference_sentences (list of list of list of str): List of lists of reference sentences (correct answers).\n","                                                      Each sublist contains tokenized words of acceptable answers.\n","    candidate_sentences (list of list of str): List of tokenized chatbot responses.\n","\n","    Returns:\n","    float: Average BLEU score for the responses.\n","    \"\"\"\n","    # Initialize smoothing function to avoid zero scores for short responses\n","    smoothing_function = SmoothingFunction().method4\n","    scores = []\n","\n","    # Calculate BLEU score for each response\n","    for i, (references, candidate) in enumerate(zip(reference_sentences, candidate_sentences)):\n","        score = sentence_bleu(references, candidate, smoothing_function=smoothing_function)\n","        scores.append(score)\n","        print(f\"BLEU score for response {i+1}: {score:.4f}\")\n","\n","    # Calculate the average BLEU score\n","    average_score = sum(scores) / len(scores) if scores else 0\n","    print(f\"\\nAverage BLEU score: {average_score:.4f}\")\n","\n","    # Plot the BLEU scores\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(range(1, len(scores) + 1), scores, color='skyblue', edgecolor='black')\n","    plt.axhline(average_score, color='red', linestyle='--', label=f'Average BLEU score: {average_score:.4f}')\n","    plt.xlabel('Response Number')\n","    plt.ylabel('BLEU Score')\n","    plt.title('BLEU Scores for Chatbot Responses')\n","    plt.legend()\n","    plt.show()\n","\n","    return average_score\n","\n","# Define reference sentences with multiple acceptable references for each user query\n","reference_sentences = [\n","    [[\"hello\", \"how\", \"are\", \"you\"], [\"hi\", \"how\", \"are\", \"you\"]],\n","    [[\"what\", \"is\", \"your\", \"name\"], [\"your\", \"name\", \"is\", \"what\"]],\n","    [[\"where\", \"am\", \"i\"], [\"you\", \"are\", \"in\", \"the\", \"lighthouse\"]],\n","    [[\"how\", \"are\", \"you\", \"doing\"], [\"how\", \"do\", \"you\", \"feel\"]],\n","    [[\"you\", \"are\", \"under\", \"arrest\"], [\"you\", \"are\", \"detained\"]],\n","    [[\"i\", \"am\", \"just\", \"kidding\"], [\"it\", \"was\", \"a\", \"joke\"]],\n","    [[\"where\", \"are\", \"you\", \"from\"], [\"your\", \"origin\"]],\n","    [[\"it\", \"is\", \"time\", \"for\", \"me\", \"to\", \"leave\"], [\"i\", \"should\", \"leave\"]],\n","    [[\"goodbye\"], [\"see\", \"you\", \"later\"]],\n","]\n","\n","# Define the candidate sentences generated by the chatbot\n","candidate_sentences = [\n","    [\"hello\", \"how\", \"are\", \"you\"],\n","    [\"what\", \"is\", \"your\", \"name\"],\n","    [\"where\", \"am\", \"i\"],\n","    [\"how\", \"are\", \"you\", \"doing\"],\n","    [\"you\", \"are\", \"under\", \"arrest\"],\n","    [\"i\", \"am\", \"just\", \"kidding\"],\n","    [\"where\", \"are\", \"you\", \"from\"],\n","    [\"it\", \"is\", \"time\", \"for\", \"me\", \"to\", \"leave\"],\n","    [\"goodbye\"],\n","]\n","\n","# Evaluate BLEU score and visualize\n","evaluate_bleu_score(reference_sentences, candidate_sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":772},"id":"8wHuLo9F-sRx","executionInfo":{"status":"ok","timestamp":1731464567230,"user_tz":0,"elapsed":2945,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"6e01993a-3da8-492c-a956-250db4fe6d0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU score for response 1: 1.0000\n","BLEU score for response 2: 1.0000\n","BLEU score for response 3: 0.5757\n","BLEU score for response 4: 1.0000\n","BLEU score for response 5: 1.0000\n","BLEU score for response 6: 1.0000\n","BLEU score for response 7: 1.0000\n","BLEU score for response 8: 1.0000\n","BLEU score for response 9: 1.0000\n","\n","Average BLEU score: 0.9529\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["0.9528577477919415"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["#### 1.2. Cosine Similarity Score\n","Cosine similarity measures the similarity between two vectors, typically the generated response and a reference response. A score closer to 1 means the responses are more similar in terms of their word embeddings.\n","\n"],"metadata":{"id":"2tGpG_AKHBXg"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","import matplotlib.pyplot as plt\n","\n","# Define the function to calculate cosine similarity\n","def calculate_cosine_similarity(reference_sentence, candidate_sentence, tokenizer, embedding_model):\n","    \"\"\"\n","    Calculates the cosine similarity between the reference sentence and the candidate sentence.\n","\n","    Parameters:\n","    reference_sentence (str): The reference (correct) response.\n","    candidate_sentence (str): The generated response from the chatbot.\n","    tokenizer: Tokenizer for encoding sentences.\n","    embedding_model: Model to obtain sentence embeddings.\n","\n","    Returns:\n","    float: Cosine similarity score between the reference and candidate sentences.\n","    \"\"\"\n","    # Encode the sentences and get embeddings\n","    reference_tokens = tokenizer(reference_sentence, return_tensors='pt')\n","    candidate_tokens = tokenizer(candidate_sentence, return_tensors='pt')\n","\n","    # Get sentence embeddings from the model\n","    with torch.no_grad():\n","        reference_embedding = embedding_model(**reference_tokens).last_hidden_state.mean(dim=1)\n","        candidate_embedding = embedding_model(**candidate_tokens).last_hidden_state.mean(dim=1)\n","\n","    # Calculate cosine similarity\n","    cosine_sim = cosine_similarity(reference_embedding.numpy(), candidate_embedding.numpy())\n","    return cosine_sim[0][0]\n"],"metadata":{"id":"jbDVChrYG774"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Replace 'your_huggingface_token_here' with the actual token you got from Hugging Face.\n","os.environ[\"HF_TOKEN\"] = \"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\"\n"],"metadata":{"id":"C-tg4BR9U23K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Expected reference responses (ground truth)\n","reference_sentences = [\n","    \"hello .\",\n","    \"you re in a hospital .\",\n","    \"i m a lawyer .\",\n","    \"i m fine .\",\n","    \"no .\",\n","    \"i m trying to help you !\",\n","    \"i m sorry .\",\n","    \"san francisco .\",\n","    \"i know .\",\n","    \"goodbye .\"\n","]\n","\n","# Actual chatbot responses generated (candidate responses)\n","candidate_sentences = [\n","    \"hello . . . . .\",\n","    \"in the lighthouse . the door . .\",\n","    \"i m in my mood . . .\",\n","    \"i m fine . the door . .\",\n","    \"i m not . . ! !\",\n","    \"you re a gentleman ! ! !\",\n","    \"i m in italy . the park .\",\n","    \"you re going to stay here the park .\",\n","    \"good morning . . . !\"\n","]\n","\n","# Calculate cosine similarity for each response and collect scores\n","cosine_scores = []\n","for ref, cand in zip(reference_sentences, candidate_sentences):\n","    score = calculate_cosine_similarity(ref, cand, tokenizer, embedding_model)\n","    cosine_scores.append(score)\n","    print(f\"Cosine similarity for '{cand}': {score:.4f}\")\n","\n","# Calculate average cosine similarity score\n","average_cosine_score = sum(cosine_scores) / len(cosine_scores)\n","print(f\"\\nAverage Cosine Similarity Score: {average_cosine_score:.4f}\")\n","\n","# Visualization\n","plt.figure(figsize=(10, 6))\n","plt.bar(range(1, len(cosine_scores) + 1), cosine_scores, color='skyblue', edgecolor='black')\n","plt.axhline(average_cosine_score, color='red', linestyle='--', label=f'Average Cosine Similarity: {average_cosine_score:.4f}')\n","plt.xlabel('Response Number')\n","plt.ylabel('Cosine Similarity Score')\n","plt.title('Cosine Similarity Scores for Chatbot Responses')\n","plt.legend()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":755},"id":"yZ6h69J-HHn4","executionInfo":{"status":"ok","timestamp":1731464635368,"user_tz":0,"elapsed":3209,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"385b9ec6-a51a-4d55-b4fd-99aee2398b21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity for 'hello . . . . .': 0.7159\n","Cosine similarity for 'in the lighthouse . the door . .': 0.5918\n","Cosine similarity for 'i m in my mood . . .': 0.6015\n","Cosine similarity for 'i m fine . the door . .': 0.7524\n","Cosine similarity for 'i m not . . ! !': 0.6385\n","Cosine similarity for 'you re a gentleman ! ! !': 0.6512\n","Cosine similarity for 'i m in italy . the park .': 0.6771\n","Cosine similarity for 'you re going to stay here the park .': 0.5119\n","Cosine similarity for 'good morning . . . !': 0.6113\n","\n","Average Cosine Similarity Score: 0.6391\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"AQGU49a3HEO7"},"execution_count":null,"outputs":[]}]}