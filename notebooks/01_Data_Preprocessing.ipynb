{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. **Mount Google Drive to Access Data**:\n","   - The function `mount_drive()` mounts Google Drive to make files stored there accessible. This is necessary for accessing datasets stored in Google Drive from Google Colab.\n","\n","2. **Define Paths for Data Access and Storage**:\n","   - The function `define_paths()` sets up file paths for input (the original dataset) and output (processed data). It ensures that the directories exist and are ready for reading and writing data related to the project.\n","\n","3. **Load NLP Tools from NLTK for Text Processing**:\n","   - The function `load_nltk_resources()` downloads and initializes various NLP tools from NLTK, such as stopwords, lemmatizer, and stemmer, which are necessary for text preprocessing tasks like removing common stopwords, reducing words to their root forms, and stemming.\n","\n","    1. **`nltk.download('stopwords')`**: Downloads a list of common words (e.g., \"and,\" \"the\") that are often removed during text preprocessing.\n","    2. **`nltk.download('wordnet')`**: Downloads the WordNet lexical database, which is used for lemmatizing words to their base or root forms.\n","    3. **`nltk.download('omw-1.4')`**: Downloads the Open Multilingual Wordnet, which provides additional linguistic data for WordNet.\n","    4. **`nltk.download('punkt')`**: Downloads a tokenizer that can split text into sentences and words, used for segmenting text during processing."],"metadata":{"id":"hpAdNyMLQNjG"}},{"cell_type":"code","source":["import os\n","import csv\n","import json\n","import codecs\n","import re\n","import unicodedata\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from google.colab import drive\n","import random\n","import string\n","import nltk\n","\n","# Step 1: Mount Google Drive to access files\n","def mount_drive():\n","    # Mount Google Drive to make sure we can access the dataset stored in Google Drive.\n","    drive.mount('/content/drive')\n","\n","# Step 2: Define paths for input and output data\n","def define_paths():\n","    # Define the paths to the corpus and processed data within Google Drive\n","    corpus_name = \"movie-corpus\"\n","    corpus = os.path.join(\"/content/drive/My Drive/Colab Notebooks/nlp_pro_babu/data\", corpus_name)\n","    processed_data_dir = os.path.join(\"/content/drive/My Drive/Colab Notebooks/nlp_pro_babu/data/processed\")\n","    os.makedirs(processed_data_dir, exist_ok=True)  # Create directory if it doesn't exist\n","    return corpus, processed_data_dir\n","\n","# Step 3: Load necessary NLP tools from NLTK\n","def load_nltk_resources():\n","    # Download necessary resources for text processing\n","    nltk.download('stopwords')\n","    nltk.download('wordnet')\n","    nltk.download('omw-1.4')\n","    nltk.download('punkt')\n","\n","    # Initialize stop words, lemmatizer, and stemmer\n","    stop_words = set(stopwords.words('english'))  # Load stop words from NLTK\n","    lemmatizer = WordNetLemmatizer()  # Initialize lemmatizer to reduce words to their base form\n","    stemmer = PorterStemmer()  # Initialize Porter stemmer for stemming words\n","    return stop_words, lemmatizer, stemmer\n"],"metadata":{"id":"S_Ybtnu7PB9g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Extracting Data from Database"],"metadata":{"id":"Wfnfxy2mQ3VZ"}},{"cell_type":"code","source":["\n","# Step 4: Function to print lines from a file for preview\n","def print_lines(file, n=10):\n","    # This function allows us to check the contents of a file, useful for debugging and verification\n","    with open(file, 'rb') as datafile:\n","        lines = datafile.readlines()\n","    for line in lines[:n]:\n","        print(line)\n","\n","# Step 5: Load lines and conversations from the dataset\n","def load_lines_and_conversations(file_name):\n","    # This function extracts individual lines and groups them into conversations to be used for training\n","    lines = {}\n","    conversations = {}\n","    with open(file_name, 'r', encoding='iso-8859-1') as f:\n","        for line in f:\n","            line_json = json.loads(line)\n","            line_obj = {\n","                \"lineID\": line_json[\"id\"],\n","                \"characterID\": line_json[\"speaker\"],\n","                \"text\": line_json[\"text\"]\n","            }\n","            lines[line_obj['lineID']] = line_obj\n","            # Group lines into conversations\n","            if line_json[\"conversation_id\"] not in conversations:\n","                conv_obj = {\n","                    \"conversationID\": line_json[\"conversation_id\"],\n","                    \"movieID\": line_json[\"meta\"][\"movie_id\"],\n","                    \"lines\": [line_obj]\n","                }\n","            else:\n","                conv_obj = conversations[line_json[\"conversation_id\"]]\n","                conv_obj[\"lines\"].insert(0, line_obj)\n","            conversations[conv_obj[\"conversationID\"]] = conv_obj\n","    return lines, conversations\n","\n","# Step 6: Extract sentence pairs from conversations\n","def extract_sentence_pairs(conversations):\n","    # Create question-answer pairs by extracting consecutive lines in each conversation\n","    qa_pairs = []\n","    for conversation in conversations.values():\n","        for i in range(len(conversation[\"lines\"]) - 1):\n","            input_line = conversation[\"lines\"][i][\"text\"].strip()\n","            target_line = conversation[\"lines\"][i+1][\"text\"].strip()\n","            if input_line and target_line:\n","                qa_pairs.append([input_line, target_line])\n","    return qa_pairs\n"],"metadata":{"id":"IKBIvA1VQWix"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 7: Additional Data Processing Techniques**\n","\n","1. **Normalize the String**:\n","   - Convert the text to lowercase and remove non-letter characters.\n","   - This helps standardize the text for easier processing by removing inconsistencies like casing and special characters.\n","\n","2. **Remove Stopwords**:\n","   - Remove common stopwords like \"is\" or \"the\" to reduce noise in the data. - This helps the model focus on the more important parts of the sentence that carry meaningful information.\n","\n","3. **Lemmatize the Sentence**:\n","   - Convert each word in the sentence to its base or dictionary form (e.g., \"running\" becomes \"run\").\n","   - Lemmatization helps standardize different forms of words and improves the efficiency of the model by reducing redundancy.\n","\n","4. **Stem the Sentence**:\n","   - Apply stemming to reduce words to their root by removing suffixes (e.g., \"studies\" becomes \"studi\").\n","   - This is another form of word normalization but may be a bit more aggressive than lemmatization.\n","\n","5. **Remove Punctuation**:\n","   - Remove punctuation marks from the text to further clean the data.\n","   - This is often necessary to make the model focus on the core content of sentences without being distracted by punctuation symbols.\n","\n","6. **Tokenize the Sentence**:\n","   - Split the sentence into individual words (tokens).\n","   - This is an important step in NLP, allowing further processing on a word-by-word basis.\n","\n","7. **Augment Data by Shuffling Words in Sentences**:\n","   - To make the model more robust, shuffle words in the sentences randomly to generate variations.\n","   -  This technique increases the number of available training pairs, which helps in training a more generalized model.\n"],"metadata":{"id":"kKFhwRU_Q7xb"}},{"cell_type":"code","source":["\n","# Step 7: Additional Data Processing Techniques\n","\n","# 7.1: Normalize the string\n","def normalize_string(s):\n","    # Normalization involves converting to lowercase and removing non-letter characters\n","    s = s.lower().strip()  # Convert to lowercase and remove leading/trailing spaces\n","    s = ''.join(\n","        c for c in unicodedata.normalize('NFD', s)  # Decompose special characters\n","        if unicodedata.category(c) != 'Mn'  # Remove accent characters\n","    )\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)  # Add space before punctuation marks\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  # Remove any character that isn't a letter or punctuation\n","    s = re.sub(r\"\\s+\", r\" \", s).strip()  # Replace multiple spaces with a single space\n","    return s\n","\n","# 7.2: Remove stopwords from the sentence\n","def remove_stopwords(sentence, stop_words):\n","    # This function removes common stopwords (e.g., 'is', 'the') to reduce noise in the data\n","    words = sentence.split()\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","    return ' '.join(filtered_words)\n","\n","# 7.3: Lemmatize the sentence\n","def lemmatize_sentence(sentence, lemmatizer):\n","    # Lemmatization reduces words to their base or root form (e.g., 'running' becomes 'run')\n","    words = sentence.split()\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","    return ' '.join(lemmatized_words)\n","\n","# 7.4: Stem the sentence\n","def stem_sentence(sentence, stemmer):\n","    # Stemming reduces words to their root form by removing suffixes (e.g., 'running' becomes 'run')\n","    words = sentence.split()\n","    stemmed_words = [stemmer.stem(word) for word in words]\n","    return ' '.join(stemmed_words)\n","\n","# 7.5: Remove punctuation from the sentence\n","def remove_punctuation(sentence):\n","    # This function removes punctuation marks to further clean the text\n","    return sentence.translate(str.maketrans('', '', string.punctuation))\n","\n","# 7.6: Tokenize the sentence\n","def tokenize_sentence(sentence):\n","    # Tokenization splits the sentence into individual words for further processing\n","    return nltk.word_tokenize(sentence)\n","\n","# 7.7: Augment data by shuffling words in sentences\n","def augment_data(pairs, num_augments=1):\n","    # This simple augmentation technique introduces variations in word order to make the model more robust\n","    augmented_pairs = []\n","    for pair in pairs:\n","        for _ in range(num_augments):\n","            input_line_words = pair[0].split()\n","            target_line_words = pair[1].split()\n","            random.shuffle(input_line_words)\n","            random.shuffle(target_line_words)\n","            augmented_pairs.append([' '.join(input_line_words), ' '.join(target_line_words)])\n","    return pairs + augmented_pairs\n","\n","# Step 8: Load, process, and augment data\n","def load_process_and_augment_data(corpus, stop_words, lemmatizer, stemmer):\n","    # Load the lines and conversations from the dataset and extract the sentence pairs\n","    lines, conversations = load_lines_and_conversations(os.path.join(corpus, \"utterances.jsonl\"))\n","    qa_pairs = extract_sentence_pairs(conversations)\n","\n","    # Step 9: Apply data processing to each pair\n","    # Normalize, remove punctuation, remove stopwords, lemmatize, and stem each sentence in the question-answer pairs\n","    processed_pairs = []\n","    for pair in qa_pairs:\n","        input_sentence, target_sentence = pair\n","        input_sentence = normalize_string(input_sentence)  # Normalize by converting to lowercase and removing non-letter characters\n","        target_sentence = normalize_string(target_sentence)\n","        input_sentence = remove_punctuation(input_sentence)  # Remove punctuation\n","        target_sentence = remove_punctuation(target_sentence)\n","        input_sentence = remove_stopwords(input_sentence, stop_words)  # Remove stopwords\n","        target_sentence = remove_stopwords(target_sentence, stop_words)\n","        input_sentence = lemmatize_sentence(input_sentence, lemmatizer)  # Lemmatize to reduce words to their base form\n","        target_sentence = lemmatize_sentence(target_sentence, lemmatizer)\n","        input_sentence = stem_sentence(input_sentence, stemmer)  # Apply stemming to further reduce words\n","        target_sentence = stem_sentence(target_sentence, stemmer)\n","        processed_pairs.append([input_sentence, target_sentence])\n","\n","    # Step 10: Augment data to increase dataset size\n","    # Apply data augmentation to increase the number of training pairs and introduce variation\n","    augmented_pairs = augment_data(processed_pairs, num_augments=1)\n","    return augmented_pairs\n","\n","# Step 11: Save the processed data\n","def save_processed_data(augmented_pairs, processed_data_dir):\n","    # Save the processed and augmented pairs to a text file for future use\n","    datafile = os.path.join(processed_data_dir, \"formatted_movie_lines.txt\")\n","    delimiter = '\\t'\n","    delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n","\n","    print(\"\\nWriting processed and augmented file...\")\n","    with open(datafile, 'w', encoding='utf-8') as outputfile:\n","        writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n","        for pair in augmented_pairs:\n","            writer.writerow(pair)\n","\n","    # Step 12: Verify the saved file\n","    # Print a few lines from the saved processed file to verify the output\n","    print(\"Sample lines from processed file:\")\n","    print_lines(datafile)\n"],"metadata":{"id":"Akwm503DQYvC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Main function to run all steps\n","def main():\n","    mount_drive()\n","    corpus, processed_data_dir = define_paths()\n","    load_nltk_resources()\n","    stop_words, lemmatizer, stemmer = load_nltk_resources()\n","    augmented_pairs = load_process_and_augment_data(corpus, stop_words, lemmatizer, stemmer)\n","    save_processed_data(augmented_pairs, processed_data_dir)\n","\n","# Run the main function\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s5DB-Z_HQb1g","executionInfo":{"status":"ok","timestamp":1731295664535,"user_tz":0,"elapsed":103906,"user":{"displayName":"DMU","userId":"10690906031860540357"}},"outputId":"df685d69-a0ca-437f-8b26-1ab05b757e36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\n","Writing processed and augmented file...\n","Sample lines from processed file:\n","b'\\t\\n'\n","b'okay\\thope\\n'\n","b'wow\\tlet go\\n'\n","b'kid know sometim becom persona know quit\\t\\n'\n","b'\\tokay gonna need learn lie\\n'\n","b'figur get good stuff eventu\\tgood stuff\\n'\n","b'good stuff\\treal\\n'\n","b'real\\tlike fear wear pastel\\n'\n","b'listen crap\\tcrap\\n'\n","b'crap\\tendless blond babbl like bore\\n'\n"]}]}]}